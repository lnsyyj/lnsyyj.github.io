<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/11/13/ceph-osd-blacklist/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/13/ceph-osd-blacklist/" class="post-title-link" itemprop="url">ceph osd blacklist</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-13 09:03:48" itemprop="dateCreated datePublished" datetime="2019-11-13T09:03:48+08:00">2019-11-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-22 16:16:36" itemprop="dateModified" datetime="2020-03-22T16:16:36+08:00">2020-03-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="什么是OSD-blacklist，如何处理？"><a href="#什么是OSD-blacklist，如何处理？" class="headerlink" title="什么是OSD blacklist，如何处理？"></a>什么是OSD blacklist，如何处理？</h1><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>Red Hat Ceph Storage</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>我正在运行<code>ceph osd dump</code>命令，它确实列出了blacklist items：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ceph osd dump</span><br><span class="line">[...]</span><br><span class="line">blacklist 10.37.192.139:0&#x2F;1308721908 expires 2019-02-27 10:10:52.049084</span><br></pre></td></tr></table></figure>
<p>这是什么意思，我该如何解决？</p>
<h3 id="决议"><a href="#决议" class="headerlink" title="决议"></a>决议</h3><p>尽管有一些控制命令可删除blacklist entries（例如<code>ceph osd blacklist rm ADDRESS[:source_port]</code>），但blacklists通常会自动维护，无需手动干预。因此，您无需采取任何措施。如有疑问，请联系Red Hat支持。</p>
<h3 id="根本原因"><a href="#根本原因" class="headerlink" title="根本原因"></a>根本原因</h3><p>blacklist最常用于CephFS场景中，以防止滞后的元数据服务器对OSD上的数据进行不良更改。</p>
<h3 id="诊断步骤"><a href="#诊断步骤" class="headerlink" title="诊断步骤"></a>诊断步骤</h3>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ceph osd blacklist ls</span><br><span class="line">listed 1 entries</span><br><span class="line">10.37.192.139:0&#x2F;1308721908 2019-02-27 10:10:52.049084</span><br></pre></td></tr></table></figure>
<p>该解决方案是Red Hat快速发布计划的一部分，提供了Red Hat工程师在为客户提供支持时创建的庞大解决方案库。 为了使您立即获得所需的知识，这些文章可能以未经编辑的原始形式出现。</p>
<h1 id="CEPH-FILESYSTEM-CLIENT-EVICTION（CEPH文件系统客户端驱逐）"><a href="#CEPH-FILESYSTEM-CLIENT-EVICTION（CEPH文件系统客户端驱逐）" class="headerlink" title="CEPH FILESYSTEM CLIENT EVICTION（CEPH文件系统客户端驱逐）"></a>CEPH FILESYSTEM CLIENT EVICTION（CEPH文件系统客户端驱逐）</h1><p>当文件系统客户端无响应或行为异常时，可能有必要强制终止其对文件系统的访问。 此过程称为eviction（驱逐）。</p>
<p>驱逐CephFS客户端会阻止其与MDS daemons和OSD daemons进一步通信。 如果客户端正在对文件系统进行buffered IO，则所有未刷新的数据都将丢失。</p>
<p>客户端可以自动退出（如果无法及时与MDS通信），也可以手动退出（由系统管理员）。</p>
<p>客户端驱逐过程适用于各种客户端，包括FUSE mounts，kernel mounts，nfs-ganesha gateways以及任何使用libcephfs的进程。</p>
<h3 id="AUTOMATIC-CLIENT-EVICTION（自动客户端逐出）"><a href="#AUTOMATIC-CLIENT-EVICTION（自动客户端逐出）" class="headerlink" title="AUTOMATIC CLIENT EVICTION（自动客户端逐出）"></a>AUTOMATIC CLIENT EVICTION（自动客户端逐出）</h3><p>在三种情况下，可能会自动将客户驱逐：</p>
<ul>
<li><p>在active MDS daemon上，如果客户端在session_autoclose（文件系统变量）秒（默认为300秒）以上未与MDS通信，则它将自动被驱逐。</p>
</li>
<li><p>在active MDS daemon上，如果客户端在mds_cap_revoke_eviction_timeout（配置选项）秒内未响应cap revoke messages。 默认情况下禁用。</p>
</li>
<li><p>在MDS启动期间（包括故障转移时），MDS称为reconnect的状态。 在此状态期间，它将等待所有客户端连接到新的MDS daemon。 如果客户端未在时间窗口内这样做（mds_reconnect_timeout，默认为45秒），则将其驱逐。</p>
</li>
</ul>
<p>如果出现以上任何一种情况，warning message将发送到cluster log。</p>
<h3 id="MANUAL-CLIENT-EVICTION（手动客户端驱逐）"><a href="#MANUAL-CLIENT-EVICTION（手动客户端驱逐）" class="headerlink" title="MANUAL CLIENT EVICTION（手动客户端驱逐）"></a>MANUAL CLIENT EVICTION（手动客户端驱逐）</h3><p>有时，管理员可能希望手动驱逐客户端。 如果客户端死亡，并且管理员不想等待其session超时；或者，如果客户端行为异常并且管理员无权访问客户端节点来卸载它。</p>
<p>首先检查客户列表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">ceph tell mds.0 client ls</span><br><span class="line"></span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;id&quot;: 4305,</span><br><span class="line">        &quot;num_leases&quot;: 0,</span><br><span class="line">        &quot;num_caps&quot;: 3,</span><br><span class="line">        &quot;state&quot;: &quot;open&quot;,</span><br><span class="line">        &quot;replay_requests&quot;: 0,</span><br><span class="line">        &quot;completed_requests&quot;: 0,</span><br><span class="line">        &quot;reconnecting&quot;: false,</span><br><span class="line">        &quot;inst&quot;: &quot;client.4305 172.21.9.34:0&#x2F;422650892&quot;,</span><br><span class="line">        &quot;client_metadata&quot;: &#123;</span><br><span class="line">            &quot;ceph_sha1&quot;: &quot;ae81e49d369875ac8b569ff3e3c456a31b8f3af5&quot;,</span><br><span class="line">            &quot;ceph_version&quot;: &quot;ceph version 12.0.0-1934-gae81e49 (ae81e49d369875ac8b569ff3e3c456a31b8f3af5)&quot;,</span><br><span class="line">            &quot;entity_id&quot;: &quot;0&quot;,</span><br><span class="line">            &quot;hostname&quot;: &quot;senta04&quot;,</span><br><span class="line">            &quot;mount_point&quot;: &quot;&#x2F;tmp&#x2F;tmpcMpF1b&#x2F;mnt.0&quot;,</span><br><span class="line">            &quot;pid&quot;: &quot;29377&quot;,</span><br><span class="line">            &quot;root&quot;: &quot;&#x2F;&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>一旦识别出要逐出的客户机，就可以使用其唯一ID或各种其他属性来识别它：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># These all work</span><br><span class="line">ceph tell mds.0 client evict id&#x3D;4305</span><br><span class="line">ceph tell mds.0 client evict client_metadata.&#x3D;4305</span><br></pre></td></tr></table></figure>
<h3 id="ADVANCED-UN-BLACKLISTING-A-CLIENT（进阶：取消blacklist客户）"><a href="#ADVANCED-UN-BLACKLISTING-A-CLIENT（进阶：取消blacklist客户）" class="headerlink" title="ADVANCED: UN-BLACKLISTING A CLIENT（进阶：取消blacklist客户）"></a>ADVANCED: UN-BLACKLISTING A CLIENT（进阶：取消blacklist客户）</h3><p>通常，列入blacklist的客户端可能无法重新连接到服务器：必须先将其unmount，然后再重新mount。</p>
<p>但是，在某些情况下，允许被驱逐的客户端尝试重新连接可能会很有用。</p>
<p>由于CephFS使用RADOS OSD blacklist控制客户端驱逐，因此可以通过从blacklist中删除CephFS客户端来重新连接它们：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd blacklist ls</span><br><span class="line">listed 1 entries</span><br><span class="line">127.0.0.1:0&#x2F;3710147553 2018-03-19 11:32:24.716146</span><br><span class="line"></span><br><span class="line">$ ceph osd blacklist rm 127.0.0.1:0&#x2F;3710147553</span><br><span class="line">un-blacklisting 127.0.0.1:0&#x2F;3710147553</span><br></pre></td></tr></table></figure>
<p>如果其他客户端访问了列入blacklist的客户端正在buffered IO的文件，则这样做可能会使数据完整性受到威胁。 也不能保证产生一个功能完备的客户端 — 在驱逐后恢复完全健康的客户端的最佳方法是unmount客户端并重新mount。</p>
<p>如果您尝试以这种方式重新连接客户端，则在FUSE客户端中将client_reconnect_stale设置为true，以提示客户端尝试重新连接。</p>
<h3 id="ADVANCED-CONFIGURING-BLACKLISTING（进阶：配置blacklist）"><a href="#ADVANCED-CONFIGURING-BLACKLISTING（进阶：配置blacklist）" class="headerlink" title="ADVANCED: CONFIGURING BLACKLISTING（进阶：配置blacklist）"></a>ADVANCED: CONFIGURING BLACKLISTING（进阶：配置blacklist）</h3><p>如果由于客户端主机速度慢或网络不可靠而频繁驱逐客户端，并且您无法解决根本问题，那么您可能希望要求MDS的严格性降低。</p>
<p>可以通过放弃其MDS sessions来响应慢速客户端，但允许他们重新打开sessions并允许他们继续与OSD对话。 要启用此模式，请在MDS节点上将mds_session_blacklist_on_timeout设置为false。</p>
<p>对于手动驱逐的等效行为，请将mds_session_blacklist_on_evict设置为false。</p>
<p>请注意，如果禁用了blacklist，则驱逐客户端只会对您发送命令的MDS产生影响。 在具有multiple active MDS daemons的系统上，您需要向每个active daemon发送驱逐命令。 启用blacklist（默认设置）后，仅将驱逐命令发送到单个MDS就足够了，因为blacklist会将其传播到其他MDS。</p>
<h3 id="BACKGROUND-BLACKLISTING-AND-OSD-EPOCH-BARRIER（背景：blacklist和OSD-epoch-barrier）"><a href="#BACKGROUND-BLACKLISTING-AND-OSD-EPOCH-BARRIER（背景：blacklist和OSD-epoch-barrier）" class="headerlink" title="BACKGROUND: BLACKLISTING AND OSD EPOCH BARRIER（背景：blacklist和OSD epoch barrier）"></a>BACKGROUND: BLACKLISTING AND OSD EPOCH BARRIER（背景：blacklist和OSD epoch barrier）</h3><p>在将客户端列入blacklist之后，有必要确保其他客户端和MDS daemons在尝试访问被列入blacklist的客户端可能已访问的任何数据对象之前，具有最新的OSDMap（包括blacklist entry）。</p>
<p>使用内部的”osdmap epoch barrier”机制可以确保这一点。</p>
<p>barrier的目的是确保当我们分发任何允许touching相同 RADOS objects的功能时，分发的客户端必须具有最新的 OSD map，不与已cancel的操作（来自 ENOSPC）或blacklist客户端（逐出）进行竞争。</p>
<p>更具体地说，设置epoch barrier的情况是：</p>
<ul>
<li>Client eviction — 客户端驱逐（客户端被列入blacklist，其他客户端必须等待post-blacklist epoch后才能touch相同的objects）。</li>
<li>客户端中的OSD map full flag handling（客户端可以从pre-full epoch取消某些OSD操作，因此其他客户端必须等到full epoch或更晚才能touching相同的objects）。</li>
<li>MDS启动，因为我们不持续维护barrier epoch，因此，必须假定重新启动后始终需要最新的OSD map。</li>
</ul>
<p>请注意，这是简单的global value。 我们可以在每个inode的基础上进行维护。 但是我们没有，因为：</p>
<ul>
<li>它将更加复杂。</li>
<li>每个inode将使用额外的4个字节的内存。</li>
<li>因为几乎每个人都拥有最新的OSD map，所以效率不会更高。 而且，在大多数情况下，每个人都会轻而易举地克服这一barrier，而不是waiting。</li>
<li>在极少数情况下遇到barrier，因此很少会看到每个inode粒度带来好处。</li>
</ul>
<p>epoch barrier与所有capability messages一起发送，并指示message的接收者避免在看到OSD epoch之前向OSD发送更多的RADOS操作。 这主要适用于客户端（将其数据直接写到文件中），但也适用于MDS，因为诸如文件大小probing和文件删除之类的操作是直接从MDS完成的。</p>
<h1 id="blacklist相关命令"><a href="#blacklist相关命令" class="headerlink" title="blacklist相关命令"></a>blacklist相关命令</h1><h3 id="1、从blacklist中添加（可选项，直到-lt-expire-gt-秒后）或删除-lt-addr-gt-，默认3600秒"><a href="#1、从blacklist中添加（可选项，直到-lt-expire-gt-秒后）或删除-lt-addr-gt-，默认3600秒" class="headerlink" title="1、从blacklist中添加（可选项，直到&lt;expire&gt;秒后）或删除&lt;addr&gt;，默认3600秒"></a>1、从blacklist中添加（可选项，直到<code>&lt;expire&gt;</code>秒后）或删除<code>&lt;addr&gt;</code>，默认3600秒</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">osd blacklist add|rm &lt;EntityAddr&gt; &#123;&lt;float[0.0-]&gt;&#125;                   add (optionally until &lt;expire&gt; seconds from now) or remove &lt;addr&gt; from blacklist</span><br></pre></td></tr></table></figure>
<p>实验1，添加删除blacklist测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph3 ~]# ceph osd blacklist add 10.20.10.28</span><br><span class="line">blacklisting 10.20.10.28:0&#x2F;0 until 2019-11-13 12:55:53.700776 (3600 sec)</span><br><span class="line">[root@ceph3 ~]# ceph osd blacklist add 10.20.10.13 6000</span><br><span class="line">blacklisting 10.20.10.13:0&#x2F;0 until 2019-11-13 13:36:16.575894 (6000 sec)</span><br><span class="line"></span><br><span class="line">[root@ceph3 ~]# ceph osd blacklist ls</span><br><span class="line">listed 2 entries</span><br><span class="line">10.20.10.13:0&#x2F;0 2019-11-13 13:36:16.575894</span><br><span class="line">10.20.10.28:0&#x2F;0 2019-11-13 12:55:53.700776</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph3 ~]# ceph osd blacklist ls</span><br><span class="line">listed 1 entries</span><br><span class="line">10.20.10.28:0&#x2F;0 2019-11-13 10:23:00.029669</span><br></pre></td></tr></table></figure>
<p>实验2，当client在blacklist中时，在client端尝试mount cephfs（ceph-client  10.20.10.2）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">1、将ceph-client加入blacklist</span><br><span class="line">[root@ceph1 ~]# ceph osd blacklist add 10.20.10.2</span><br><span class="line">[root@ceph1 ~]# ceph osd blacklist ls</span><br><span class="line">listed 1 entries</span><br><span class="line">10.20.10.2:0&#x2F;0 2019-11-13 15:42:54.260358</span><br><span class="line"></span><br><span class="line">2、ceph-client尝试mount cephfs</span><br><span class="line">[root@ceph-client ~]# ceph-fuse &#x2F;root&#x2F;ceph-fuse&#x2F; --verbose</span><br><span class="line">ceph-fuse[1664]: starting ceph client</span><br><span class="line">2019-11-13 14:45:18.902688 7f8f3b0db0c0 -1 init, newargv &#x3D; 0x55c15933c000 newargc&#x3D;10</span><br><span class="line">ceph-fuse[1664]: ceph mount failed with (1) Operation not permitted</span><br><span class="line"></span><br><span class="line">3、将ceph-client从blacklist中删除</span><br><span class="line">[root@ceph1 ~]# ceph osd blacklist rm 10.20.10.2</span><br><span class="line">un-blacklisting 10.20.10.2:0&#x2F;0</span><br><span class="line"></span><br><span class="line">[root@ceph-client ~]# ceph-fuse &#x2F;root&#x2F;ceph-fuse&#x2F; </span><br><span class="line">ceph-fuse[1704]: starting ceph client</span><br><span class="line">2019-11-13 14:49:16.400939 7f9326c7c0c0 -1 init, newargv &#x3D; 0x557665de4ea0 newargc&#x3D;9</span><br><span class="line">ceph-fuse[1704]: starting fuse</span><br><span class="line"></span><br><span class="line">[root@ceph-client ~]# df -Th</span><br><span class="line">ceph-fuse      fuse.ceph-fuse   93G     0   93G   0% &#x2F;root&#x2F;ceph-fuse</span><br></pre></td></tr></table></figure>
<h3 id="2、清除所有列入blacklist的客户端"><a href="#2、清除所有列入blacklist的客户端" class="headerlink" title="2、清除所有列入blacklist的客户端"></a>2、清除所有列入blacklist的客户端</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">osd blacklist clear                   clear all blacklisted clients</span><br></pre></td></tr></table></figure>
<p>实验1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph3 ~]# ceph osd blacklist add 10.20.10.28</span><br><span class="line">blacklisting 10.20.10.28:0&#x2F;0 until 2019-11-13 12:53:48.463948 (3600 sec)</span><br><span class="line">[root@ceph3 ~]# ceph osd blacklist add 10.20.10.13</span><br><span class="line">blacklisting 10.20.10.13:0&#x2F;0 until 2019-11-13 12:53:56.846733 (3600 sec)</span><br><span class="line"></span><br><span class="line">[root@ceph3 ~]# ceph osd blacklist ls</span><br><span class="line">listed 2 entries</span><br><span class="line">10.20.10.13:0&#x2F;0 2019-11-13 12:53:56.846733</span><br><span class="line">10.20.10.28:0&#x2F;0 2019-11-13 12:53:48.463948</span><br><span class="line"></span><br><span class="line">[root@ceph3 ~]# ceph osd blacklist clear</span><br><span class="line"> removed all blacklist entries</span><br><span class="line"></span><br><span class="line">[root@ceph3 ~]# ceph osd blacklist ls</span><br><span class="line">listed 0 entries</span><br></pre></td></tr></table></figure>
<h3 id="3、显示列入blacklist的客户端"><a href="#3、显示列入blacklist的客户端" class="headerlink" title="3、显示列入blacklist的客户端"></a>3、显示列入blacklist的客户端</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">osd blacklist ls --format json                   show blacklisted clients</span><br></pre></td></tr></table></figure>
<p>实验1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# ceph osd blacklist ls --format json</span><br><span class="line">listed 1 entries</span><br><span class="line"></span><br><span class="line">[&#123;&quot;addr&quot;:&quot;10.20.10.2:0&#x2F;0&quot;,&quot;until&quot;:&quot;2019-11-13 17:10:56.217959&quot;&#125;]</span><br><span class="line"></span><br><span class="line">&#x2F;0 表示：AsyncMessenger stuff approximately unique ID set by the Constructor for use in entity_addr_t</span><br></pre></td></tr></table></figure>
<h3 id="4、如果客户端在session-autoclose-lt-value-gt-秒（默认为300秒）以上未与MDS通信，则它将自动被驱逐。"><a href="#4、如果客户端在session-autoclose-lt-value-gt-秒（默认为300秒）以上未与MDS通信，则它将自动被驱逐。" class="headerlink" title="4、如果客户端在session_autoclose &lt;value&gt;秒（默认为300秒）以上未与MDS通信，则它将自动被驱逐。"></a>4、如果客户端在<code>session_autoclose &lt;value&gt;</code>秒（默认为300秒）以上未与MDS通信，则它将自动被驱逐。</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fs set &lt;fs_name&gt; max_mds|max_file_size|allow_new_snaps|inline_data|cluster_down|allow_multimds|allow_dirfrags| balancer|standby_count_wanted|session_timeout|session_autoclose &lt;val&gt; &#123;&lt;confirm&gt;&#125;			set fs parameter &lt;var&gt; to &lt;val&gt;</span><br></pre></td></tr></table></figure>
<p>实验1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# ceph fs set cephfs session_autoclose 400</span><br></pre></td></tr></table></figure>
<h3 id="5、获取有关一个文件系统的信息"><a href="#5、获取有关一个文件系统的信息" class="headerlink" title="5、获取有关一个文件系统的信息"></a>5、获取有关一个文件系统的信息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph fs get &lt;fs_name&gt; --format json</span><br></pre></td></tr></table></figure>
<p>实验1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# ceph fs get cephfs --format json</span><br><span class="line">&#123;</span><br><span class="line">	&quot;mdsmap&quot;: &#123;</span><br><span class="line">		&quot;epoch&quot;: 19,</span><br><span class="line">		&quot;flags&quot;: 12,</span><br><span class="line">		&quot;ever_allowed_features&quot;: 0,</span><br><span class="line">		&quot;explicitly_allowed_features&quot;: 0,</span><br><span class="line">		&quot;created&quot;: &quot;2019-11-11 11:16:05.316461&quot;,</span><br><span class="line">		&quot;modified&quot;: &quot;2019-11-13 15:59:17.551876&quot;,</span><br><span class="line">		&quot;tableserver&quot;: 0,</span><br><span class="line">		&quot;root&quot;: 0,</span><br><span class="line">		&quot;session_timeout&quot;: 60,</span><br><span class="line">		&quot;session_autoclose&quot;: 400,</span><br><span class="line">		&quot;max_file_size&quot;: 1099511627776,</span><br><span class="line">		&quot;last_failure&quot;: 0,</span><br><span class="line">		&quot;last_failure_osd_epoch&quot;: 104,</span><br><span class="line">		&quot;compat&quot;: &#123;</span><br><span class="line">			&quot;compat&quot;: &#123;&#125;,</span><br><span class="line">			&quot;ro_compat&quot;: &#123;&#125;,</span><br><span class="line">			&quot;incompat&quot;: &#123;</span><br><span class="line">				&quot;feature_1&quot;: &quot;base v0.20&quot;,</span><br><span class="line">				&quot;feature_2&quot;: &quot;client writeable ranges&quot;,</span><br><span class="line">				&quot;feature_3&quot;: &quot;default file layouts on dirs&quot;,</span><br><span class="line">				&quot;feature_4&quot;: &quot;dir inode in separate object&quot;,</span><br><span class="line">				&quot;feature_5&quot;: &quot;mds uses versioned encoding&quot;,</span><br><span class="line">				&quot;feature_6&quot;: &quot;dirfrag is stored in omap&quot;,</span><br><span class="line">				&quot;feature_8&quot;: &quot;no anchor table&quot;,</span><br><span class="line">				&quot;feature_9&quot;: &quot;file layout v2&quot;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;max_mds&quot;: 1,</span><br><span class="line">		&quot;in&quot;: [0],</span><br><span class="line">		&quot;up&quot;: &#123;</span><br><span class="line">			&quot;mds_0&quot;: 4335</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;failed&quot;: [],</span><br><span class="line">		&quot;damaged&quot;: [],</span><br><span class="line">		&quot;stopped&quot;: [],</span><br><span class="line">		&quot;info&quot;: &#123;</span><br><span class="line">			&quot;gid_4335&quot;: &#123;</span><br><span class="line">				&quot;gid&quot;: 4335,</span><br><span class="line">				&quot;name&quot;: &quot;ceph2&quot;,</span><br><span class="line">				&quot;rank&quot;: 0,</span><br><span class="line">				&quot;incarnation&quot;: 14,</span><br><span class="line">				&quot;state&quot;: &quot;up:active&quot;,</span><br><span class="line">				&quot;state_seq&quot;: 41535,</span><br><span class="line">				&quot;addr&quot;: &quot;10.20.10.13:6804&#x2F;622620898&quot;,</span><br><span class="line">				&quot;standby_for_rank&quot;: 0,</span><br><span class="line">				&quot;standby_for_fscid&quot;: -1,</span><br><span class="line">				&quot;standby_for_name&quot;: &quot;&quot;,</span><br><span class="line">				&quot;standby_replay&quot;: true,</span><br><span class="line">				&quot;export_targets&quot;: [],</span><br><span class="line">				&quot;features&quot;: 4611087853746454523</span><br><span class="line">			&#125;,</span><br><span class="line">			&quot;gid_4456&quot;: &#123;</span><br><span class="line">				&quot;gid&quot;: 4456,</span><br><span class="line">				&quot;name&quot;: &quot;ceph3&quot;,</span><br><span class="line">				&quot;rank&quot;: 0,</span><br><span class="line">				&quot;incarnation&quot;: 0,</span><br><span class="line">				&quot;state&quot;: &quot;up:standby-replay&quot;,</span><br><span class="line">				&quot;state_seq&quot;: 2,</span><br><span class="line">				&quot;addr&quot;: &quot;10.20.10.25:6805&#x2F;1639008809&quot;,</span><br><span class="line">				&quot;standby_for_rank&quot;: 0,</span><br><span class="line">				&quot;standby_for_fscid&quot;: -1,</span><br><span class="line">				&quot;standby_for_name&quot;: &quot;&quot;,</span><br><span class="line">				&quot;standby_replay&quot;: true,</span><br><span class="line">				&quot;export_targets&quot;: [],</span><br><span class="line">				&quot;features&quot;: 4611087853746454523</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;data_pools&quot;: [6],</span><br><span class="line">		&quot;metadata_pool&quot;: 7,</span><br><span class="line">		&quot;enabled&quot;: true,</span><br><span class="line">		&quot;fs_name&quot;: &quot;cephfs&quot;,</span><br><span class="line">		&quot;balancer&quot;: &quot;&quot;,</span><br><span class="line">		&quot;standby_count_wanted&quot;: 1</span><br><span class="line">	&#125;,</span><br><span class="line">	&quot;id&quot;: 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="blacklist相关配置"><a href="#blacklist相关配置" class="headerlink" title="blacklist相关配置"></a>blacklist相关配置</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">MON相关配置</span><br><span class="line">	客户端blacklist entries保留在OSD map中的持续时间（以秒为单位）</span><br><span class="line">    Option(&quot;mon_osd_blacklist_default_expire&quot;, Option::TYPE_FLOAT, Option::LEVEL_ADVANCED)</span><br><span class="line">    .set_default(1_hr)</span><br><span class="line">    .add_service(&quot;mon&quot;)</span><br><span class="line">    .set_description(&quot;Duration in seconds that blacklist entries for clients remain in the OSD map&quot;),</span><br><span class="line"></span><br><span class="line">	MDS daemons的blacklist entries保留在OSD map中的持续时间（以秒为单位）</span><br><span class="line">    Option(&quot;mon_mds_blacklist_interval&quot;, Option::TYPE_FLOAT, Option::LEVEL_DEV)</span><br><span class="line">    .set_default(1_day)</span><br><span class="line">    .set_min(1_hr)</span><br><span class="line">    .add_service(&quot;mon&quot;)</span><br><span class="line">    .set_description(&quot;Duration in seconds that blacklist entries for MDS daemons remain in the OSD map&quot;),</span><br><span class="line"></span><br><span class="line">RBD相关配置</span><br><span class="line">	是否将损坏锁的客户端列入blacklist</span><br><span class="line">    Option(&quot;rbd_blacklist_on_break_lock&quot;, Option::TYPE_BOOL, Option::LEVEL_ADVANCED)</span><br><span class="line">    .set_default(true)</span><br><span class="line">    .set_description(&quot;whether to blacklist clients whose lock was broken&quot;),</span><br><span class="line"></span><br><span class="line">	blacklist的秒数 - OSD 默认值为 0</span><br><span class="line">    Option(&quot;rbd_blacklist_expire_seconds&quot;, Option::TYPE_UINT, Option::LEVEL_ADVANCED)</span><br><span class="line">    .set_default(0)</span><br><span class="line">    .set_description(&quot;number of seconds to blacklist - set to 0 for OSD default&quot;),</span><br><span class="line"></span><br><span class="line">MDS相关配置</span><br><span class="line">	是否将sessions已过期的客户端列入blacklist</span><br><span class="line">    Option(&quot;mds_session_blacklist_on_timeout&quot;, Option::TYPE_BOOL, Option::LEVEL_ADVANCED)</span><br><span class="line">    .set_default(true)</span><br><span class="line">    .set_description(&quot;blacklist clients whose sessions have become stale&quot;),</span><br><span class="line"></span><br><span class="line">	是否将被逐出的客户端列入blacklist</span><br><span class="line">    Option(&quot;mds_session_blacklist_on_evict&quot;, Option::TYPE_BOOL, Option::LEVEL_ADVANCED)</span><br><span class="line">    .set_default(true)</span><br><span class="line">    .set_description(&quot;blacklist clients that have been evicted&quot;),</span><br><span class="line"></span><br><span class="line">	数秒后，没有响应MDS的“cap revoke messages”的客户端将被驱逐。（默认为0，表示关闭该功能）</span><br><span class="line">    Option(&quot;mds_cap_revoke_eviction_timeout&quot;, Option::TYPE_FLOAT, Option::LEVEL_ADVANCED)</span><br><span class="line">     .set_default(0)</span><br><span class="line">     .set_description(&quot;number of seconds after which clients which have not responded to cap revoke messages by the MDS are evicted.&quot;),</span><br><span class="line"></span><br><span class="line">	MDS重新连接恢复状态期间等待客户端重新连接的超时时间（以秒为单位）</span><br><span class="line">    Option(&quot;mds_reconnect_timeout&quot;, Option::TYPE_FLOAT, Option::LEVEL_ADVANCED)</span><br><span class="line">    .set_default(45)</span><br><span class="line">    .set_description(&quot;timeout in seconds to wait for clients to reconnect during MDS reconnect recovery state&quot;),</span><br></pre></td></tr></table></figure>


<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>【1】<a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/3944931">https://access.redhat.com/solutions/3944931</a></p>
<p>【2】<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/mimic/cephfs/eviction/">https://docs.ceph.com/docs/mimic/cephfs/eviction/</a></p>
<h1 id="接口（CLI后加–format-json可以以json格式输出结果）"><a href="#接口（CLI后加–format-json可以以json格式输出结果）" class="headerlink" title="接口（CLI后加–format json可以以json格式输出结果）"></a>接口（CLI后加–format json可以以json格式输出结果）</h1><p>1、添加客户端到blacklist（add (optionally until <code>&lt;expire&gt; </code>seconds from now)<code>&lt;addr&gt;</code>from blacklist）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">osd blacklist add &lt;EntityAddr&gt; &#123;&lt;float[0.0-]&gt;&#125;</span><br></pre></td></tr></table></figure>
<p>2、从blacklist中删除客户端（remove <code>&lt;addr&gt;</code> from blacklist）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">osd blacklist rm &lt;EntityAddr&gt;</span><br></pre></td></tr></table></figure>
<p>3、清除所有列入blacklist的客户端（clear all blacklisted clients）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">osd blacklist clear</span><br></pre></td></tr></table></figure>
<p>4、显示列入blacklist的客户端（show blacklisted clients）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">osd blacklist ls</span><br></pre></td></tr></table></figure>
<p>5、设置session_autoclose，客户端在指定秒数未与MDS通信，则驱逐接口，加入blacklist（set fs parameter <code>&lt;var&gt; </code>to <code>&lt;val&gt;</code>）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fs set &lt;fs_name&gt; session_autoclose &lt;val&gt;</span><br></pre></td></tr></table></figure>
<p>6、获取有关一个文件系统的session_autoclose信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph fs get &lt;fs_name&gt;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/10/29/udev%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/29/udev%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86/" class="post-title-link" itemprop="url">udev设备管理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-29 15:19:32" itemprop="dateCreated datePublished" datetime="2019-10-29T15:19:32+08:00">2019-10-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-22 16:16:36" itemprop="dateModified" datetime="2020-03-22T16:16:36+08:00">2020-03-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Linuxcast学习笔记，视频地址：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=eV4InZop--0">https://www.youtube.com/watch?v=eV4InZop--0</a></p>
<h1 id="udev是什么"><a href="#udev是什么" class="headerlink" title="udev是什么"></a>udev是什么</h1><p>udev是动态管理设备的机制（/dev/目录下的设备）。udev允许我们自己写一些rule配置文件来控制udev默认的行为动作。默认配置文件在/etc/udev/目录下，在/etc/udev/rules.d/下为默认rule。如70-persistent-net.rules文件，udev在工作时，每次检查/etc/udev/rules.d/目录下的配置文件，并且按照数字的顺序来加载并应用这些配置。</p>
<p>udev允许我们在一个设备连接到计算机的时候，或者已经连接上，或者卸载的时候执行一些特殊的动作。（设备连接时、设备连接上、设备断开时）</p>
<h1 id="如何使用udev修改设备默认名称"><a href="#如何使用udev修改设备默认名称" class="headerlink" title="如何使用udev修改设备默认名称"></a>如何使用udev修改设备默认名称</h1><p>如果我们想修改一个设备，那么我们需要唯一的定位一个设备，通过udevadm命令。通常serial（串号）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph2 ~]# udevadm info -a -n &#x2F;dev&#x2F;vdc</span><br><span class="line"></span><br><span class="line">Udevadm info starts with the device specified by the devpath and then</span><br><span class="line">walks up the chain of parent devices. It prints for every device</span><br><span class="line">found, all possible attributes in the udev rules key format.</span><br><span class="line">A rule to match, can be composed by the attributes of the device</span><br><span class="line">and the attributes from one single parent device.</span><br><span class="line"></span><br><span class="line">  looking at device &#39;&#x2F;devices&#x2F;pci0000:00&#x2F;0000:00:09.0&#x2F;virtio4&#x2F;block&#x2F;vdc&#39;:</span><br><span class="line">    KERNEL&#x3D;&#x3D;&quot;vdc&quot;</span><br><span class="line">    SUBSYSTEM&#x3D;&#x3D;&quot;block&quot;</span><br><span class="line">    DRIVER&#x3D;&#x3D;&quot;&quot;</span><br><span class="line">    ATTR&#123;ro&#125;&#x3D;&#x3D;&quot;0&quot;</span><br><span class="line">    ATTR&#123;size&#125;&#x3D;&#x3D;&quot;104857600&quot;</span><br><span class="line">    ATTR&#123;stat&#125;&#x3D;&#x3D;&quot;    7223       11   474136    19244   400088    30660  1866945   871549        0   840441   872404&quot;</span><br><span class="line">    ATTR&#123;cache_type&#125;&#x3D;&#x3D;&quot;write back&quot;</span><br><span class="line">    ATTR&#123;range&#125;&#x3D;&#x3D;&quot;16&quot;</span><br><span class="line">    ATTR&#123;discard_alignment&#125;&#x3D;&#x3D;&quot;0&quot;</span><br><span class="line">    ATTR&#123;ext_range&#125;&#x3D;&#x3D;&quot;256&quot;</span><br><span class="line">    ATTR&#123;serial&#125;&#x3D;&#x3D;&quot;e850ae75-fcb2-4432-a&quot;</span><br><span class="line">    ATTR&#123;alignment_offset&#125;&#x3D;&#x3D;&quot;0&quot;</span><br><span class="line">    ATTR&#123;inflight&#125;&#x3D;&#x3D;&quot;       0        0&quot;</span><br><span class="line">    ATTR&#123;removable&#125;&#x3D;&#x3D;&quot;0&quot;</span><br><span class="line">    ATTR&#123;capability&#125;&#x3D;&#x3D;&quot;50&quot;</span><br><span class="line"></span><br><span class="line">  looking at parent device &#39;&#x2F;devices&#x2F;pci0000:00&#x2F;0000:00:09.0&#x2F;virtio4&#39;:</span><br><span class="line">    KERNELS&#x3D;&#x3D;&quot;virtio4&quot;</span><br><span class="line">    SUBSYSTEMS&#x3D;&#x3D;&quot;virtio&quot;</span><br><span class="line">    DRIVERS&#x3D;&#x3D;&quot;virtio_blk&quot;</span><br><span class="line">    ATTRS&#123;device&#125;&#x3D;&#x3D;&quot;0x0002&quot;</span><br><span class="line">    ATTRS&#123;features&#125;&#x3D;&#x3D;&quot;0010101001110000000000000000110010000000000000000000000000000000&quot;</span><br><span class="line">    ATTRS&#123;status&#125;&#x3D;&#x3D;&quot;0x0000000f&quot;</span><br><span class="line">    ATTRS&#123;vendor&#125;&#x3D;&#x3D;&quot;0x1af4&quot;</span><br><span class="line"></span><br><span class="line">  looking at parent device &#39;&#x2F;devices&#x2F;pci0000:00&#x2F;0000:00:09.0&#39;:</span><br><span class="line">    KERNELS&#x3D;&#x3D;&quot;0000:00:09.0&quot;</span><br><span class="line">    SUBSYSTEMS&#x3D;&#x3D;&quot;pci&quot;</span><br><span class="line">    DRIVERS&#x3D;&#x3D;&quot;virtio-pci&quot;</span><br><span class="line">    ATTRS&#123;irq&#125;&#x3D;&#x3D;&quot;10&quot;</span><br><span class="line">    ATTRS&#123;subsystem_vendor&#125;&#x3D;&#x3D;&quot;0x1af4&quot;</span><br><span class="line">    ATTRS&#123;broken_parity_status&#125;&#x3D;&#x3D;&quot;0&quot;</span><br><span class="line">    ATTRS&#123;class&#125;&#x3D;&#x3D;&quot;0x010000&quot;</span><br><span class="line">    ATTRS&#123;driver_override&#125;&#x3D;&#x3D;&quot;(null)&quot;</span><br><span class="line">    ATTRS&#123;consistent_dma_mask_bits&#125;&#x3D;&#x3D;&quot;64&quot;</span><br><span class="line">    ATTRS&#123;dma_mask_bits&#125;&#x3D;&#x3D;&quot;64&quot;</span><br><span class="line">    ATTRS&#123;local_cpus&#125;&#x3D;&#x3D;&quot;f&quot;</span><br><span class="line">    ATTRS&#123;device&#125;&#x3D;&#x3D;&quot;0x1001&quot;</span><br><span class="line">    ATTRS&#123;enable&#125;&#x3D;&#x3D;&quot;1&quot;</span><br><span class="line">    ATTRS&#123;msi_bus&#125;&#x3D;&#x3D;&quot;&quot;</span><br><span class="line">    ATTRS&#123;local_cpulist&#125;&#x3D;&#x3D;&quot;0-3&quot;</span><br><span class="line">    ATTRS&#123;vendor&#125;&#x3D;&#x3D;&quot;0x1af4&quot;</span><br><span class="line">    ATTRS&#123;subsystem_device&#125;&#x3D;&#x3D;&quot;0x0002&quot;</span><br><span class="line">    ATTRS&#123;numa_node&#125;&#x3D;&#x3D;&quot;-1&quot;</span><br><span class="line">    ATTRS&#123;d3cold_allowed&#125;&#x3D;&#x3D;&quot;0&quot;</span><br><span class="line"></span><br><span class="line">  looking at parent device &#39;&#x2F;devices&#x2F;pci0000:00&#39;:</span><br><span class="line">    KERNELS&#x3D;&#x3D;&quot;pci0000:00&quot;</span><br><span class="line">    SUBSYSTEMS&#x3D;&#x3D;&quot;&quot;</span><br><span class="line">    DRIVERS&#x3D;&#x3D;&quot;&quot;</span><br></pre></td></tr></table></figure>
<p> 在/etc/udev/rules.d/下创建rule，99-linuxcast.rules，编辑规则。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KERNEL&#x3D;&#x3D;&quot;sd*&quot;, ATTR&#123;serial&#125;&#x3D;&#x3D;&quot;e850ae75-fcb2-4432-a&quot;, NAME&#x3D;&quot;yujiangvdc%n&quot;</span><br></pre></td></tr></table></figure>
<p>KERNEL意思是内核识别出来这个设备是什么名字，两个等号==是做比较，一个等号=是赋值。</p>
<p>通过KERNEL与ATTR{serial}就可以唯一定位到一个设备，要把这个设备修改为其他名字用NAME，udev会自动把%n替换成分区号。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/10/25/ceph-L-to-N-%E5%8D%87%E7%BA%A7%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/25/ceph-L-to-N-%E5%8D%87%E7%BA%A7%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">ceph L to N 升级总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-25 10:07:08" itemprop="dateCreated datePublished" datetime="2019-10-25T10:07:08+08:00">2019-10-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-22 16:16:36" itemprop="dateModified" datetime="2020-03-22T16:16:36+08:00">2020-03-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/releases/nautilus/">官方原文</a></p>
<p><a target="_blank" rel="noopener" href="https://pve.proxmox.com/wiki/Ceph_Luminous_to_Nautilus">参考1</a></p>
<h3 id="从Luminous之前的版本进行升级（比如jewel版本）"><a href="#从Luminous之前的版本进行升级（比如jewel版本）" class="headerlink" title="从Luminous之前的版本进行升级（比如jewel版本）"></a>从Luminous之前的版本进行升级（比如jewel版本）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">必须先升级到Luminous（12.2.z），然后再尝试升级到Nautilus。 另外，您的集群必须在运行Luminous的同时至少完成了所有PG的一次scrub，并在OSD map中设置了recovery_deletes和purged_snapdirs flags。</span><br><span class="line"></span><br><span class="line">[root@ceph1 ~]# ceph osd dump | grep ^flags</span><br><span class="line">flags sortbitwise,recovery_deletes,purged_snapdirs</span><br><span class="line"></span><br><span class="line">recovery_deletes flags：Ceph现在在recovery过程中处理delete操作，而不是在peering过程中。以前，将down或out超过15分钟的OSD带回到群集中会导致placement group peering时间延长。peering过程需要很长时间才能完成，因为delete操作是在合并placement group日志作为peering过程的一部分时内联处理的。结果，对处于peering状态的placement group的操作被blocked。通过此更新，Ceph可以在正常recovery过程中而不是peering过程中处理delete操作。结果可以使peering过程更快完成，并且操作不再blocked。（This was fixed with the help of Red Hat Ceph Storage feature request #1452780 and this was released in 2.4 errata version 10.2.7-32.el7cp.）</span><br><span class="line"></span><br><span class="line">purged_snapdirs flags：一旦snapsets全部转换，则设置purged_snapdirs OSDMap flags，这样可以更轻松地测试upgrade + conversion是否已完成。特别是，micim+将能够更简单地测试该flags，而无需等待完整的PG统计信息来知道升级到luminous以外的版本是否安全。</span><br></pre></td></tr></table></figure>
<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、在从Luminous升级到Nautilus的过程中，将monitors升级到Nautilus后，将无法使用Luminous ceph-osd daemon创建新的OSD。避免在升级过程中添加或替换任何OSD。</span><br><span class="line">2、避免在升级过程中创建任何RADOS pools。</span><br><span class="line">3、您可以使用ceph version(s)命令在每个阶段监视升级进度，该命令将告诉您每种ceph daemon正在运行的ceph版本。</span><br></pre></td></tr></table></figure>
<h1 id="自研管理平台用到的接口，对比输出是否有修改，以免影响前台功能"><a href="#自研管理平台用到的接口，对比输出是否有修改，以免影响前台功能" class="headerlink" title="自研管理平台用到的接口，对比输出是否有修改，以免影响前台功能"></a>自研管理平台用到的接口，对比输出是否有修改，以免影响前台功能</h1><h1 id="Ceph节点升级过程"><a href="#Ceph节点升级过程" class="headerlink" title="Ceph节点升级过程"></a>Ceph节点升级过程</h1><p>1、确认OSD map包含recovery_deletes和purged_snapdirs flags（否则，将导致您的monitor daemons在启动时拒绝加入quorum，从而使其无法运行）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">执行命令：ceph osd dump | grep ^flags</span><br><span class="line">命令输出：flags sortbitwise,recovery_deletes,purged_snapdirs</span><br><span class="line"></span><br><span class="line">如果没有recovery_deletes和purged_snapdirs flags需要手动触发pg scrub，并等待大约24-48小时（根据数据量评估）</span><br><span class="line">执行命令：ceph pg dump pgs_brief | cut -d &quot; &quot; -f 1 | xargs -n1 ceph pg scrub</span><br></pre></td></tr></table></figure>
<p>2、确保集群稳定，集群状态HEALTH_OK，没有down掉的OSD或无法恢复的OSD。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">执行命令：ceph health detail</span><br><span class="line">命令输出：HEALTH_OK</span><br><span class="line"></span><br><span class="line">执行命令：ceph osd tree | grep down</span><br><span class="line">命令输出：空</span><br><span class="line"></span><br><span class="line">执行命令：ceph osd tree | grep out</span><br><span class="line">命令输出：空</span><br></pre></td></tr></table></figure>
<p>3、设置noout flags</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">执行命令：ceph osd set noout</span><br><span class="line">命令输出：noout is set</span><br><span class="line"></span><br><span class="line">执行命令：ceph health detail</span><br><span class="line">命令输出：</span><br><span class="line">HEALTH_WARN noout flag(s) set</span><br><span class="line">OSDMAP_FLAGS noout flag(s) set</span><br></pre></td></tr></table></figure>
<p>4、配置centos ceph 14 Luminous mirror</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>5、升级monitors（通过安装新ceph packages并重新启动monitor daemons来升级monitors）</p>
<p>在<strong>每个monitors主机上</strong>执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">5.1、安装monitor packages</span><br><span class="line">执行命令：yum install ceph-mon</span><br><span class="line">命令输出：提示安装ceph-mon以及依赖</span><br><span class="line"></span><br><span class="line">5.2、重启ceph monitor服务</span><br><span class="line">执行命令：systemctl restart ceph-mon.target</span><br><span class="line">命令输出：空</span><br></pre></td></tr></table></figure>
<p>monitors启动之后，查找nautilus字符串来验证monitor升级是否完成。确保有min_mon_release 14 (nautilus)字样，如果没有说明尚未升级和重启monitor。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">执行命令：ceph mon dump | grep min_mon_release</span><br><span class="line">命令输出：</span><br><span class="line">dumped monmap epoch 2</span><br><span class="line">min_mon_release 14 (nautilus)</span><br></pre></td></tr></table></figure>
<p>6、升级ceph-mgr（通过安装新ceph packages并重新启动ceph-mgr daemons来升级ceph-mgr）</p>
<p>如果使用Ceph Dashboard需要安装ceph-mgr-dashboard package。</p>
<p>在<strong>每个ceph-mgr主机上</strong>执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">6.1、安装ceph-mgr packages</span><br><span class="line">执行命令：yum install ceph-mgr</span><br><span class="line">命令输出：提示安装ceph-mgr以及依赖</span><br><span class="line"></span><br><span class="line">6.2、安装ceph-mgr-dashboard packages</span><br><span class="line">执行命令：yum install ceph-mgr-dashboard</span><br><span class="line">命令输出：提示安装ceph-mgr-dashboard以及依赖</span><br><span class="line"></span><br><span class="line">6.3、重启ceph mgr服务</span><br><span class="line">执行命令：systemctl restart ceph-mgr.target</span><br><span class="line">命令输出：空</span><br></pre></td></tr></table></figure>
<p>通过ceph -s验证ceph-mgr daemons是否正在运行（确保mgr状态active，standbys mgr加入）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">执行命令：ceph -s</span><br><span class="line">命令输出：</span><br><span class="line">...</span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum community-ceph-2,community-ceph-3,community-ceph-1 (age 25m)</span><br><span class="line">    mgr: community-ceph-2(active, since 3d), standbys: community-ceph-3, community-ceph-1</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>7、升级ceph-osd（通过安装新ceph packages并重新启动ceph-osd daemons来升级ceph-osd）</p>
<p>在<strong>每个ceph-osd主机上</strong>执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">7.1、安装ceph-osd packages</span><br><span class="line">执行命令：yum install ceph-osd</span><br><span class="line">命令输出：提示安装ceph-osd以及依赖</span><br><span class="line"></span><br><span class="line">7.2、重启ceph osd服务</span><br><span class="line">执行命令：systemctl restart ceph-osd.target</span><br><span class="line">命令输出：空</span><br></pre></td></tr></table></figure>
<p>查看OSD升级的进度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">执行命令：ceph osd versions</span><br><span class="line">命令输出：</span><br><span class="line">&#123;</span><br><span class="line">   &quot;ceph version 12.2.x (...) luminous (stable)&quot;: 2,</span><br><span class="line">   &quot;ceph version 14.2.4 (...) nautilus (stable)&quot;: 4,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用ceph-volume接管ceph-disk创建的OSD，在<strong>每个OSD主机上</strong>执行，<strong>执行前确保每个OSD都正在运行，无down或out的OSD</strong>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">1、确保OSD运行正常</span><br><span class="line">执行命令：ceph osd tree | grep down</span><br><span class="line">命令输出：空</span><br><span class="line">执行命令：ceph osd tree | grep out</span><br><span class="line">命令输出：空</span><br><span class="line"></span><br><span class="line">2、所有使用ceph-disk创建的并正在运行的OSDs，从OSD data partition或directory中捕获元数据</span><br><span class="line">执行命令：ceph-volume simple scan</span><br><span class="line">命令输出：</span><br><span class="line"></span><br><span class="line">执行命令后，会生成类似&#x2F;etc&#x2F;ceph&#x2F;osd&#x2F;0-ab0a204a-42e3-4a47-ab4c-0888edf429cb.json文件，文件内容为：</span><br><span class="line">&#123;</span><br><span class="line">    &quot;active&quot;: &quot;ok&quot;, </span><br><span class="line">    &quot;block&quot;: &#123;</span><br><span class="line">        &quot;path&quot;: &quot;&#x2F;dev&#x2F;disk&#x2F;by-partuuid&#x2F;0818811f-d70e-4ff0-91c9-58cd701c9a19&quot;, </span><br><span class="line">        &quot;uuid&quot;: &quot;0818811f-d70e-4ff0-91c9-58cd701c9a19&quot;</span><br><span class="line">    &#125;, </span><br><span class="line">    &quot;block_uuid&quot;: &quot;0818811f-d70e-4ff0-91c9-58cd701c9a19&quot;, </span><br><span class="line">    &quot;bluefs&quot;: 1, </span><br><span class="line">    &quot;ceph_fsid&quot;: &quot;c4051efa-1997-43ef-8497-fb02bdf08233&quot;, </span><br><span class="line">    &quot;cluster_name&quot;: &quot;ceph&quot;, </span><br><span class="line">    &quot;data&quot;: &#123;</span><br><span class="line">        &quot;path&quot;: &quot;&#x2F;dev&#x2F;vdc1&quot;, </span><br><span class="line">        &quot;uuid&quot;: &quot;ab0a204a-42e3-4a47-ab4c-0888edf429cb&quot;</span><br><span class="line">    &#125;, </span><br><span class="line">    &quot;fsid&quot;: &quot;ab0a204a-42e3-4a47-ab4c-0888edf429cb&quot;, </span><br><span class="line">    &quot;keyring&quot;: &quot;AQB1FLFdXVHVARAARTKkxT1xgrDNU&#x2F;QECUqdxA&#x3D;&#x3D;&quot;, </span><br><span class="line">    &quot;kv_backend&quot;: &quot;rocksdb&quot;, </span><br><span class="line">    &quot;magic&quot;: &quot;ceph osd volume v026&quot;, </span><br><span class="line">    &quot;mkfs_done&quot;: &quot;yes&quot;, </span><br><span class="line">    &quot;ready&quot;: &quot;ready&quot;, </span><br><span class="line">    &quot;systemd&quot;: &quot;&quot;, </span><br><span class="line">    &quot;type&quot;: &quot;bluestore&quot;, </span><br><span class="line">    &quot;whoami&quot;: 0</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">3、使systemd units可以mount已配置的devices，并启动Ceph OSD</span><br><span class="line">执行命令：ceph-volume simple activate --all</span><br><span class="line">命令输出：</span><br><span class="line">--&gt; activating OSD specified in &#x2F;etc&#x2F;ceph&#x2F;osd&#x2F;1-fe327306-54a4-4362-870d-92d28cf65e42.json</span><br><span class="line">Running command: ln -snf &#x2F;dev&#x2F;vdc2 &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-1&#x2F;block</span><br><span class="line">Running command: chown -R ceph:ceph &#x2F;dev&#x2F;vdc2</span><br><span class="line">Running command: systemctl enable ceph-volume@simple-1-fe327306-54a4-4362-870d-92d28cf65e42</span><br><span class="line">Running command: ln -sf &#x2F;dev&#x2F;null &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;ceph-disk@.service</span><br><span class="line">--&gt; All ceph-disk systemd units have been disabled to prevent OSDs getting triggered by UDEV events</span><br><span class="line">Running command: systemctl enable --runtime ceph-osd@1</span><br><span class="line">Running command: systemctl start ceph-osd@1</span><br><span class="line">--&gt; Successfully activated OSD 1 with FSID fe327306-54a4-4362-870d-92d28cf65e42</span><br><span class="line"></span><br><span class="line">4、重启每个OSD主机，确认OSD是否开机自启</span><br><span class="line">执行命令：reboot</span><br><span class="line">命令输出：无</span><br></pre></td></tr></table></figure>
<p>8、升级ceph-mds（通过安装新ceph packages并重新启动ceph-mds daemons来升级ceph-mds）</p>
<p>在<strong>每个ceph-mds主机上</strong>执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">记录rank数量，并将ranks数减少到1，通过ceph mds stat命令输出可以查看rank数量。</span><br><span class="line">&#123;0&#x3D;ceph3&#x3D;up:active&#125;代表rank数为1，&#123;0&#x3D;ceph3&#x3D;up:active,1&#x3D;ceph2&#x3D;up:active&#125;代表rank数为2，以此类推。</span><br><span class="line"></span><br><span class="line">8.1、查看当前rank数量</span><br><span class="line">执行命令：ceph mds stat</span><br><span class="line">命令输出：cephfs-1&#x2F;1&#x2F;1 up  &#123;0&#x3D;ceph3&#x3D;up:active&#125;, 2 up:standby</span><br><span class="line">如果rank为2，命令输出为：cephfs-2&#x2F;2&#x2F;2 up  &#123;0&#x3D;ceph3&#x3D;up:active,1&#x3D;ceph2&#x3D;up:active&#125;, 1 up:standby</span><br><span class="line"></span><br><span class="line">8.2、查看cephfs名称</span><br><span class="line">执行命令：ceph fs ls</span><br><span class="line">命令输出：name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</span><br><span class="line"></span><br><span class="line">8.3、将ranks数减少到1</span><br><span class="line">执行命令：ceph fs set &lt;fs_name&gt; max_mds 1</span><br><span class="line">命令输出：空</span><br><span class="line"></span><br><span class="line">8.4、通过定期检查状态，等待集群停用所有non-zero ranks，当&#123;0&#x3D;ceph3&#x3D;up:active&#125;时为已停用所有non-zero ranks</span><br><span class="line">执行命令：ceph status</span><br><span class="line">命令输出：cephfs-1&#x2F;1&#x2F;1 up  &#123;0&#x3D;ceph3&#x3D;up:active&#125;, 2 up:standby</span><br><span class="line"></span><br><span class="line">8.5、使用以下命令使所有standby MDS daemons在适当的主机offline</span><br><span class="line">执行命令：systemctl stop ceph-mds@&lt;daemon_name&gt;</span><br><span class="line">命令输出：空</span><br><span class="line"></span><br><span class="line">8.6、确认只有一个MDS处于online，并且cephfs只有一个rank 0</span><br><span class="line">执行命令：ceph status</span><br><span class="line">命令输出：</span><br><span class="line">...</span><br><span class="line">    mds: cephfs-1&#x2F;1&#x2F;1 up  &#123;0&#x3D;ceph3&#x3D;up:active&#125;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">8.7、通过安装新packages并重新启动daemon来升级MDS daemon（在每个mds节点上执行）</span><br><span class="line">执行命令：yum install ceph-mds</span><br><span class="line">命令输出：提示安装ceph-mds以及依赖</span><br><span class="line"></span><br><span class="line">执行命令：systemctl restart ceph-mds.target</span><br><span class="line">命令输出：空</span><br><span class="line"></span><br><span class="line">8.8、重新启动所有已的offline standby MDS daemons</span><br><span class="line">执行命令：systemctl restart ceph-mds.target</span><br><span class="line">命令输出：空</span><br><span class="line"></span><br><span class="line">8.9、恢复max_mds原始值</span><br><span class="line">执行命令：ceph fs set &lt;fs_name&gt; max_mds &lt;original_max_mds&gt;</span><br><span class="line">命令输出：空</span><br></pre></td></tr></table></figure>
<p>9、升级ceph-radosgw（通过安装新ceph packages并重新启动ceph-radosgw daemons来升级ceph-radosgw）</p>
<p>在<strong>每个ceph-radosgw主机上</strong>执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">5.1、安装radosgw packages</span><br><span class="line">执行命令：yum install ceph-radosgw</span><br><span class="line">命令输出：提示安装ceph-radosgw以及依赖</span><br><span class="line"></span><br><span class="line">5.2、重启ceph radosgw服务</span><br><span class="line">执行命令：systemctl restart ceph-radosgw.target</span><br><span class="line">命令输出：空</span><br></pre></td></tr></table></figure>
<p>10、启用所有Nautilus的新功能来完成升级</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">执行命令：ceph osd require-osd-release nautilus</span><br><span class="line">命令输出：空</span><br></pre></td></tr></table></figure>
<p>11、清除noout flags</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">执行命令：ceph osd unset noout</span><br><span class="line">命令输出：noout is unset</span><br></pre></td></tr></table></figure>
<p>12、启用新的 v2 network protocol</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">执行命令：ceph mon enable-msgr2</span><br><span class="line">命令输出：空</span><br></pre></td></tr></table></figure>
<h1 id="Client节点升级过程"><a href="#Client节点升级过程" class="headerlink" title="Client节点升级过程"></a>Client节点升级过程</h1><p>1、Client节点升级package</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">执行命令：yum install ceph-common librados2 librbd1 python-rbd python-rados -y</span><br><span class="line">命令输出：提示安装ceph-common librados2 librbd1 python-rbd python-rados以及依赖</span><br></pre></td></tr></table></figure>
<p>2、Client节点确认升级后的版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">执行命令：ceph --version</span><br><span class="line">命令输出：ceph version 14.2.4 (75f4de193b3ea58512f204623e6c5a16e6c1e1ba) nautilus (stable)</span><br></pre></td></tr></table></figure>
<h1 id="Ceph端升级后修复"><a href="#Ceph端升级后修复" class="headerlink" title="Ceph端升级后修复"></a>Ceph端升级后修复</h1><h3 id="1、Legacy-BlueStore-stats-reporting-detected-on-6-OSD-s"><a href="#1、Legacy-BlueStore-stats-reporting-detected-on-6-OSD-s" class="headerlink" title="1、Legacy BlueStore stats reporting detected on 6 OSD(s)"></a>1、Legacy BlueStore stats reporting detected on 6 OSD(s)</h3><p>问题描述：</p>
<p>​    使用ceph -s命令查看集群状态时，出现Legacy BlueStore stats reporting detected on 6 OSD(s)</p>
<p>解决办法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop ceph-osd@$OSDID</span><br><span class="line">ceph-bluestore-tool repair –path &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-$OSDID</span><br><span class="line">systemctl start ceph-osd@$OSDID</span><br></pre></td></tr></table></figure>
<p>参考资料：</p>
<p>【1】<a target="_blank" rel="noopener" href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/036002.html">http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/036002.html</a></p>
<p>关于执行ceph-bluestore-tool repair报错问题确认：<a target="_blank" rel="noopener" href="https://tracker.ceph.com/issues/42297">https://tracker.ceph.com/issues/42297</a></p>
<h3 id="2、3-monitors-have-not-enabled-msgr2"><a href="#2、3-monitors-have-not-enabled-msgr2" class="headerlink" title="2、3 monitors have not enabled msgr2"></a>2、3 monitors have not enabled msgr2</h3><p>问题描述：</p>
<p>​    使用ceph -s命令查看集群状态时，出现 3 monitors have not enabled msgr2</p>
<p>解决办法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph mon enable-msgr2</span><br><span class="line">systemctl restart ceph-mon@ceph1.service</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/10/21/samba/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/21/samba/" class="post-title-link" itemprop="url">samba</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-21 19:26:56" itemprop="dateCreated datePublished" datetime="2019-10-21T19:26:56+08:00">2019-10-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-22 16:16:36" itemprop="dateModified" datetime="2020-03-22T16:16:36+08:00">2020-03-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>系统环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat &#x2F;etc&#x2F;redhat-release </span><br><span class="line">CentOS Linux release 7.4.1708 (Core) </span><br></pre></td></tr></table></figure>
<h1 id="samba安装"><a href="#samba安装" class="headerlink" title="samba安装"></a>samba安装</h1><p>安装samba</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install -y samba</span><br><span class="line"></span><br><span class="line">samba Version 4.9.1</span><br></pre></td></tr></table></figure>
<h1 id="Ceph-KRBD使用samba"><a href="#Ceph-KRBD使用samba" class="headerlink" title="Ceph KRBD使用samba"></a>Ceph KRBD使用samba</h1><p>1、首先创建pool</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create rbd 8</span><br></pre></td></tr></table></figure>
<p>2、在pool中创建rbd</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">rbd create --size 10G rbd&#x2F;rbd-1</span><br><span class="line"></span><br><span class="line"># rbd info rbd&#x2F;rbd-1</span><br><span class="line">rbd image &#39;rbd-1&#39;:</span><br><span class="line">	size 10GiB in 2560 objects</span><br><span class="line">	order 22 (4MiB objects)</span><br><span class="line">	block_name_prefix: rbd_data.10c06b8b4567</span><br><span class="line">	format: 2</span><br><span class="line">	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">	flags: </span><br><span class="line">	create_timestamp: Wed Oct 23 10:04:16 2019</span><br></pre></td></tr></table></figure>
<p>3、在linux上创建挂载点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir ceph-rbd-1-mountpoint</span><br></pre></td></tr></table></figure>
<p>4、map pool中的rbd到linux</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">rbd map rbd&#x2F;rbd-1</span><br><span class="line"></span><br><span class="line"># lsmod | grep rbd</span><br><span class="line">rbd                    83733  2 </span><br><span class="line">libceph               306742  1 rbd</span><br><span class="line"></span><br><span class="line"># sudo rbd showmapped</span><br><span class="line">id pool image snap device    </span><br><span class="line">0  rbd  rbd-1 -    &#x2F;dev&#x2F;rbd0</span><br></pre></td></tr></table></figure>
<p>5、格式化文件系统</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkfs.xfs &#x2F;dev&#x2F;rbd&#x2F;rbd&#x2F;rbd-1</span><br></pre></td></tr></table></figure>
<p>6、mount到挂载点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mount &#x2F;dev&#x2F;rbd&#x2F;rbd&#x2F;rbd-1 &#x2F;root&#x2F;ceph-rbd-1-mountpoint&#x2F;</span><br><span class="line"></span><br><span class="line"># df -Th</span><br><span class="line">Filesystem     Type      Size  Used Avail Use% Mounted on</span><br><span class="line">&#x2F;dev&#x2F;rbd0      xfs        10G   33M   10G   1% &#x2F;root&#x2F;ceph-rbd-1-mountpoint</span><br></pre></td></tr></table></figure>
<p>7、samba预操作（关闭防火墙）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">setenforce 0</span><br><span class="line"></span><br><span class="line">持久化关闭selinux</span><br><span class="line">vim &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">SELINUX&#x3D;disabled</span><br></pre></td></tr></table></figure>
<p>8、配置samba（samba默认会共享linux home目录，如果自定义需要自己配置），并重启服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;samba&#x2F;smb.conf</span><br><span class="line"></span><br><span class="line">[global]</span><br><span class="line">        workgroup &#x3D; SAMBA</span><br><span class="line">        security &#x3D; user</span><br><span class="line">        passdb backend &#x3D; tdbsam</span><br><span class="line">        printing &#x3D; cups</span><br><span class="line">        printcap name &#x3D; cups</span><br><span class="line">        load printers &#x3D; yes</span><br><span class="line">        cups options &#x3D; raw</span><br><span class="line"></span><br><span class="line">[homes]</span><br><span class="line">        comment &#x3D; Home Directories</span><br><span class="line">        valid users &#x3D; %S, %D%w%S</span><br><span class="line">        browseable &#x3D; No</span><br><span class="line">        read only &#x3D; No</span><br><span class="line">        inherit acls &#x3D; Yes</span><br><span class="line">        writable &#x3D; Yes</span><br><span class="line"></span><br><span class="line">[ceph]</span><br><span class="line">        comment &#x3D; ceph</span><br><span class="line">        path &#x3D; &#x2F;root&#x2F;ceph-rbd-1-mountpoint&#x2F;</span><br><span class="line">        valid users &#x3D; %S, %D%w%S</span><br><span class="line">        browseable &#x3D; No</span><br><span class="line">        read only &#x3D; No</span><br><span class="line">        inherit acls &#x3D; Yes</span><br><span class="line">        writable &#x3D; Yes</span><br><span class="line"></span><br><span class="line">[printers]</span><br><span class="line">        comment &#x3D; All Printers</span><br><span class="line">        path &#x3D; &#x2F;var&#x2F;tmp</span><br><span class="line">        printable &#x3D; Yes</span><br><span class="line">        create mask &#x3D; 0600</span><br><span class="line">        browseable &#x3D; No</span><br><span class="line"></span><br><span class="line">[print$]</span><br><span class="line">        comment &#x3D; Printer Drivers</span><br><span class="line">        path &#x3D; &#x2F;var&#x2F;lib&#x2F;samba&#x2F;drivers</span><br><span class="line">        write list &#x3D; @printadmin root</span><br><span class="line">        force group &#x3D; @printadmin</span><br><span class="line">        create mask &#x3D; 0664</span><br><span class="line">        directory mask &#x3D; 0775</span><br><span class="line"></span><br><span class="line">systemctl restart smb</span><br></pre></td></tr></table></figure>
<p>9、为samba添加ceph用户</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">smbpasswd -a ceph</span><br></pre></td></tr></table></figure>
<p>10、客户端访问samba</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\\10.20.10.23\ceph</span><br></pre></td></tr></table></figure>
<p>window清除samba连接，重新登录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\JiangYu&gt;net use</span><br><span class="line">会记录新的网络连接。</span><br><span class="line">状态       本地        远程                      网络</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">OK                     \\10.20.10.23\IPC$        Microsoft Windows Network</span><br><span class="line">命令成功完成。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">C:\Users\JiangYu&gt;net use \\10.20.10.23\IPC$ &#x2F;del</span><br><span class="line">\\10.20.10.23\IPC$ 已经删除。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">C:\Users\JiangYu&gt;net use</span><br><span class="line">会记录新的网络连接。</span><br><span class="line">列表是空的。</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/lnsyyj/lnsyyj.github.io/hexo/Blog/source/_posts/samba/linuxcast-smb-%E7%99%BB%E5%BD%95%E5%90%8E%E6%9F%A5%E7%9C%8B%E6%96%87%E4%BB%B6-1.png"></p>
<p><img src="https://raw.githubusercontent.com/lnsyyj/lnsyyj.github.io/hexo/Blog/source/_posts/samba/linuxcast-smb-%E7%99%BB%E5%BD%95%E5%90%8E%E6%9F%A5%E7%9C%8B%E6%96%87%E4%BB%B6-2.png"></p>
<p>11、因为是使用root用户创建的ceph-rbd-1-mountpoint目录，所以ceph用户没有权限对这个目录进行写操作，需要修改目录所有者，修改后就可以对这个目录进行创建文件等写操作了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown ceph:ceph ceph-rbd-1-mountpoint&#x2F;</span><br></pre></td></tr></table></figure>
<h1 id="samba配置"><a href="#samba配置" class="headerlink" title="samba配置"></a>samba配置</h1><p>1、查看配置文件帮助手册</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">man smb.conf 5</span><br></pre></td></tr></table></figure>
<p>2、smb.conf配置文件语法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">;或#开头为注释</span><br></pre></td></tr></table></figure>
<p>3、检查配置文件语法命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">testparm &#x2F;etc&#x2F;samba&#x2F;smb.conf</span><br></pre></td></tr></table></figure>
<p>4、Security-Enhanced Linux (SELinux) Notes</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">启用samba_domain_controller Boolean以允许Samba PDC使用useradd和groupadd二进制。以root用户身份运行以下命令以启用此Boolean：</span><br><span class="line">setsebool -P samba_domain_controller on</span><br><span class="line"></span><br><span class="line">如果要通过Samba共享home directories，请打开samba_enable_home_dirs Boolean。以root用户身份运行以下命令以启用此Boolean：</span><br><span class="line">setsebool -P samba_enable_home_dirs on</span><br><span class="line"></span><br><span class="line">如果创建新directories，例如新的顶级directories，请使用samba_share_t对其进行标记，以便SELinux允许Samba对其进行读写。不要使用samba_share_t标记系统目录，例如&#x2F;etc&#x2F;和&#x2F;home&#x2F;，因为这样的目录应该已经具有SELinux label了。</span><br><span class="line"></span><br><span class="line">运行&quot;ls -ldZ &#x2F;path&#x2F;to&#x2F;directory&quot;命令以查看给定目录的当前SELinux label。</span><br><span class="line"></span><br><span class="line">仅在创建的文件和目录上设置SELinux labels。使用chcon命令临时更改label：</span><br><span class="line">chcon -t samba_share_t &#x2F;path&#x2F;to&#x2F;directory</span><br><span class="line"></span><br><span class="line">重新标记文件系统或运行诸如restorecon之类的命令时，通过chcon进行的更改将丢失。</span><br><span class="line"></span><br><span class="line">使用samba_export_all_ro或samba_export_all_rw Boolean共享系统目录。 </span><br><span class="line">要共享这样的目录并仅允许只读权限：</span><br><span class="line">setsebool -P samba_export_all_ro on</span><br><span class="line">要共享此类目录并允许读写权限：</span><br><span class="line">setsebool -P samba_export_all_rw on</span><br><span class="line"></span><br><span class="line">要运行脚本（preexec&#x2F;root prexec&#x2F;print command&#x2F;...），请将它们复制到&#x2F;var&#x2F;lib&#x2F;samba&#x2F;scripts&#x2F;目录中，以便SELinux允许smbd运行它们。</span><br><span class="line">注意，如果将脚本移动到&#x2F;var&#x2F;lib&#x2F;samba&#x2F;scripts&#x2F;，它们将保留其现有的SELinux labels，这些labels可能是SELinux不允许smbd运行的labels。复制脚本将得到正确的SELinux labels。</span><br><span class="line">以root用户身份运行&quot;restorecon -R -v &#x2F;var&#x2F;lib&#x2F;samba&#x2F;scripts&quot;命令，以将正确的SELinux labels应用于这些文件。</span><br></pre></td></tr></table></figure>
<p>5、配置文件段</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br></pre></td><td class="code"><pre><span class="line">#&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; Global Settings &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">[global]</span><br><span class="line"></span><br><span class="line"># ----------------------- Network-Related Options -------------------------</span><br><span class="line">#</span><br><span class="line"># workgroup &#x3D; Windows NT domain name 或 workgroup name, 例如, MYGROUP.</span><br><span class="line">#</span><br><span class="line"># server string &#x3D; 等效于Windows NT Description字段（用于描述）.</span><br><span class="line">#</span><br><span class="line"># netbios name &#x3D; 用于指定不与hostname绑定的server name，最多15个字符。</span><br><span class="line">#</span><br><span class="line"># interfaces &#x3D; 用于将Samba配置为侦听多个network interfaces。如果您有多个interfaces，则可以使用&quot;interfaces &#x3D;&quot;选项来配置Samba侦听哪些interface。 切勿忽略localhost interface (lo)。</span><br><span class="line">#</span><br><span class="line"># hosts allow &#x3D; 允许连接的主机。This option can also be used on a per-share basis.</span><br><span class="line">#</span><br><span class="line"># hosts deny &#x3D; 不允许主机连接。 This option can also be used on a per-share basis.</span><br><span class="line">#</span><br><span class="line">	workgroup &#x3D; MYGROUP</span><br><span class="line">	server string &#x3D; Samba Server Version %v</span><br><span class="line"></span><br><span class="line">;	netbios name &#x3D; MYSERVER</span><br><span class="line"></span><br><span class="line">;	interfaces &#x3D; lo eth0 192.168.12.2&#x2F;24 192.168.13.2&#x2F;24</span><br><span class="line">;	hosts allow &#x3D; 127. 192.168.12. 192.168.13.</span><br><span class="line"></span><br><span class="line"># --------------------------- Logging Options -----------------------------</span><br><span class="line">#</span><br><span class="line"># log file &#x3D; 指定日志文件写入的位置以及它们的拆分方式。</span><br><span class="line">#</span><br><span class="line"># max log size &#x3D; 指定允许的最大日志文件大小。 日志文件达到&quot;max log size&quot;指定的大小时，将对其进行rotated。</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">	# log files split per-machine:</span><br><span class="line">	log file &#x3D; &#x2F;var&#x2F;log&#x2F;samba&#x2F;log.%m</span><br><span class="line">	# maximum size of 50KB per log file, then rotate:</span><br><span class="line">	max log size &#x3D; 50</span><br><span class="line"></span><br><span class="line"># ----------------------- Standalone Server Options ------------------------</span><br><span class="line">#</span><br><span class="line"># security &#x3D; Samba运行的模式。可以将其设置为user, share (不建议使用), 或 server (不建议使用).</span><br><span class="line">#</span><br><span class="line"># passdb backend &#x3D; 用于存储用户信息的backend（后端）。新安装应使用tdbsam或ldapsam。tdbsam不需要其他配置。&quot;smbpasswd&quot;可用于向后兼容。</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">	security &#x3D; user</span><br><span class="line">	passdb backend &#x3D; tdbsam</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ----------------------- Domain Members Options ------------------------</span><br><span class="line">#</span><br><span class="line"># security &#x3D; 必须设置为 domain 或 ads.</span><br><span class="line">#</span><br><span class="line"># passdb backend &#x3D; 用于存储用户信息的backend（后端）。新安装应使用tdbsam或ldapsam。tdbsam不需要其他配置。&quot;smbpasswd&quot;可用于向后兼容。</span><br><span class="line">#</span><br><span class="line"># realm &#x3D; 当设置了&quot;security &#x3D; ads&quot;选项时才会用到该选项。</span><br><span class="line">#</span><br><span class="line"># password server &#x3D; 当设置了&quot;security &#x3D; server&quot;选项或无法使用DNS定位Domain Controller时，才使用此选项。参数列表可以包括My_PDC_Name, [My_BDC_Name], 和[My_Next_BDC_Name]</span><br><span class="line">#</span><br><span class="line"># password server &#x3D; My_PDC_Name [My_BDC_Name] [My_Next_BDC_Name]</span><br><span class="line">#</span><br><span class="line"># 使用 &quot;password server &#x3D; *&quot; 自动定位 Domain Controllers.</span><br><span class="line"></span><br><span class="line">;	security &#x3D; domain</span><br><span class="line">;	passdb backend &#x3D; tdbsam</span><br><span class="line">;	realm &#x3D; MY_REALM</span><br><span class="line"></span><br><span class="line">;	password server &#x3D; &lt;NT-Server-Name&gt;</span><br><span class="line"></span><br><span class="line"># ----------------------- Domain Controller Options ------------------------</span><br><span class="line">#</span><br><span class="line"># security &#x3D; 必须将domain controllers设置为user。</span><br><span class="line">#</span><br><span class="line"># passdb backend &#x3D; 用于存储用户信息的backend（后端）。新安装应使用tdbsam或ldapsam。tdbsam不需要其他配置。&quot;smbpasswd&quot;可用于向后兼容。</span><br><span class="line">#</span><br><span class="line"># domain master &#x3D; 将Samba指定为Domain Master Browser，从而允许Samba整理subnets之间的browse lists。 如果您已经有Windows NT domain controller执行此任务，请不要使用&quot;domain master&quot;选项。</span><br><span class="line">#</span><br><span class="line"># domain logons &#x3D; 允许Samba为Windows workstations提供网络登录服务。</span><br><span class="line">#</span><br><span class="line"># logon script &#x3D; 指定在登录时在客户端上运行的脚本。 必须在名为NETLOGON的共享中提供这些脚本。</span><br><span class="line">#</span><br><span class="line"># logon path &#x3D; 指定（使用UNC path）用户配置文件的存储位置。</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">;	security &#x3D; user</span><br><span class="line">;	passdb backend &#x3D; tdbsam</span><br><span class="line"></span><br><span class="line">;	domain master &#x3D; yes</span><br><span class="line">;	domain logons &#x3D; yes</span><br><span class="line"></span><br><span class="line">	# 以下登录脚本名称由machine name确定(%m):</span><br><span class="line">;	logon script &#x3D; %m.bat</span><br><span class="line">	# 以下登录脚本名称由UNIX user确定:</span><br><span class="line">;	logon script &#x3D; %u.bat</span><br><span class="line">;	logon path &#x3D; \\%L\Profiles\%u</span><br><span class="line">	# 使用empty path禁用profile support</span><br><span class="line">;	logon path &#x3D;</span><br><span class="line"></span><br><span class="line">	# 可以在domain controller或独立machine上使用各种脚本来添加或删除相应的UNIX帐户：</span><br><span class="line"></span><br><span class="line">;	add user script &#x3D; &#x2F;usr&#x2F;sbin&#x2F;useradd &quot;%u&quot; -n -g users</span><br><span class="line">;	add group script &#x3D; &#x2F;usr&#x2F;sbin&#x2F;groupadd &quot;%g&quot;</span><br><span class="line">;	add machine script &#x3D; &#x2F;usr&#x2F;sbin&#x2F;useradd -n -c &quot;Workstation (%u)&quot; -M -d &#x2F;nohome -s &#x2F;bin&#x2F;false &quot;%u&quot;</span><br><span class="line">;	delete user script &#x3D; &#x2F;usr&#x2F;sbin&#x2F;userdel &quot;%u&quot;</span><br><span class="line">;	delete user from group script &#x3D; &#x2F;usr&#x2F;sbin&#x2F;userdel &quot;%u&quot; &quot;%g&quot;</span><br><span class="line">;	delete group script &#x3D; &#x2F;usr&#x2F;sbin&#x2F;groupdel &quot;%g&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ----------------------- Browser Control Options ----------------------------</span><br><span class="line">#</span><br><span class="line"># local master &#x3D; 设置为no时，Samba不会成为网络上的master browser。 设置为yes时，将应用常规election（选举） rules。</span><br><span class="line">#</span><br><span class="line"># os level &#x3D; 确定服务器在master browser选举中的优先级。 默认值应该合理。</span><br><span class="line">#</span><br><span class="line"># preferred master &#x3D; 设置为yes时，Samba会在启动时强制进行local browser选举（并使其赢得选举的几率略高）。</span><br><span class="line">#</span><br><span class="line">;	local master &#x3D; no</span><br><span class="line">;	os level &#x3D; 33</span><br><span class="line">;	preferred master &#x3D; yes</span><br><span class="line"></span><br><span class="line">#----------------------------- Name Resolution -------------------------------</span><br><span class="line">#</span><br><span class="line"># 本节详细介绍了对 Windows Internet Name Service (WINS) 的支持.</span><br><span class="line">#</span><br><span class="line"># 注意：Samba可以是WINS服务器，也可以是WINS客户端，但不能同时是两者。</span><br><span class="line">#</span><br><span class="line"># wins support &#x3D; 设置为yes时，Samba的NMBD组件启用其WINS服务器。</span><br><span class="line">#</span><br><span class="line"># wins server &#x3D; 告诉Samba的NMBD组件是WINS客户端。</span><br><span class="line">#</span><br><span class="line"># wins proxy &#x3D; 设置为yes时，Samba代表不支持WINS的客户端回答name resolution查询。 为此，网络上至少必须有一个WINS服务器。 默认为no。</span><br><span class="line">#</span><br><span class="line"># dns proxy &#x3D; 设置为yes时，Samba尝试通过DNS nslookups解析NetBIOS名称。</span><br><span class="line"></span><br><span class="line">;	wins support &#x3D; yes</span><br><span class="line">;	wins server &#x3D; w.x.y.z</span><br><span class="line">;	wins proxy &#x3D; yes</span><br><span class="line"></span><br><span class="line">;	dns proxy &#x3D; yes</span><br><span class="line"></span><br><span class="line"># --------------------------- Printing Options -----------------------------</span><br><span class="line">#</span><br><span class="line"># 本部分中的选项使您可以配置non-default（非默认） printing system。</span><br><span class="line">#</span><br><span class="line"># load printers &#x3D; 设置为yes时，将自动加载printers列表，而不是单独进行设置。</span><br><span class="line">#</span><br><span class="line"># cups options &#x3D; 允许您将选项传递到CUPS库。 例如，将此选项设置为raw，则可以在Windows客户端上使用驱动程序。</span><br><span class="line">#</span><br><span class="line"># printcap name &#x3D; 用于指定备用的printcap文件。</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">	load printers &#x3D; yes</span><br><span class="line">	cups options &#x3D; raw</span><br><span class="line"></span><br><span class="line">;	printcap name &#x3D; &#x2F;etc&#x2F;printcap</span><br><span class="line">	# 自动获取UNIX System V系统上的printers列表：</span><br><span class="line">;	printcap name &#x3D; lpstat</span><br><span class="line">;	printing &#x3D; cups</span><br><span class="line"></span><br><span class="line"># --------------------------- File System Options ---------------------------</span><br><span class="line">#</span><br><span class="line"># 如果文件系统支持扩展属性，并且启用了这些属性（通常通过&quot;user_xattr&quot; mount选项），则可以取消注释本节中的选项。</span><br><span class="line"># 这些选项允许管理员指定DOS属性存储在扩展属性中，并且还确保Samba不会更改permission bits。</span><br><span class="line">#</span><br><span class="line"># 注意：这些选项可以按per-share使用。 全局设置它们（在[global]部分中）使它们成为所有共享的默认值。</span><br><span class="line"></span><br><span class="line">;	map archive &#x3D; no</span><br><span class="line">;	map hidden &#x3D; no</span><br><span class="line">;	map read only &#x3D; no</span><br><span class="line">;	map system &#x3D; no</span><br><span class="line">;	store dos attributes &#x3D; yes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; Share Definitions &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">[homes]</span><br><span class="line">	comment &#x3D; Home Directories</span><br><span class="line">	browseable &#x3D; no</span><br><span class="line">	writable &#x3D; yes</span><br><span class="line">;	valid users &#x3D; %S</span><br><span class="line">;	valid users &#x3D; MYDOMAIN\%S</span><br><span class="line"></span><br><span class="line">[printers]</span><br><span class="line">	comment &#x3D; All Printers</span><br><span class="line">	path &#x3D; &#x2F;var&#x2F;spool&#x2F;samba</span><br><span class="line">	browseable &#x3D; no</span><br><span class="line">	guest ok &#x3D; no</span><br><span class="line">	writable &#x3D; no</span><br><span class="line">	printable &#x3D; yes</span><br><span class="line"></span><br><span class="line"># 取消注释以下内容，并为Domain Logons创建netlogon目录：</span><br><span class="line">;	[netlogon]</span><br><span class="line">;	comment &#x3D; Network Logon Service</span><br><span class="line">;	path &#x3D; &#x2F;var&#x2F;lib&#x2F;samba&#x2F;netlogon</span><br><span class="line">;	guest ok &#x3D; yes</span><br><span class="line">;	writable &#x3D; no</span><br><span class="line">;	share modes &#x3D; no</span><br><span class="line"></span><br><span class="line"># 取消注释以下内容以提供特定的roaming profile share。</span><br><span class="line"># 默认为使用用户的home目录：</span><br><span class="line">;	[Profiles]</span><br><span class="line">;	path &#x3D; &#x2F;var&#x2F;lib&#x2F;samba&#x2F;profiles</span><br><span class="line">;	browseable &#x3D; no</span><br><span class="line">;	guest ok &#x3D; yes</span><br><span class="line"></span><br><span class="line"># 一个只读公共目录，但&quot;staff&quot; group中的用户（具有写权限）除外：</span><br><span class="line">;	[public]</span><br><span class="line">;	comment &#x3D; Public Stuff</span><br><span class="line">;	path &#x3D; &#x2F;home&#x2F;samba</span><br><span class="line">;	public &#x3D; yes</span><br><span class="line">;	writable &#x3D; no</span><br><span class="line">;	printable &#x3D; no</span><br><span class="line">;	write list &#x3D; +staff</span><br></pre></td></tr></table></figure>
<p>6、用户操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">smbpasswd -a 增加用户（linux系统用户）</span><br><span class="line">smbpasswd -d 冻结用户</span><br><span class="line">smbpasswd -e 解冻用户</span><br><span class="line">smbpasswd -n 将用户的密码设置成空.</span><br></pre></td></tr></table></figure>


<h1 id="LinuxCast笔记"><a href="#LinuxCast笔记" class="headerlink" title="LinuxCast笔记"></a>LinuxCast笔记</h1><p><img src="https://raw.githubusercontent.com/lnsyyj/lnsyyj.github.io/hexo/Blog/source/_posts/samba/linuxcast-smb-1.png"></p>
<p><img src="https://github.com/lnsyyj/lnsyyj.github.io/blob/hexo/Blog/source/_posts/samba/linuxcast-smb-2.png?raw=true"></p>
<p><img src="https://github.com/lnsyyj/lnsyyj.github.io/blob/hexo/Blog/source/_posts/samba/linuxcast-smb-3.png?raw=true"></p>
<p><img src="https://github.com/lnsyyj/lnsyyj.github.io/blob/hexo/Blog/source/_posts/samba/linuxcast-smb-4.png?raw=true"></p>
<p><img src="https://raw.githubusercontent.com/lnsyyj/lnsyyj.github.io/hexo/Blog/source/_posts/samba/linuxcast-smb-5.png"></p>
<p><img src="https://raw.githubusercontent.com/lnsyyj/lnsyyj.github.io/hexo/Blog/source/_posts/samba/linuxcast-smb-6.png"></p>
<p><img src="https://github.com/lnsyyj/lnsyyj.github.io/blob/hexo/Blog/source/_posts/samba/linuxcast-smb-7.png?raw=true"></p>
<p><img src="https://raw.githubusercontent.com/lnsyyj/lnsyyj.github.io/hexo/Blog/source/_posts/samba/linuxcast-smb-9.png"></p>
<p><img src="https://github.com/lnsyyj/lnsyyj.github.io/blob/hexo/Blog/source/_posts/samba/linuxcast-smb-10.png?raw=true"></p>
<p><img src="https://raw.githubusercontent.com/lnsyyj/lnsyyj.github.io/hexo/Blog/source/_posts/samba/linuxcast-smb-11.png"></p>
<h1 id="samba相关rpm"><a href="#samba相关rpm" class="headerlink" title="samba相关rpm"></a>samba相关rpm</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line">Name        : ctdb</span><br><span class="line">Summary     : A Clustered Database based on Samba&#39;s Trivial Database (TDB)</span><br><span class="line">Description : CTDB是Samba和其他项目用来存储临时数据的TDB数据库的集群实现。如果应用程序已经在使用TDB来存储临时数据，则很容易将该应用程序转换为群集感知型，而使用CTDB。</span><br><span class="line"></span><br><span class="line">Name        : samba-client</span><br><span class="line">Summary     : Samba client programs</span><br><span class="line">Description : samba-client package提供了一些SMB&#x2F;CIFS客户端，以补充Linux中内置的SMB&#x2F;CIFS filesystem。这些客户端允许访问SMB&#x2F;CIFS shares并打印到SMB&#x2F;CIFS printers。</span><br><span class="line"></span><br><span class="line">Name        : samba-devel</span><br><span class="line">Summary     : Developer tools for Samba libraries</span><br><span class="line">Description : samba-devel package包含开发Samba套件时的SMB，RPC和其他程序所需libraries的header files。</span><br><span class="line"></span><br><span class="line">Name        : samba-vfs-glusterfs</span><br><span class="line">Summary     : Samba VFS module for GlusterFS</span><br><span class="line">Description : 包含GlusterFS集成Samba VFS的模块。</span><br><span class="line"></span><br><span class="line">Name        : ctdb-tests</span><br><span class="line">Summary     : CTDB clustered database test suite</span><br><span class="line">Description : CTDB的测试套件。CTDB是Samba和其他项目用来存储临时数据的TDB数据库的集群实现。如果应用程序已经在使用TDB来存储临时数据，则很容易将该应用程序转换为群集感知型，而使用CTDB。</span><br><span class="line"></span><br><span class="line">Name        : samba-client-libs</span><br><span class="line">Summary     : Samba client libraries</span><br><span class="line">Description : samba-client-libs package包含SMB&#x2F;CIFS客户端所需的internal libraries。</span><br><span class="line"></span><br><span class="line">Name        : samba-krb5-printing</span><br><span class="line">Summary     : Samba CUPS backend for printing with Kerberos</span><br><span class="line">Description : 如果您需要 Kerberos 进行print jobs，通过 SMB后端连接到printer cups，则需要安装该软件包。它将允许cups访问发出print job的用户的 Kerberos credentials cache。</span><br><span class="line"></span><br><span class="line">Name        : samba-winbind</span><br><span class="line">Summary     : Samba winbind</span><br><span class="line">Description : samba-winbind package提供了winbind NSS library和一些客户端工具。 Winbind使Linux成为Windows domains中的正式成员，并在Linux上使用Windows user和group帐户。</span><br><span class="line"></span><br><span class="line">Name        : libsmbclient</span><br><span class="line">Summary     : The SMB client library</span><br><span class="line">Description : libsmbclient包含来自Samba套件的SMB客户端library。</span><br><span class="line"></span><br><span class="line">Name        : samba-common-libs</span><br><span class="line">Summary     : Libraries used by both Samba servers and clients</span><br><span class="line">Description : samba-common-libs package包含SMB&#x2F;CIFS客户端所需的internal libraries。</span><br><span class="line"></span><br><span class="line">Name        : samba-libs</span><br><span class="line">Summary     : Samba libraries</span><br><span class="line">Description : samba-libs package包含Samba套件的SMB，RPC和其他协议所需的libraries。</span><br><span class="line"></span><br><span class="line">Name        : samba-winbind-clients</span><br><span class="line">Summary     : Samba winbind clients</span><br><span class="line">Description : samba-winbind-clients package提供wbinfo和ntlm_auth工具。</span><br><span class="line"></span><br><span class="line">Name        : libsmbclient-devel</span><br><span class="line">Summary     : Developer tools for the SMB client library</span><br><span class="line">Description : libsmbclient-devel package包含开发相关Samba套件的SMB client library link所需的header files和libraries。</span><br><span class="line"></span><br><span class="line">Name        : samba-common-tools</span><br><span class="line">Summary     : Tools for Samba servers and clients</span><br><span class="line">Description : samba-common-tools package包含用于Samba servers和SMB&#x2F;CIFS clients的工具。</span><br><span class="line"></span><br><span class="line">Name        : samba-python</span><br><span class="line">Summary     : Samba Python libraries</span><br><span class="line">Description : samba-python package包含Python程序中使用SMB，RPC和其他Samba提供的协议的程序所需的Python libraries。</span><br><span class="line"></span><br><span class="line">Name        : samba-winbind-krb5-locator</span><br><span class="line">Summary     : Samba winbind krb5 locator</span><br><span class="line">Description : winbind krb5 locator是系统kerberos library的plugin，以允许本地kerberos library使用与samba和winbind相同的KDC。</span><br><span class="line"></span><br><span class="line">Name        : libwbclient</span><br><span class="line">Summary     : The winbind client library</span><br><span class="line">Description : libwbclient package包含Samba套件中的winbind client library。</span><br><span class="line"></span><br><span class="line">Name        : samba-dc</span><br><span class="line">Summary     : Samba AD Domain Controller</span><br><span class="line">Description : samba-dc package提供AD Domain Controller功能</span><br><span class="line"></span><br><span class="line">Name        : samba-python-test</span><br><span class="line">Summary     : Samba Python libraries</span><br><span class="line">Description : samba-python-test package包含Samba test suite使用的Python libraries。 如果要运行全套Samba测试，则需要安装此package。</span><br><span class="line"></span><br><span class="line">Name        : samba-winbind-modules</span><br><span class="line">Summary     : Samba winbind modules</span><br><span class="line">Description : samba-winbind-modules package提供了与Winbind Daemon通信所需的NSS library和PAM module。</span><br><span class="line"></span><br><span class="line">Name        : libwbclient-devel</span><br><span class="line">Summary     : Developer tools for the winbind library</span><br><span class="line">Description : libwbclient-devel package为wbclient library提供了developer tools。</span><br><span class="line"></span><br><span class="line">Name        : samba-dc-libs</span><br><span class="line">Summary     : Samba AD Domain Controller Libraries</span><br><span class="line">Description : samba-dc-libs package包含DC去链接SMB，RPC和其他协议所需的库。</span><br><span class="line"></span><br><span class="line">Name        : samba-test</span><br><span class="line">Summary     : Testing tools for Samba servers and clients</span><br><span class="line">Description : samba-test为Samba的server和client packages提供测试工具。</span><br><span class="line"></span><br><span class="line">Name        : samba</span><br><span class="line">Summary     : Server and Client software to interoperate with Windows machines</span><br><span class="line">Description : Samba is the standard Windows interoperability suite of programs</span><br><span class="line">            : for Linux and Unix.</span><br><span class="line"></span><br><span class="line">Name        : samba-debuginfo</span><br><span class="line">Summary     : Debug information for package samba</span><br><span class="line">Description : 该软件包提供了samba软件包的debug information。</span><br><span class="line"></span><br><span class="line">Name        : samba-test-libs</span><br><span class="line">Summary     : Libraries need by the testing tools for Samba servers and clients</span><br><span class="line">Description : samba-test-libs提供测试工具所需的libraries。</span><br><span class="line"></span><br><span class="line">Name        : samba-common</span><br><span class="line">Summary     : Samba servers 和 clients使用的文件</span><br><span class="line">Description : samba-common提供Samba的server和client packages所需的文件。</span><br><span class="line"></span><br><span class="line">Name        : samba-pidl</span><br><span class="line">Summary     : Perl IDL编译器</span><br><span class="line">Description : samba-pidl package包含Samba和Wireshark用于解析IDL和类似协议的Perl IDL编译器</span><br></pre></td></tr></table></figure>


<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>【1】<a target="_blank" rel="noopener" href="https://blog.csdn.net/skdkjzz/article/details/42101363">https://blog.csdn.net/skdkjzz/article/details/42101363</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/10/21/Ceph-DISKPREDICTION-MODULE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/21/Ceph-DISKPREDICTION-MODULE/" class="post-title-link" itemprop="url">Ceph DISKPREDICTION MODULE</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-21 14:41:49" itemprop="dateCreated datePublished" datetime="2019-10-21T14:41:49+08:00">2019-10-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-22 16:16:35" itemprop="dateModified" datetime="2020-03-22T16:16:35+08:00">2020-03-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="DISKPREDICTION-MODULE"><a href="#DISKPREDICTION-MODULE" class="headerlink" title="DISKPREDICTION MODULE"></a>DISKPREDICTION MODULE</h3><p>磁盘预测模块支持两种模式：cloud mode和local mode。 在cloud mode下，磁盘和Ceph操作状态信息是从Ceph群集中收集的，并通过Internet发送到基于云的DiskPrediction服务器。 DiskPrediction服务器分析数据，并提供Ceph群集的性能和磁盘运行状况的分析和预测结果。</p>
<p>local mode不需要任何外部服务器即可进行数据分析和输出结果。 在local mode下，磁盘<em>diskprediction</em> module将内部predictor module用于磁盘预测服务，然后将磁盘预测结果返回给Ceph系统。</p>
<p>Local predictor: 70% 的准确性</p>
<p>Cloud predictor for free: 95% 的准确性</p>
<h1 id="ENABLING"><a href="#ENABLING" class="headerlink" title="ENABLING"></a>ENABLING</h1><p>运行以下命令以在Ceph环境中启用磁盘预测模块：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph mgr module enable diskprediction_cloud</span><br><span class="line">ceph mgr module enable diskprediction_local</span><br></pre></td></tr></table></figure>
<p>选择预测模式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set global device_failure_prediction_mode local</span><br></pre></td></tr></table></figure>
<p>或</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set global device_failure_prediction_mode cloud</span><br></pre></td></tr></table></figure>
<p>要禁用预测，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set global device_failure_prediction_mode none</span><br></pre></td></tr></table></figure>
<h1 id="CONNECTION-SETTINGS"><a href="#CONNECTION-SETTINGS" class="headerlink" title="CONNECTION SETTINGS"></a>CONNECTION SETTINGS</h1><p>connection settings用于Ceph和DiskPrediction服务器之间的连接。</p>
<h3 id="LOCAL-MODE"><a href="#LOCAL-MODE" class="headerlink" title="LOCAL MODE"></a>LOCAL MODE</h3><p>diskprediction module利用Ceph设备运行状况检查来收集磁盘运行状况指标，并使用内部predictor module来生成磁盘故障预测并返回Ceph。 因此，在local mode下不需要连接设置。 local predictor module至少需要六个设备健康状况数据集才能实施预测。</p>
<p>运行以下命令以使用本地预测变量预测设备的预期寿命。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph device predict-life-expectancy &lt;device id&gt;</span><br><span class="line"></span><br><span class="line">[root@community-ceph-1 ~]# ceph device predict-life-expectancy 0d2a946c-413f-43f4-b</span><br><span class="line">unknown</span><br></pre></td></tr></table></figure>
<h3 id="CLOUD-MODE"><a href="#CLOUD-MODE" class="headerlink" title="CLOUD MODE"></a>CLOUD MODE</h3><p>在cloud mode下，需要用户注册。 用户必须在<a target="_blank" rel="noopener" href="https://www.diskprophet.com/#/%E4%B8%8A%E6%B3%A8%E5%86%8C%E5%85%B6%E5%B8%90%E6%88%B7%EF%BC%8C%E4%BB%A5%E6%8E%A5%E6%94%B6%E4%BB%A5%E4%B8%8BDiskPrediction%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BF%A1%E6%81%AF%E4%BB%A5%E8%BF%9B%E8%A1%8C%E8%BF%9E%E6%8E%A5%E8%AE%BE%E7%BD%AE%E3%80%82">https://www.diskprophet.com/#/上注册其帐户，以接收以下DiskPrediction服务器信息以进行连接设置。</a></p>
<p>Certificate file path: 确认用户注册后，系统将发送一封确认电子邮件，其中包括证书文件下载链接。 下载证书文件并将其保存到Ceph系统。 运行以下命令来验证文件。 如果没有证书文件验证，则无法完成连接设置。</p>
<p>DiskPrediction server: DiskPrediction服务器名称。 如果需要，它可以是IP地址。</p>
<p>Connection account: 用于在Ceph和DiskPrediction服务器之间建立连接的帐户名</p>
<p>Connection password: 用于在Ceph和DiskPrediction服务器之间建立连接的密码</p>
<p>运行以下命令以完成连接设置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device set-cloud-prediction-config &lt;diskprediction_server&gt; &lt;connection_account&gt; &lt;connection_password&gt; &lt;certificate file path&gt;</span><br></pre></td></tr></table></figure>
<p>您可以使用以下命令显示连接设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device show-prediction-config</span><br></pre></td></tr></table></figure>
<p>其他可选配置设置如下：</p>
<p>diskprediction_upload_metrics_interval: 指示定期将Ceph性能指标发送到DiskPrediction服务器的频率。 默认值为10分钟。</p>
<p>diskprediction_upload_smart_interval: 指示定期将Ceph物理设备信息发送到DiskPrediction服务器的频率。 默认值为12小时。</p>
<p>diskprediction_retrieve_prediction_interval: 指示Ceph有时会定期从DiskPrediction服务器检索物理设备预测数据。 默认值为12小时。</p>
<h1 id="DISKPREDICTION-DATA"><a href="#DISKPREDICTION-DATA" class="headerlink" title="DISKPREDICTION DATA"></a>DISKPREDICTION DATA</h1><p>diskprediction module主动向/从DiskPrediction服务器发送/检索以下数据。</p>
<h3 id="METRICS-DATA"><a href="#METRICS-DATA" class="headerlink" title="METRICS DATA"></a>METRICS DATA</h3><ul>
<li>Ceph cluster status</li>
</ul>
<table>
<thead>
<tr>
<th align="left">key</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="left">cluster_health</td>
<td align="left">Ceph health check status</td>
</tr>
<tr>
<td align="left">num_mon</td>
<td align="left">Number of monitor node</td>
</tr>
<tr>
<td align="left">num_mon_quorum</td>
<td align="left">Number of monitors in quorum</td>
</tr>
<tr>
<td align="left">num_osd</td>
<td align="left">Total number of OSD</td>
</tr>
<tr>
<td align="left">num_osd_up</td>
<td align="left">Number of OSDs that are up</td>
</tr>
<tr>
<td align="left">num_osd_in</td>
<td align="left">Number of OSDs that are in cluster</td>
</tr>
<tr>
<td align="left">osd_epoch</td>
<td align="left">Current epoch of OSD map</td>
</tr>
<tr>
<td align="left">osd_bytes</td>
<td align="left">Total capacity of cluster in bytes</td>
</tr>
<tr>
<td align="left">osd_bytes_used</td>
<td align="left">Number of used bytes on cluster</td>
</tr>
<tr>
<td align="left">osd_bytes_avail</td>
<td align="left">Number of available bytes on cluster</td>
</tr>
<tr>
<td align="left">num_pool</td>
<td align="left">Number of pools</td>
</tr>
<tr>
<td align="left">num_pg</td>
<td align="left">Total number of placement groups</td>
</tr>
<tr>
<td align="left">num_pg_active_clean</td>
<td align="left">Number of placement groups in active+clean state</td>
</tr>
<tr>
<td align="left">num_pg_active</td>
<td align="left">Number of placement groups in active state</td>
</tr>
<tr>
<td align="left">num_pg_peering</td>
<td align="left">Number of placement groups in peering state</td>
</tr>
<tr>
<td align="left">num_object</td>
<td align="left">Total number of objects on cluster</td>
</tr>
<tr>
<td align="left">num_object_degraded</td>
<td align="left">Number of degraded (missing replicas) objects</td>
</tr>
<tr>
<td align="left">num_object_misplaced</td>
<td align="left">Number of misplaced (wrong location in the cluster) objects</td>
</tr>
<tr>
<td align="left">num_object_unfound</td>
<td align="left">Number of unfound objects</td>
</tr>
<tr>
<td align="left">num_bytes</td>
<td align="left">Total number of bytes of all objects</td>
</tr>
<tr>
<td align="left">num_mds_up</td>
<td align="left">Number of MDSs that are up</td>
</tr>
<tr>
<td align="left">num_mds_in</td>
<td align="left">Number of MDS that are in cluster</td>
</tr>
<tr>
<td align="left">num_mds_failed</td>
<td align="left">Number of failed MDS</td>
</tr>
<tr>
<td align="left">mds_epoch</td>
<td align="left">Current epoch of MDS map</td>
</tr>
</tbody></table>
<ul>
<li>Ceph mon/osd performance counts  </li>
</ul>
<p>Mon:</p>
<table>
<thead>
<tr>
<th align="left">key</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="left">num_sessions</td>
<td align="left">Current number of opened monitor sessions</td>
</tr>
<tr>
<td align="left">session_add</td>
<td align="left">Number of created monitor sessions</td>
</tr>
<tr>
<td align="left">session_rm</td>
<td align="left">Number of remove_session calls in monitor</td>
</tr>
<tr>
<td align="left">session_trim</td>
<td align="left">Number of trimed monitor sessions</td>
</tr>
<tr>
<td align="left">num_elections</td>
<td align="left">Number of elections monitor took part in</td>
</tr>
<tr>
<td align="left">election_call</td>
<td align="left">Number of elections started by monitor</td>
</tr>
<tr>
<td align="left">election_win</td>
<td align="left">Number of elections won by monitor</td>
</tr>
<tr>
<td align="left">election_lose</td>
<td align="left">Number of elections lost by monitor</td>
</tr>
</tbody></table>
<p>Osd:</p>
<table>
<thead>
<tr>
<th align="left">key</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="left">op_wip</td>
<td align="left">Replication operations currently being processed (primary)</td>
</tr>
<tr>
<td align="left">op_in_bytes</td>
<td align="left">Client operations total write size</td>
</tr>
<tr>
<td align="left">op_r</td>
<td align="left">Client read operations</td>
</tr>
<tr>
<td align="left">op_out_bytes</td>
<td align="left">Client operations total read size</td>
</tr>
<tr>
<td align="left">op_w</td>
<td align="left">Client write operations</td>
</tr>
<tr>
<td align="left">op_latency</td>
<td align="left">Latency of client operations (including queue time)</td>
</tr>
<tr>
<td align="left">op_process_latency</td>
<td align="left">Latency of client operations (excluding queue time)</td>
</tr>
<tr>
<td align="left">op_r_latency</td>
<td align="left">Latency of read operation (including queue time)</td>
</tr>
<tr>
<td align="left">op_r_process_latency</td>
<td align="left">Latency of read operation (excluding queue time)</td>
</tr>
<tr>
<td align="left">op_w_in_bytes</td>
<td align="left">Client data written</td>
</tr>
<tr>
<td align="left">op_w_latency</td>
<td align="left">Latency of write operation (including queue time)</td>
</tr>
<tr>
<td align="left">op_w_process_latency</td>
<td align="left">Latency of write operation (excluding queue time)</td>
</tr>
<tr>
<td align="left">op_rw</td>
<td align="left">Client read-modify-write operations</td>
</tr>
<tr>
<td align="left">op_rw_in_bytes</td>
<td align="left">Client read-modify-write operations write in</td>
</tr>
<tr>
<td align="left">op_rw_out_bytes</td>
<td align="left">Client read-modify-write operations read out</td>
</tr>
<tr>
<td align="left">op_rw_latency</td>
<td align="left">Latency of read-modify-write operation (including queue time)</td>
</tr>
<tr>
<td align="left">op_rw_process_latency</td>
<td align="left">Latency of read-modify-write operation (excluding queue time)</td>
</tr>
</tbody></table>
<ul>
<li>Ceph pool statistics</li>
</ul>
<table>
<thead>
<tr>
<th align="left">key</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="left">bytes_used</td>
<td align="left">Per pool bytes used</td>
</tr>
<tr>
<td align="left">max_avail</td>
<td align="left">Max available number of bytes in the pool</td>
</tr>
<tr>
<td align="left">objects</td>
<td align="left">Number of objects in the pool</td>
</tr>
<tr>
<td align="left">wr_bytes</td>
<td align="left">Number of bytes written in the pool</td>
</tr>
<tr>
<td align="left">dirty</td>
<td align="left">Number of bytes dirty in the pool</td>
</tr>
<tr>
<td align="left">rd_bytes</td>
<td align="left">Number of bytes read in the pool</td>
</tr>
<tr>
<td align="left">stored_raw</td>
<td align="left">Bytes used in pool including copies made</td>
</tr>
</tbody></table>
<ul>
<li>Ceph physical device metadata</li>
</ul>
<table>
<thead>
<tr>
<th align="left">key</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="left">disk_domain_id</td>
<td align="left">Physical device identify id</td>
</tr>
<tr>
<td align="left">disk_name</td>
<td align="left">Device attachment name</td>
</tr>
<tr>
<td align="left">disk_wwn</td>
<td align="left">Device wwn</td>
</tr>
<tr>
<td align="left">model</td>
<td align="left">Device model name</td>
</tr>
<tr>
<td align="left">serial_number</td>
<td align="left">Device serial number</td>
</tr>
<tr>
<td align="left">size</td>
<td align="left">Device size</td>
</tr>
<tr>
<td align="left">vendor</td>
<td align="left">Device vendor name</td>
</tr>
</tbody></table>
<ul>
<li>Ceph each objects correlation information</li>
<li>The module agent information</li>
<li>The module agent cluster information</li>
<li>The module agent host information</li>
</ul>
<h3 id="SMART-DATA"><a href="#SMART-DATA" class="headerlink" title="SMART DATA"></a>SMART DATA</h3><ul>
<li>Ceph physical device SMART data (provided by Ceph <em>devicehealth</em> module)</li>
</ul>
<h3 id="PREDICTION-DATA"><a href="#PREDICTION-DATA" class="headerlink" title="PREDICTION DATA"></a>PREDICTION DATA</h3><ul>
<li>  Ceph physical device prediction data</li>
</ul>
<h1 id="RECEIVING-PREDICTED-HEALTH-STATUS-FROM-A-CEPH-OSD-DISK-DRIVE（从CEPH-OSD磁盘驱动器中接收预期的健康状况）"><a href="#RECEIVING-PREDICTED-HEALTH-STATUS-FROM-A-CEPH-OSD-DISK-DRIVE（从CEPH-OSD磁盘驱动器中接收预期的健康状况）" class="headerlink" title="RECEIVING PREDICTED HEALTH STATUS FROM A CEPH OSD DISK DRIVE（从CEPH OSD磁盘驱动器中接收预期的健康状况）"></a>RECEIVING PREDICTED HEALTH STATUS FROM A CEPH OSD DISK DRIVE（从CEPH OSD磁盘驱动器中接收预期的健康状况）</h1><p>您可以使用以下命令从Ceph OSD磁盘驱动器接收预测的健康状态。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device get-predicted-status &lt;device id&gt;</span><br></pre></td></tr></table></figure>
<p>get-predicted-status命令返回：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;near_failure&quot;: &quot;Good&quot;,</span><br><span class="line">    &quot;disk_wwn&quot;: &quot;5000011111111111&quot;,</span><br><span class="line">    &quot;serial_number&quot;: &quot;111111111&quot;,</span><br><span class="line">    &quot;predicted&quot;: &quot;2018-05-30 18:33:12&quot;,</span><br><span class="line">    &quot;attachment&quot;: &quot;sdb&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th align="left">Attribute</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="left">near_failure</td>
<td align="left">The disk failure prediction state: Good/Warning/Bad/Unknown</td>
</tr>
<tr>
<td align="left">disk_wwn</td>
<td align="left">Disk WWN number</td>
</tr>
<tr>
<td align="left">serial_number</td>
<td align="left">Disk serial number</td>
</tr>
<tr>
<td align="left">predicted</td>
<td align="left">Predicted date</td>
</tr>
<tr>
<td align="left">attachment</td>
<td align="left">device name on the local system</td>
</tr>
</tbody></table>
<p>磁盘故障预测状态的near_failure属性在下表中指示磁盘预期寿命。</p>
<table>
<thead>
<tr>
<th align="left">near_failure</th>
<th align="left">Life expectancy (weeks)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Good</td>
<td align="left">&gt; 6 weeks</td>
</tr>
<tr>
<td align="left">Warning</td>
<td align="left">2 weeks ~ 6 weeks</td>
</tr>
<tr>
<td align="left">Bad</td>
<td align="left">&lt; 2 weeks</td>
</tr>
</tbody></table>
<h1 id="DEBUGGING"><a href="#DEBUGGING" class="headerlink" title="DEBUGGING"></a>DEBUGGING</h1><p>如果要调试DiskPrediction module映射到Ceph日志记录级别，请使用以下命令。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[mgr]</span><br><span class="line"></span><br><span class="line">    debug mgr &#x3D; 20</span><br></pre></td></tr></table></figure>
<p>将日志记录设置为管理器调试时，模块将打印出带有前缀mgr [diskprediction]的日志记录消息，以便于过滤。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/10/21/Ceph-DEVICE-MANAGEMENT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/21/Ceph-DEVICE-MANAGEMENT/" class="post-title-link" itemprop="url">Ceph DEVICE MANAGEMENT</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-21 14:18:11" itemprop="dateCreated datePublished" datetime="2019-10-21T14:18:11+08:00">2019-10-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-22 16:16:35" itemprop="dateModified" datetime="2020-03-22T16:16:35+08:00">2020-03-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="DEVICE-MANAGEMENT"><a href="#DEVICE-MANAGEMENT" class="headerlink" title="DEVICE MANAGEMENT"></a>DEVICE MANAGEMENT</h3><p>Ceph跟踪哪个daemons消耗了哪些hardware storage devices（例如HDD，SSD），并收集有关这些devices的运行状况指标，以提供预测和/或自动响应硬件故障的工具。</p>
<h1 id="DEVICE-TRACKING"><a href="#DEVICE-TRACKING" class="headerlink" title="DEVICE TRACKING"></a>DEVICE TRACKING</h1><p>您可以查询哪些存储设备正在使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ceph device ls</span><br><span class="line"></span><br><span class="line">DEVICE               HOST:DEV                       DAEMONS LIFE EXPECTANCY </span><br><span class="line">0d2a946c-413f-43f4-b community-ceph-2.novalocal:vdc osd.0</span><br><span class="line">451e48d6-913e-4f93-a community-ceph-1.novalocal:vdd osd.5</span><br><span class="line">935b6018-1dfe-4cf9-8 community-ceph-1.novalocal:vdc osd.2</span><br><span class="line">abe09d21-d950-47b0-9 community-ceph-2.novalocal:vdd osd.3</span><br><span class="line">bf37729e-9d83-48e9-9 community-ceph-3.novalocal:vdc osd.1</span><br><span class="line">d48dcf29-fe58-4e3e-a community-ceph-3.novalocal:vdd osd.4</span><br></pre></td></tr></table></figure>
<p>您还可以按daemon或host列出devices：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ceph device ls-by-daemon &lt;daemon&gt;</span><br><span class="line">ceph device ls-by-host &lt;host&gt;</span><br><span class="line"></span><br><span class="line">[root@community-ceph-1 ~]# ceph device ls-by-daemon osd.0</span><br><span class="line">DEVICE               HOST:DEV                       EXPECTED FAILURE </span><br><span class="line">0d2a946c-413f-43f4-b community-ceph-2.novalocal:vdc                  </span><br><span class="line"></span><br><span class="line">[root@community-ceph-1 ~]# ceph device ls-by-host community-ceph-2.novalocal</span><br><span class="line">DEVICE               DEV DAEMONS EXPECTED FAILURE </span><br><span class="line">0d2a946c-413f-43f4-b vdc osd.0                    </span><br><span class="line">abe09d21-d950-47b0-9 vdd osd.3                    </span><br></pre></td></tr></table></figure>
<p>对于任何单个设备，您可以通过以下方式查询有关其位置以及如何使用它的信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ceph device info &lt;devid&gt;</span><br><span class="line"></span><br><span class="line">[root@community-ceph-1 ~]# ceph device info 0d2a946c-413f-43f4-b</span><br><span class="line">device 0d2a946c-413f-43f4-b</span><br><span class="line">attachment community-ceph-2.novalocal:vdc</span><br><span class="line">daemons osd.0</span><br></pre></td></tr></table></figure>
<h1 id="ENABLING-MONITORING"><a href="#ENABLING-MONITORING" class="headerlink" title="ENABLING MONITORING"></a>ENABLING MONITORING</h1><p>Ceph还可以监视与您的设备关联的健康指标。 例如，SATA硬盘实现了一个称为SMART的标准，该标准提供了有关设备使用情况和运行状况的内部指标，例如开机小时数，电源循环次数或不可恢复的读取错误。 其他设备类型（例如SAS和NVMe）实现了一组相似的指标（通过略有不同的标准）。 Ceph可以通过smartctl工具收集所有这些信息。</p>
<p>您可以通过以下方式启用或禁用运行状况监视：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device monitoring on</span><br></pre></td></tr></table></figure>
<p>或</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device monitoring off</span><br></pre></td></tr></table></figure>
<h1 id="SCRAPING"><a href="#SCRAPING" class="headerlink" title="SCRAPING"></a>SCRAPING</h1><p>如果启用了监视，则将定期自动scraped指标。 该间隔可以配置为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set mgr mgr&#x2F;devicehealth&#x2F;scrape_frequency &lt;seconds&gt;</span><br></pre></td></tr></table></figure>
<p>默认值为每24小时scrape一次。</p>
<p>您可以使用以下方法手动触发所有设备的scrape：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device scrape-health-metrics</span><br></pre></td></tr></table></figure>
<p>单个设备可以用以下方式scraped：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device scrape-health-metrics &lt;device-id&gt;</span><br></pre></td></tr></table></figure>
<p>或单个daemon的设备可以通过以下方式进行scraped：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device scrape-daemon-health-metrics &lt;who&gt;</span><br></pre></td></tr></table></figure>
<p>可以使用以下命令检索设备的存储健康指标（可选地，用于特定时间戳）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device get-health-metrics &lt;devid&gt; [sample-timestamp]</span><br></pre></td></tr></table></figure>
<h1 id="FAILURE-PREDICTION"><a href="#FAILURE-PREDICTION" class="headerlink" title="FAILURE PREDICTION"></a>FAILURE PREDICTION</h1><p>Ceph可以根据其收集的健康指标预测预期寿命和设备故障。 共有三种模式：</p>
<ul>
<li>none: 禁用设备故障预测。</li>
<li>local: 使用来自ceph-mgr daemon的预训练预测模型</li>
<li>cloud: 使用ProphetStor运行的外部云服务共享设备运行状况和性能指标，并使用其免费服务或付费服务进行更准确的预测</li>
</ul>
<p>预测模式可以配置为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set global device_failure_prediction_mode &lt;mode&gt;</span><br></pre></td></tr></table></figure>
<p>预测通常在后台定期进行，因此填充预期寿命值可能需要一些时间。 您可以从以下输出中看到所有设备的预期寿命：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device ls</span><br></pre></td></tr></table></figure>
<p>您还可以使用以下方法查询特定设备的metadata：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device info &lt;devid&gt;</span><br></pre></td></tr></table></figure>
<p>您可以使用以下命令显式地强制预测设备的预期寿命：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device predict-life-expectancy &lt;devid&gt;</span><br></pre></td></tr></table></figure>
<p>如果您未使用Ceph的内部设备故障预测，但是拥有一些有关设备故障的外部信息源，则可以通过以下方式告知Ceph设备的预期寿命：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device set-life-expectancy &lt;devid&gt; &lt;from&gt; [&lt;to&gt;]</span><br></pre></td></tr></table></figure>
<p>预期寿命以时间间隔表示，因此不确定性可以以宽间隔的形式表示。 间隔结束也可以不指定。</p>
<h1 id="HEALTH-ALERTS"><a href="#HEALTH-ALERTS" class="headerlink" title="HEALTH ALERTS"></a>HEALTH ALERTS</h1><p>mgr/devicehealth/warn_threshold控制在生成运行状况警告之前，预期的设备故障必须多长时间。</p>
<p>可以使用以下方法检查所有设备的存储预期寿命，并生成任何适当的健康警报：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph device check-health</span><br></pre></td></tr></table></figure>
<h1 id="AUTOMATIC-MITIGATION"><a href="#AUTOMATIC-MITIGATION" class="headerlink" title="AUTOMATIC MITIGATION"></a>AUTOMATIC MITIGATION</h1><p>如果启用了mgr/devicehealth/self_heal选项（默认情况下），则对于预计将很快发现故障的设备，模块将通过将设备标记为“out”来自动将数据迁移到这些设备之外。</p>
<p>mgr/devicehealth/mark_out_threshold控制在我们将osd自动标记为“out”之前，预期的设备故障必须多长时间。</p>
<h3 id="原文："><a href="#原文：" class="headerlink" title="原文："></a>原文：</h3><h3 id="https-docs-ceph-com-docs-master-rados-operations-devices-devices"><a href="#https-docs-ceph-com-docs-master-rados-operations-devices-devices" class="headerlink" title="https://docs.ceph.com/docs/master/rados/operations/devices/#devices"></a><a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/rados/operations/devices/#devices">https://docs.ceph.com/docs/master/rados/operations/devices/#devices</a></h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/10/16/Ceph%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7%E6%96%B0%E7%89%B9%E6%80%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/16/Ceph%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7%E6%96%B0%E7%89%B9%E6%80%A7/" class="post-title-link" itemprop="url">Ceph版本升级新特性</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-16 22:13:29" itemprop="dateCreated datePublished" datetime="2019-10-16T22:13:29+08:00">2019-10-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-22 16:16:35" itemprop="dateModified" datetime="2020-03-22T16:16:35+08:00">2020-03-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="V14-2-0-NAUTILUS-MAJOR-CHANGES-FROM-MIMIC"><a href="#V14-2-0-NAUTILUS-MAJOR-CHANGES-FROM-MIMIC" class="headerlink" title="V14.2.0 NAUTILUS MAJOR CHANGES FROM MIMIC"></a>V14.2.0 NAUTILUS MAJOR CHANGES FROM MIMIC</h1><p>参考：</p>
<p>【1】<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/releases/nautilus/">https://docs.ceph.com/docs/master/releases/nautilus/</a></p>
<p>【2】<a target="_blank" rel="noopener" href="https://blog.csdn.net/Z_Stand/article/details/89739908">https://blog.csdn.net/Z_Stand/article/details/89739908</a></p>
<p>这是Ceph Nautilus的第一个稳定版本。</p>
<ul>
<li><p>Dashboard</p>
<p><a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/mgr/dashboard/#mgr-dashboard">Ceph Dashboard</a>增加了许多新功能：</p>
<ul>
<li>Support for multiple users / roles</li>
<li>SSO (SAMLv2) for user authentication</li>
<li>Auditing support（审计支持）</li>
<li>New landing page, showing more metrics and health info</li>
<li>I18N support（国际化）</li>
<li>REST API documentation with Swagger API</li>
<li>Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。总体目标是使客户端和文件系统作为服务器以同样的速度来更新。文件的方法，参数和模型紧密集成到服务器端的代码，允许API来始终保持同步。作者：天马行空LQ<br>  链接：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/66a14ea07622">https://www.jianshu.com/p/66a14ea07622</a><br>来源：简书<br>  著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</li>
</ul>
<p>Ceph management新功能：</p>
<ul>
<li>OSD management (mark as down/out, change OSD settings, recovery profiles)</li>
<li>Cluster config settings editor</li>
<li>Ceph Pool management (create/modify/delete)</li>
<li><a target="_blank" rel="noopener" href="https://www.oschina.net/news/2609">ECP</a> management</li>
<li>RBD mirroring configuration</li>
<li>Embedded Grafana Dashboards (derived from Ceph Metrics)</li>
<li>CRUSH map viewer</li>
<li>NFS Ganesha management</li>
<li>iSCSI target management (via <a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/rbd/iscsi-overview/#ceph-iscsi">Ceph iSCSI Gateway</a>)</li>
<li>RBD QoS configuration</li>
<li>Ceph Manager (ceph-mgr) module management</li>
<li>Prometheus alert Management</li>
</ul>
</li>
</ul>
<p>而且，Ceph Dashboard现在被拆分到ceph-mgr-dashboard的package。 如果您的package management software在安装ceph-mgr时失败，则可能需要单独安装ceph-mgr-dashboard。</p>
<ul>
<li><p>RADOS</p>
<ul>
<li>现在可以随时减少每个pool的placement groups (PGs)数，并且可以根据群集利用率或管理员提示<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/rados/operations/placement-groups/#pg-autoscaler">自动调整PG数</a>。</li>
<li>新的<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/rados/configuration/msgr2/#msgr2">v2 wire protocol</a>支持线路加密</li>
<li>群集可以跟踪OSD和Monitor daemons消耗的物理<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/rados/operations/devices/#devices">存储设备</a>以及运行状况指标（即SMART），并且群集可以通过预先训练的预测模型或者基于云预测服务来<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/mgr/diskprediction/#diskprediction">报告预测的HDD或SSD故障</a>。</li>
<li>可通过ceph osd numa-status命令轻松监视OSD daemons的NUMA节点，并通过osd_numa_node config选项进行配置。</li>
<li>现在，当使用BlueStore OSD时，空间利用率将按object data，omap data和internal metadata，pool以及压缩前和压缩后的大小进行细分。</li>
<li>在执行recovery和backfill时，OSD可以更有效地选择重要的PG和objects优先处理。</li>
<li>在设备出现问题以后，像recovery这种长期运行在后台的进程，可以使用ceph status命令查看进度。</li>
<li>增加了一个实验性的（耦合层） <a target="_blank" rel="noopener" href="https://www.usenix.org/conference/fast18/presentation/vajha">Coupled-Layer “Clay” erasure code</a> plugin，该plugin可减少大多数recovery操作所需的网络带宽和IO。</li>
</ul>
</li>
<li><p>RGW</p>
<ul>
<li>S3 lifecycle可以在storage classes与tiering层之间转换</li>
<li>Beast取代了civetweb成为默认的web frontend，从而提高了整体性能。</li>
<li>支持新的publish/subscribe基础架构，允许RGW将events推送至serverless frameworks如knative或data pipelies如Kafka。</li>
<li>新增一系列身份验证功能，使用OAuth2和OpenID::connect的STS联合以及OPA（开放策略代理）身份验证委派原型。</li>
<li>新的archive zone federation功能可将所有objects（包括历史记录）完全保留在一个单独的zone中。</li>
</ul>
</li>
<li><p>CephFS</p>
<ul>
<li>对于具有large caches和large RAM并长期运行的客户端，MDS的稳定性已大大提高。Cache trimming（调整）和client capability recall现在受到限制，以防止MDS过载。</li>
<li>现在可以在Rook管理的群集环境中通过NFS-Ganesha导出CephFS。Ceph负责管理集群并确保高可用性和可伸缩性。 <a target="_blank" rel="noopener" href="https://ceph.com/community/deploying-a-cephnfs-server-cluster-with-rook/">入门演示</a>。 预计在未来Nautilus的次要版本中实现此功能的更多自动化。</li>
<li>MDS mds_standby_for_*，mon_force_standby_active和mds_standby_replay配置选项已过时。 相反，操作者现在可以在CephFS文件系统上<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/cephfs/standby/#mds-standby-replay">设置新的</a>allow_standby_replay标志。 该设置会使文件系统由standbys变为standby-replay，并且任何可用的rank都会生效。（一个 rank 可看作是一个元数据分片）</li>
<li>MDS支持客户端释放缓存的同时释放自己的存储端缓存，这个过程可由MDS admin socket命令 <code>cache drop</code>来完成</li>
<li>现在可以检查MDS中正在进行的scrub的进度。 此外，scrub可能会暂停或中止。 有关更多信息，请参见<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/cephfs/scrub/#mds-scrub">scrub文档</a>。</li>
<li>通过<code>ceph volume</code> command-line-interface提供了一个用于创建volumes的新interface。</li>
<li>新的<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/cephfs/cephfs-shell/">cephfs-shell工具</a>可用于处理CephFS文件系统而无需mounting。</li>
<li>为了简洁，清楚和有用，已重新格式化了来自ceph status与CephFS相关的输出。</li>
<li>Lazy IO已进行了改进。客户端可以使用ceph_open C/C++ API的新CEPH_O_LAZY flag将其打开或通过配置选项client_force_lazyio将其打开。（LazyIO放松了POSIX语义。 即使文件由多个客户端上的多个应用程序打开，也允许缓冲的读/写操作。 应用程序负责自己管理缓存的一致性。自Nautilus发行以来，Libcephfs支持LazyIO。）</li>
<li>现在可以通过ceph fs fail命令快速关闭CephFS文件系统。有关更多信息，请参见 <a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/cephfs/administration/#cephfs-administration">the administration page</a>。</li>
</ul>
</li>
<li><p>RBD</p>
<ul>
<li>Images可以在尽量短的停机时间内迁移，以帮助在pool之间移动images或移动到新的layouts。</li>
<li>新的rbd perf image iotop和rbd perf image iostat命令为所有RBD images提供了类似于iotop和iostat的IO监视器。</li>
<li>现在，ceph-mgr Prometheus exporter新增一个用于所有RBD images的IO监视器。</li>
<li>支持pool中的单独image namespaces，以便进行租户隔离。</li>
</ul>
</li>
<li><p>Misc</p>
<ul>
<li>Ceph 拥有一组新的<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/mgr/orchestrator_cli/#orchestrator-cli-module">orchestrator modules</a>，可直接与外部orchestrators像ceph-ansible, DeepSea, Rook, or simply ssh通过一致的CLI interface(and, eventually, Dashboard) 进行交互。</li>
</ul>
</li>
</ul>
<h1 id="V13-2-0-MIMIC-MAJOR-CHANGES-FROM-LUMINOUS"><a href="#V13-2-0-MIMIC-MAJOR-CHANGES-FROM-LUMINOUS" class="headerlink" title="V13.2.0 MIMIC MAJOR CHANGES FROM LUMINOUS"></a>V13.2.0 MIMIC MAJOR CHANGES FROM LUMINOUS</h1><ul>
<li><p>Dashboard</p>
<ul>
<li>Ceph Luminous中引入的（只读）Ceph manager dashboard已由<a target="_blank" rel="noopener" href="https://openattic.org/">openATTIC</a> Ceph management tool新实现取代，提供了具有<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/mgr/dashboard/#mgr-dashboard">许多其他管理功能</a>。</li>
</ul>
</li>
<li><p>RADOS</p>
<ul>
<li>现在，配置选项可以由monitor集中存储和管理。</li>
<li>进行recovery或rebalancing操作时，monitor daemon占用的disk space大大减少。</li>
<li>当OSD从最近的故障中恢复时，异步恢复功能可减少请求的tail latency（少量响应的延迟高于均值，我们把这些响应称为尾延迟）。</li>
<li>scrub时OSD冲突请求抢占tail latency减少。</li>
</ul>
</li>
<li><p>RGW</p>
<ul>
<li>RGW可以将zone （或subset of buckets）复制到外部云存储服务（例如S3）。</li>
<li>RGW在versioned buckets功能上支持S3 multi-factor authentication API。<ul>
<li>AWS Multi-Factor Authentication（MFA）它在用户名和密码的基础上增加了一层额外的保护。启用MFA后，当用户登录AWS网站时，系统将提示他们输入用户名和密码以及来自其AWS MFA设备的身份验证代码。这些因素综合在一起，为您的AWS账户设置和资源提供了更高的安全性。</li>
</ul>
</li>
<li>版本控制是在相同的存储桶中保留对象的多个变量的方法。对于 Amazon S3 桶中存储的每个对象，您可以使用版本控制功能来保存、检索和还原它们的各个版本。使用版本控制能够轻松从用户意外操作和应用程序故障中恢复数据。</li>
<li>Beast frontend不再进行实验，被认为是稳定的并可以使用。</li>
</ul>
</li>
<li><p>CephFS</p>
<ul>
<li>Snapshots与多个MDS daemons结合使用时，现在稳定。</li>
</ul>
</li>
<li><p>RBD</p>
<ul>
<li>Image clones不再需要明确的protect和unprotect步骤。</li>
<li>可以将Images deep-copied（包括与parent image和associated snapshots的任何克隆链接）到新pool或修改data layouts。</li>
</ul>
</li>
<li><p>Misc</p>
<ul>
<li>由于在Stretch中缺少GCC 8，我们已删除了Mimic的Debian构建。我们希望Debian的构建将在2019年初发布Buster后回归，并希望在Buster可用后构建最终的Luminous发行版（以及以后的Mimic point发行版）。</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/10/15/AUTOSCALING-PLACEMENT-GROUPS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/15/AUTOSCALING-PLACEMENT-GROUPS/" class="post-title-link" itemprop="url">AUTOSCALING PLACEMENT GROUPS</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-15 14:26:54" itemprop="dateCreated datePublished" datetime="2019-10-15T14:26:54+08:00">2019-10-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-22 16:16:35" itemprop="dateModified" datetime="2020-03-22T16:16:35+08:00">2020-03-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="AUTOSCALING-PLACEMENT-GROUPS"><a href="#AUTOSCALING-PLACEMENT-GROUPS" class="headerlink" title="AUTOSCALING PLACEMENT GROUPS"></a>AUTOSCALING PLACEMENT GROUPS</h1><p>Placement groups (PGs)是ceph分布数据的内部实现。您可以通过启用pg-autoscaling允许根据集群的使用方式提出建议或自动调整PG。</p>
<p>系统中的每个pool都有一个pg_autoscale_mode属性，可以将其设置为off，on或warn。</p>
<ul>
<li><p>off: Disable该pool的autoscaling，<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/rados/operations/placement-groups/#choosing-number-of-placement-groups">Choosing the number of Placement Groups</a></p>
</li>
<li><p>on: 为指定的pool启用PG count自动调整。</p>
</li>
<li><p>warn: 当调整PG count时提出health alerts</p>
</li>
</ul>
<p>要为已有的pool设置autoscaling mode，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &lt;pool-name&gt; pg_autoscale_mode &lt;mode&gt;</span><br></pre></td></tr></table></figure>
<p>例如，要对池foo启用autoscaling，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set foo pg_autoscale_mode on</span><br></pre></td></tr></table></figure>
<p>您还可以使用以下命令配置默认pg_autoscale_mode，该默认pg_autoscale_mode应用于以后创建的任何pool：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set global osd_pool_default_pg_autoscale_mode &lt;mode&gt;</span><br></pre></td></tr></table></figure>
<h3 id="VIEWING-PG-SCALING-RECOMMENDATIONS"><a href="#VIEWING-PG-SCALING-RECOMMENDATIONS" class="headerlink" title="VIEWING PG SCALING RECOMMENDATIONS"></a>VIEWING PG SCALING RECOMMENDATIONS</h3><p>您可以使用以下命令查看每个pool，pool的相对利用率以及对PG count的任何建议更改：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool autoscale-status</span><br></pre></td></tr></table></figure>
<p>输出将类似于：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">POOL                         SIZE  TARGET SIZE  RATE  RAW CAPACITY   RATIO  TARGET RATIO  BIAS  PG_NUM  NEW PG_NUM  AUTOSCALE </span><br><span class="line">cephfs_metadata             1540k                3.0        594.0G  0.0000                 4.0       8              warn      </span><br><span class="line">default.rgw.meta            1536k                3.0        594.0G  0.0000                 1.0       8              warn      </span><br><span class="line">cephfs_data                    0                 3.0        594.0G  0.0000                 1.0       8              warn      </span><br><span class="line">default.rgw.buckets.index      0                 3.0        594.0G  0.0000                 1.0       8              warn      </span><br><span class="line">default.rgw.control            0                 3.0        594.0G  0.0000                 1.0       8              warn      </span><br><span class="line">yujiang                        0        553.2G   1.0        594.0G  0.9313                 1.0     512              on        </span><br><span class="line">.rgw.root                   1344k                3.0        594.0G  0.0000                 1.0       8              warn      </span><br><span class="line">rbd                        576.0k                3.0        594.0G  0.0000                 1.0       4              on        </span><br><span class="line">default.rgw.log                0                 3.0        594.0G  0.0000                 1.0       8              warn      </span><br></pre></td></tr></table></figure>
<p>SIZE是存储在pool中的数据量。TARGET SIZE（如果存在）是管理员希望最终存储在该pool中的数据量。系统使用两个值中的较大者进行计算。</p>
<p>RATE是pool的multiplier（乘数或倍数），它确定要消耗多少raw（原始） storage capacity。例如，3个副本池的比率为3.0，而k=4，m=2擦除编码池的比率为1.5。</p>
<p>RAW CAPACITY是OSD上负责存储该pool（可能还有其他pool）数据的raw storage capacity的总量。RATIO是该pool消耗的总容量的比率（即ratio = size * rate / raw capacity）。</p>
<p>TARGET RATIO（如果存在）是管理员指定他们希望该pool使用的存储空间的比率。系统使用actual ratio和target ratio中的较大者进行计算。 如果同时指定了target size bytes和ratio ，则ratio优先。</p>
<p>PG_NUM是该pool的当前PG数。系统认为应将pool的pg_num更改为NEW PG_NUM（如果存在）。它始终是2的幂，并且仅在“理想”值与当前值的差异大于3倍时才存在。</p>
<p>最后一列，AUTOSCALE，是pool pg_autoscale_mode，on, off或warn。</p>
<h3 id="AUTOMATED-SCALING"><a href="#AUTOMATED-SCALING" class="headerlink" title="AUTOMATED SCALING"></a>AUTOMATED SCALING</h3><p>最简单的方法是允许集群根据使用情况自动扩展PG。Ceph将查看可用的总存储量和整个系统的target PG数量，查看每个pool中存储了多少数据并尝试分配相应的PG。该系统的方法相对保守，只有当当前 PG （pg_num） 的数量比它认为的要小 3 倍以上时，才会对pool进行更改。</p>
<p>每个 OSD 的target PG 数基于可配置的 mon_target_pg_per_osd（默认值：100），可通过以下功能进行调整：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set global mon_target_pg_per_osd 100</span><br></pre></td></tr></table></figure>
<p>autoscaler根据每个per-subtree分析pool并进行调整。由于每个pool可能映射到不同的 CRUSH rule，并且每个rule可以跨不同的设备分发数据，所以Ceph将考虑独立使用层次结构的每个subtree。例如，映射到ssd类的OSD的pool和映射到hdd类的OSD的pool将分别具有最佳PG counts，具体取决于这些相应设备类型的数量。</p>
<h3 id="SPECIFYING-EXPECTED-POOL-SIZE（指定预期的pool大小）"><a href="#SPECIFYING-EXPECTED-POOL-SIZE（指定预期的pool大小）" class="headerlink" title="SPECIFYING EXPECTED POOL SIZE（指定预期的pool大小）"></a>SPECIFYING EXPECTED POOL SIZE（指定预期的pool大小）</h3><p>首次创建集群或pool时，它将占用集群总容量的一小部分，并在系统中显示只需要少量的placement groups。但是，在大多数情况下，集群管理员最好知道哪些pool会随着时间的推移消耗大部分系统容量。通过向ceph提供这些信息，可以从一开始就使用更适当数量的pg，从而防止pg-num中的后续更改以及在进行调整时与移动数据相关的开销。</p>
<p>pool的target size*可通过两种方式指定：按pool的绝对大小（即字节）或群集总容量的ratio（比率）指定。</p>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set mypool target_size_bytes 100T</span><br></pre></td></tr></table></figure>
<p>会告诉系统mypool预计会占用100 TiB的空间。 或者：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set mypool target_size_ratio .9</span><br></pre></td></tr></table></figure>
<p>告诉系统mypool预计会消耗群集总容量的90％。</p>
<p>您还可以使用ceph osd pool create命令的可选<code>--target-size-bytes &lt;bytes&gt;</code>或<code>--target-size-ratio &lt;ratio&gt;</code>参数在创建时设置pool的target size。</p>
<p>请注意，如果指定了不可能的target size值（例如，容量大于整个群集的容量或ratio(s)之和大于1.0），则会引发health警告（POOL_TARET_SIZE_RATIO_OVERCOMMITTED或POOL_TARGET_SIZE_BYTES_OVERCOMMITTED）。<a target="_blank" rel="noopener" href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg56416.html">https://www.mail-archive.com/ceph-users@lists.ceph.com/msg56416.html</a></p>
<h3 id="SPECIFYING-BOUNDS-ON-A-POOL’S-PGS（在pool的PGS上指定界限）"><a href="#SPECIFYING-BOUNDS-ON-A-POOL’S-PGS（在pool的PGS上指定界限）" class="headerlink" title="SPECIFYING BOUNDS ON A POOL’S PGS（在pool的PGS上指定界限）"></a>SPECIFYING BOUNDS ON A POOL’S PGS（在pool的PGS上指定界限）</h3><p>也可以为一个pool指定最小数量的PG。设置下限可防止Ceph将PG编号减少（或建议减少）到配置的编号以下。</p>
<p>您可以使用以下方法设置pool的最小PG数量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &lt;pool-name&gt; pg_num_min &lt;num&gt;</span><br></pre></td></tr></table></figure>
<p>您还可以使用ceph osd pool create命令的可选<code>--pg-num-min &lt;num&gt;</code>参数指定创建pool时的最小PG count。</p>
<h1 id="A-PRESELECTION-OF-PG-NUM"><a href="#A-PRESELECTION-OF-PG-NUM" class="headerlink" title="A PRESELECTION OF PG_NUM"></a>A PRESELECTION OF PG_NUM</h1><p>使用以下方法创建新pool时：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create &#123;pool-name&#125; [pg_num]</span><br></pre></td></tr></table></figure>
<p>选择pg_num的值是可选的。 如果您未指定pg_num，则集群可以（默认情况下）根据pool中存储的数据为您自动对其进行调整（请参见上文， <a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/rados/operations/placement-groups/#pg-autoscaler">Autoscaling placement groups</a>）。</p>
<p>或者，可以显式提供pg_num。 但是，是否指定pg_num值并不影响群集是否自动调整该值。 要启用或禁用自动调整，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &#123;pool-name&#125; pg_autoscaler_mode (on|off|warn)</span><br></pre></td></tr></table></figure>
<p>传统上，每个OSD PG的”rule of thumb”为100。使用balancer的附加功能（默认情况下也启用的），每个OSD大约50 PG可能是合理的。autoscaler通常为您提供：</p>
<ul>
<li>使每个pool中的PG与pool中的数据成比例</li>
<li>考虑到每个PG在OSD上的replication或erasuring-coding，最终每个OSD会有50-100个PG</li>
</ul>
<h1 id="HOW-ARE-PLACEMENT-GROUPS-USED（如何使用PLACEMENT-GROUPS）"><a href="#HOW-ARE-PLACEMENT-GROUPS-USED（如何使用PLACEMENT-GROUPS）" class="headerlink" title="HOW ARE PLACEMENT GROUPS USED（如何使用PLACEMENT GROUPS）"></a>HOW ARE PLACEMENT GROUPS USED（如何使用PLACEMENT GROUPS）</h1><p>placement group (PG)聚集pool中的objects，因为以每个object为基础跟踪object placement和object metadata在计算上是昂贵的，即，具有数百万个object的系统无法实际以每个object为基础跟踪placement。</p>
<p><img src="https://docs.ceph.com/docs/master/_images/ditaa-1fde157d24b63e3b465d96eb6afea22078c85a90.png"></p>
<p>Ceph客户端将计算object应位于哪个placement group中。它通过hashing object ID并根据定义的pool中PG的数量和pool ID进行操作来实现此目的。有关详细信息，请参见 <a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/architecture#mapping-pgs-to-osds">Mapping PGs to OSDs</a>。</p>
<p>placement group中object的内容存储在一组OSD中。 例如，在大小为2的replicated pool中，每个placement group将在两个OSD上存储objects，如下所示。</p>
<p><img src="https://docs.ceph.com/docs/master/_images/ditaa-3c86866fb6edc99dad6ccf51e25e536806f0b079.png"></p>
<p>如果OSD #2失败，则将另一个分配给Placement Group #1，并用OSD #1中所有objects的副本填充。 如果pool大小从2更改为3，则会将一个额外的OSD分配给该placement group，并将接收该placement group中所有objects的副本。</p>
<p>Placement groups不拥有OSD； 他们与同一资源pool甚至其他资源pool中的其他placement groups共享它。 如果OSD #2失败，则Placement Group #2还必须使用OSD #3恢复objects的副本。</p>
<p>当placement groups的数量增加时，将为新的placement groups分配OSD。CRUSH函数的结果也将更改，并且先前placement groups中的某些objects将被复制到新的placement groups中，并从旧的placement groups中删除。</p>
<h1 id="PLACEMENT-GROUPS-TRADEOFFS（权衡）"><a href="#PLACEMENT-GROUPS-TRADEOFFS（权衡）" class="headerlink" title="PLACEMENT GROUPS TRADEOFFS（权衡）"></a>PLACEMENT GROUPS TRADEOFFS（权衡）</h1><p>数据持久性以及所有OSD之间的均匀分配都需要更多的placement groups，但应将其数量减少到最少，以节省CPU和内存。</p>
<h3 id="DATA-DURABILITY（数据持久性）"><a href="#DATA-DURABILITY（数据持久性）" class="headerlink" title="DATA DURABILITY（数据持久性）"></a>DATA DURABILITY（数据持久性）</h3><p>OSD发生故障后，数据丢失的风险会增加，直到完全恢复其中包含的数据为止。 假设有一种情况会导致单个placement group中的数据永久丢失：</p>
<ul>
<li>OSD失败，并且它包含的object的所有副本均丢失。对于placement group中的所有objects，副本的数量突然从3个减少到2个。</li>
<li>Ceph通过选择一个新的OSD重新创建所有objects的第三个副本，开始对该placement group的恢复。</li>
<li>在同一placement group内的另一个OSD在新OSD完全填充第三份副本之前发生故障。 这样，某些objects将只有一个幸存副本。</li>
<li>Ceph选择了另一个OSD并保持复制objects以恢复所需的副本数。</li>
<li>在同一placement group中的第三个OSD在恢复完成之前发生故障。 如果此OSD包含object的唯一剩余副本，则它将永久丢失。</li>
</ul>
<p>在三个副本pool中包含10个OSD和512个placement groups的集群中，CRUSH将为每个placement groups提供三个OSD。 最后，每个OSD将托管(512 * 3) / 10 = ~150 Placement Groups。 当第一个OSD发生故障时，以上情况将同时启动所有150个placement groups的恢复。</p>
<p>恢复的150个placement groups可能均匀分布在剩余的9个OSD上。 因此，每个剩余的OSD可能会将objects的副本发送给所有其他objects，并且还可能会接收一些要存储的新objects，因为它们已成为新placement group的一部分。</p>
<p>完成恢复所需的时间完全取决于Ceph集群的架构。 假设每个OSD由一台机器上的1TB SSD托管，并且所有OSD都连接到10Gb/s交换机，并且单个OSD的恢复在M分钟内完成。 如果每台计算机使用不带SSD journal的spinners和1Gb/s交换机的两个OSD，则速度至少要慢一个数量级。</p>
<p>在这种大小的集群中，placement groups的数量几乎对数据持久性没有影响。 可能是128或8192，恢复速度不会变慢或变快。</p>
<p>但是，将相同的Ceph集群增加到20个OSD而不是10个OSD可能会加快恢复速度，从而显着提高数据的持久性。 现在，每个OSD只能参与约75个placement groups，而不是只有10个OSD时的约150个placement groups，并且仍然需要全部19个剩余OSD执行相同数量的object副本才能恢复。 但是，如果10个OSD必须每个复制大约100GB，则现在它们必须每个复制50GB。 如果网络是瓶颈，恢复将以两倍的速度进行。 换句话说，当OSD数量增加时，恢复速度会更快。</p>
<p>如果该群集增长到40个OSD，则每个OSD将仅托管约35个placement groups。 如果OSD死亡，则恢复将保持更快的速度，除非它被另一个瓶颈阻塞。 但是，如果该集群增长到200个OSD，则每个OSD将仅托管约7个placement groups。 如果OSD死亡，则将在这些placement groups中的最多约21 (7 * 3)个OSD之间进行恢复：恢复将比有40个OSD时花费的时间更长，这意味着应增加placement groups的数量。</p>
<p>无论恢复时间有多短，第二个OSD在进行过程中都有可能发生故障。 在上述10个OSD群集中，如果它们中的任何一个失败，则〜17个placement groups（即，正在恢复〜150/9个placement groups）将只有一个幸存副本。 并且如果剩余的8个OSD中的任何一个失败，则两个placement groups的最后一个objects很可能会丢失（即〜17/8个placement groups，仅恢复了一个剩余副本）。</p>
<p>当群集的大小增加到20个OSD时，丢失三个OSD损坏的Placement Groups的数量将减少。 第二个OSD丢失将降级〜4个（即恢复到约75个/ 19个Placement Groups），而不是〜17个，而第三个OSD丢失则仅在它是包含尚存副本的四个OSD之一时才丢失数据。 换句话说，如果在恢复时间范围内丢失一个OSD的概率为0.0001％，则它从具有10个OSD的群集中的17 * 10 * 0.0001％变为具有20个OSD的群集中的4 * 20 * 0.0001％。</p>
<p>简而言之，更多的OSD意味着恢复更快，较低的级联故障风险，从而导致Placement Group的永久丢失。 就数据持久性而言，在少于50个OSD的群集中，具有512或4096个Placement Group大致等效。</p>
<p>注意：向集群添加的新OSD可能需要很长时间才能分配有分配给它的placement groups。 但是，不会降低任何object的质量，也不会影响集群中包含的数据的持久性。</p>
<h3 id="OBJECT-DISTRIBUTION-WITHIN-A-POOL（pool内的object分布）"><a href="#OBJECT-DISTRIBUTION-WITHIN-A-POOL（pool内的object分布）" class="headerlink" title="OBJECT DISTRIBUTION WITHIN A POOL（pool内的object分布）"></a>OBJECT DISTRIBUTION WITHIN A POOL（pool内的object分布）</h3><p>理想情况下，object均匀地分布在每个placement group中。 由于CRUSH计算每个object的placement group，但实际上不知道该placement group内每个OSD中存储了多少数据，因此placement group数与OSD数之比可能会显着影响数据的分布。</p>
<p>例如，如果在三个副本pool中有一个用于十个OSD的placement group，则仅使用三个OSD，因为CRUSH别无选择。 当有更多的placement group可用时，object更有可能在其中均匀分布。 CRUSH还尽一切努力在所有现有的placement group中平均分配OSD。</p>
<p>只要Placement Groups比OSD多一个或两个数量级，则分布应该均匀。 例如，用于3个OSD的256个Placement Groups，用于10个OSD的512或1024个Placement Groups等。</p>
<p>数据分布不均可能是由OSD与placement groups之间的比率以外的因素引起的。 由于CRUSH未考虑object的大小，因此一些非常大的object可能会造成不平衡。 假设有100万个4K object（总计4GB）均匀分布在10个OSD的1024个placement groups中。 他们将在每个OSD上使用4GB / 10 = 400MB。 如果将一个400MB object添加到pool中，则支持放置object的placement groups的三个OSD将填充400MB + 400MB = 800MB，而其余七个将仅占据400MB。</p>
<h3 id="MEMORY-CPU-AND-NETWORK-USAGE（内存，CPU和网络使用情况）"><a href="#MEMORY-CPU-AND-NETWORK-USAGE（内存，CPU和网络使用情况）" class="headerlink" title="MEMORY, CPU AND NETWORK USAGE（内存，CPU和网络使用情况）"></a>MEMORY, CPU AND NETWORK USAGE（内存，CPU和网络使用情况）</h3><p>对于每个placement group，OSD和MON始终需要内存，网络和CPU，并且在恢复期间甚至更多。 通过对placement group内的object进行聚类objects来共享此开销是它们存在的主要原因之一。</p>
<p>最小化placement groups的数量可以节省大量资源。</p>
<h1 id="CHOOSING-THE-NUMBER-OF-PLACEMENT-GROUPS（选择PLACEMENT-GROUPS的数量）"><a href="#CHOOSING-THE-NUMBER-OF-PLACEMENT-GROUPS（选择PLACEMENT-GROUPS的数量）" class="headerlink" title="CHOOSING THE NUMBER OF PLACEMENT GROUPS（选择PLACEMENT GROUPS的数量）"></a>CHOOSING THE NUMBER OF PLACEMENT GROUPS（选择PLACEMENT GROUPS的数量）</h1><p>如果您有超过50个OSD，我们建议每个OSD大约有50-100个placement groups，以平衡资源使用，数据持久性和分发。 如果OSD少于50个，则最好在上述<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/rados/operations/placement-groups/#preselection">preselection</a>中进行选择。 对于单个objects pool，可以使用以下公式获取baseline：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">             (OSDs * 100)</span><br><span class="line">Total PGs &#x3D;  ------------</span><br><span class="line">              pool size</span><br></pre></td></tr></table></figure>
<p>pool size是replicated pools的副本数或erasure coded pools的K + M总和（由ceph osd erasure-code-profile get返回）。</p>
<p>然后，您应该检查设计Ceph集群的方式，以最大程度地提高<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/rados/operations/placement-groups/#data-durability">数据持久性</a>，<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/rados/operations/placement-groups/#object-distribution">对象分配</a>并最小化<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/rados/operations/placement-groups/#resource-usage">资源使用</a>。</p>
<p>结果应始终四舍五入到最接近的2的幂。</p>
<p>只有2的幂可以平衡placement groups之间的objects数量。 其他值将导致OSD上的数据分布不均。</p>
<p>例如，对于具有200个OSD和3个副本的pool大小的集群，您可以按以下方式估计PG的数量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(200 * 100)</span><br><span class="line">----------- &#x3D; 6667. Nearest power of 2: 8192</span><br><span class="line">     3</span><br></pre></td></tr></table></figure>
<p>当使用多个data pools存储objects时，需要确保在每个pool的placement groups数量与每个OSD的placement groups数量之间取得平衡，以便获得合理的placement groups总数，从而为每个OSD提供合理的低偏差而不会增加系统资源的负担或使对等进程太慢。</p>
<p>例如，一个10个pool的集群，每个pool在十个OSD上具有512个placement groups，则总共有5120个placement groups分布在十个OSD上，即每个OSD 512个placement groups。 那不会使用太多资源。 但是，如果创建了1000个pool，每个pool有512个placement groups，则OSD将分别处理约50,000个placement groups，这将需要更多的资源和时间来进行对等。</p>
<p>您可能会发现<a target="_blank" rel="noopener" href="http://ceph.com/pgcalc/">PGCalc</a>工具很有帮助。</p>
<h1 id="SET-THE-NUMBER-OF-PLACEMENT-GROUPS（设置PLACEMENT-GROUPS数）"><a href="#SET-THE-NUMBER-OF-PLACEMENT-GROUPS（设置PLACEMENT-GROUPS数）" class="headerlink" title="SET THE NUMBER OF PLACEMENT GROUPS（设置PLACEMENT GROUPS数）"></a>SET THE NUMBER OF PLACEMENT GROUPS（设置PLACEMENT GROUPS数）</h1><p>要设置pool中的placement groups数量，必须在创建pool时指定placement groups的数量。有关详细信息，请参见<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/master/rados/operations/pools#createpool">Create a Pool</a>。 即使在创建pool之后，您也可以使用以下方法更改placement groups的数量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &#123;pool-name&#125; pg_num &#123;pg_num&#125;</span><br></pre></td></tr></table></figure>
<p>增加placement groups的数量之后，还必须增加placement（pgp_num）的数量，然后集群才能重新平衡。 pgp_num将是CRUSH算法考虑placement的placement groups的数量。 pg_num的增加会拆分placement groups，但是数据将不会迁移到较新的placement groups，直到placement的placement groups为止。 pgp_num增加了。 pgp_num应该等于pg_num。 要增加用于placement的placement groups的数量，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &#123;pool-name&#125; pgp_num &#123;pgp_num&#125;</span><br></pre></td></tr></table></figure>
<p>当减少PG的数量时，将自动为您调整pgp_num。</p>
<h1 id="GET-THE-NUMBER-OF-PLACEMENT-GROUPS（获取PLACEMENT-GROUPS数）"><a href="#GET-THE-NUMBER-OF-PLACEMENT-GROUPS（获取PLACEMENT-GROUPS数）" class="headerlink" title="GET THE NUMBER OF PLACEMENT GROUPS（获取PLACEMENT GROUPS数）"></a>GET THE NUMBER OF PLACEMENT GROUPS（获取PLACEMENT GROUPS数）</h1><p>要获取pool中的placement groups数，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool get &#123;pool-name&#125; pg_num</span><br></pre></td></tr></table></figure>
<h1 id="GET-A-CLUSTER’S-PG-STATISTICS（获取集群的PG统计信息）"><a href="#GET-A-CLUSTER’S-PG-STATISTICS（获取集群的PG统计信息）" class="headerlink" title="GET A CLUSTER’S PG STATISTICS（获取集群的PG统计信息）"></a>GET A CLUSTER’S PG STATISTICS（获取集群的PG统计信息）</h1><p>要获取集群中placement groups的统计信息，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg dump [--format &#123;format&#125;]</span><br></pre></td></tr></table></figure>
<p>有效格式为plain（默认）和json。</p>
<h1 id="GET-STATISTICS-FOR-STUCK-PGS（获取STUCK-PGS的统计信息）"><a href="#GET-STATISTICS-FOR-STUCK-PGS（获取STUCK-PGS的统计信息）" class="headerlink" title="GET STATISTICS FOR STUCK PGS（获取STUCK PGS的统计信息）"></a>GET STATISTICS FOR STUCK PGS（获取STUCK PGS的统计信息）</h1><p>要获取所有处于指定状态的placement groups的统计信息，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg dump_stuck inactive|unclean|stale|undersized|degraded [--format &lt;format&gt;] [-t|--threshold &lt;seconds&gt;]</span><br></pre></td></tr></table></figure>
<p>Inactive Placement groups无法处理读写，因为它们正在等待OSD包含最新数据。</p>
<p>Unclean Placement groups包含的object未复制所需的次数。。 他们应该正在恢复。</p>
<p>Stale Placement groups处于未知状态—承载这些PG的OSD在一段时间内未向monitor报告（由mon_osd_report_timeout配置）。</p>
<p>有效格式为plain（默认）和json。 阈值定义placement group在将其包括在返回的统计信息之前卡住的最小秒数（默认为300秒）。</p>
<h1 id="GET-A-PG-MAP"><a href="#GET-A-PG-MAP" class="headerlink" title="GET A PG MAP"></a>GET A PG MAP</h1><p>要获取特placement group map，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg map &#123;pg-id&#125;</span><br></pre></td></tr></table></figure>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg map 1.6c</span><br></pre></td></tr></table></figure>
<p>Ceph将返回placement group map，placement group和OSD状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">osdmap e13 pg 1.6c (1.6c) -&gt; up [1,0] acting [1,0]</span><br></pre></td></tr></table></figure>
<h1 id="GET-A-PGS-STATISTICS（获取PGS统计信息）"><a href="#GET-A-PGS-STATISTICS（获取PGS统计信息）" class="headerlink" title="GET A PGS STATISTICS（获取PGS统计信息）"></a>GET A PGS STATISTICS（获取PGS统计信息）</h1><p>要检索特定placement group的统计信息，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg &#123;pg-id&#125; query</span><br></pre></td></tr></table></figure>
<h1 id="SCRUB-A-PLACEMENT-GROUP"><a href="#SCRUB-A-PLACEMENT-GROUP" class="headerlink" title="SCRUB A PLACEMENT GROUP"></a>SCRUB A PLACEMENT GROUP</h1><p>要scrub a placement group，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg scrub &#123;pg-id&#125;</span><br></pre></td></tr></table></figure>
<p>Ceph检查primary和任何replica nodes生成的placement group中所有objects的目录进行比较，以确保没有丢失或不匹配的objects，并且它们的内容一致。 假设所有副本都匹配，则扫描可确保所有与snapshot-related的object metadata都是一致的。 通过日志报告错误。</p>
<p>要清理特定pool中的所有placement groups，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool scrub &#123;pool-name&#125;</span><br></pre></td></tr></table></figure>
<h1 id="PRIORITIZE-BACKFILL-RECOVERY-OF-A-PLACEMENT-GROUP-S-（优先考虑PLACEMENT-GROUP的BACKFILL-RECOVERY）"><a href="#PRIORITIZE-BACKFILL-RECOVERY-OF-A-PLACEMENT-GROUP-S-（优先考虑PLACEMENT-GROUP的BACKFILL-RECOVERY）" class="headerlink" title="PRIORITIZE BACKFILL/RECOVERY OF A PLACEMENT GROUP(S)（优先考虑PLACEMENT GROUP的BACKFILL/RECOVERY）"></a>PRIORITIZE BACKFILL/RECOVERY OF A PLACEMENT GROUP(S)（优先考虑PLACEMENT GROUP的BACKFILL/RECOVERY）</h1><p>您可能会遇到这样的情况，即一堆placement groups需要recovery和/或backfill，并且某些特定的groups保存的数据比其他的更为重要（例如，那些PG可能保存正在运行的机器使用的images的数据，而其他PG可能由不活动的机器使用/较少的相关数据）。 在这种情况下，您可能希望优先考虑恢复这些groups，以便更早恢复存储在这些groups上的数据的性能和/或可用性。 为此（在backfill或recovery期间将特定的placement group(s)标记为优先），请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph pg force-recovery &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]</span><br><span class="line">ceph pg force-backfill &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]</span><br></pre></td></tr></table></figure>
<p>这将导致Ceph首先在其他placement groups之前对指定的placement groups执行recovery或backfill。 这不会中断当前正在进行的backfill或recovery，但会导致尽快处理指定的PG。 如果您改变主意或优先考虑wrong groups，请使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph pg cancel-force-recovery &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]</span><br><span class="line">ceph pg cancel-force-backfill &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]</span><br></pre></td></tr></table></figure>
<p>这将从这些PG中删除“force” flag，并将以默认顺序对其进行处理。 同样，这不会影响当前正在处理的placement groups，只会影响仍在排队的placement groups。</p>
<p>recovery或backfill group后，将自动清除“force” flag。</p>
<p>同样，您可以使用以下命令强制Ceph首先对指定pool中的所有placement groups执行recovery或backfill：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool force-recovery &#123;pool-name&#125;</span><br><span class="line">ceph osd pool force-backfill &#123;pool-name&#125;</span><br></pre></td></tr></table></figure>
<p>或</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool cancel-force-recovery &#123;pool-name&#125;</span><br><span class="line">ceph osd pool cancel-force-backfill &#123;pool-name&#125;</span><br></pre></td></tr></table></figure>
<p>如果您改变主意，则可以恢复到默认的recovery或backfill优先级。</p>
<p>请注意，这些命令可能会破坏Ceph内部优先级计算的顺序，因此请谨慎使用！ 特别是，如果您有多个当前共享相同底层OSD的pool，并且某些特定pool中的数据比其他pool更重要，我们建议您使用以下命令以更好的顺序重新排列所有pool的recovery/backfill优先级：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &#123;pool-name&#125; recovery_priority &#123;value&#125;</span><br></pre></td></tr></table></figure>
<p>例如，如果您有10个pool，则可以将最重要的一个优先级设置为10，下一个9，等等。或者您可以不理会大多数pool，而说3个重要的pool分别设置为优先级1或优先级3、2、1。</p>
<h1 id="REVERT-LOST（永不消失）"><a href="#REVERT-LOST（永不消失）" class="headerlink" title="REVERT LOST（永不消失）"></a>REVERT LOST（永不消失）</h1><p>如果集群丢失了一个或多个object，并且您决定放弃对丢失数据的搜索，则必须将unfound的object标记为lost。</p>
<p>如果已查询所有可能的位置并且仍然丢失了objects，则可能必须放弃丢失的objects。</p>
<p>当前唯一受支持的选项是“revert”，它可以回滚到该object的先前版本，或者（如果是新object）则完全忘记它。 要将“unfound”的object标记为“lost”，请执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg &#123;pg-id&#125; mark_unfound_lost revert|delete</span><br></pre></td></tr></table></figure>
<p>重要说明：请谨慎使用此功能，因为它可能会使期望object存在的应用程序感到困惑。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/10/14/ceph-%E9%97%AE%E9%A2%98%E5%88%97%E8%A1%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/14/ceph-%E9%97%AE%E9%A2%98%E5%88%97%E8%A1%A8/" class="post-title-link" itemprop="url">ceph 问题列表</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-14 15:47:02" itemprop="dateCreated datePublished" datetime="2019-10-14T15:47:02+08:00">2019-10-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-22 16:16:36" itemprop="dateModified" datetime="2020-03-22T16:16:36+08:00">2020-03-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h1><h2 id="Ceph升级（L-to-N）引发的问题"><a href="#Ceph升级（L-to-N）引发的问题" class="headerlink" title="Ceph升级（L to N）引发的问题"></a>Ceph升级（L to N）引发的问题</h2><table>
<thead>
<tr>
<th>问题ID</th>
<th>1</th>
</tr>
</thead>
<tbody><tr>
<td>问题出现版本</td>
<td>pre-14.2.3</td>
</tr>
<tr>
<td>问题现象</td>
<td>…<br/>POOLS:<br/>    POOL                           ID      STORED      OBJECTS     USED        %USED     MAX AVAIL <br/>    data                             0      63 TiB      44.59M      63 TiB     30.21        48 TiB <br/>…<br/>but when one OSD was updated it changed to<br/>POOLS:<br/>    POOL                           ID      STORED      OBJECTS     USED        %USED     MAX AVAIL <br/>    data                             0     558 GiB      43.50M    <br/>1.7 TiB      1.22        45 TiB</td>
</tr>
<tr>
<td>问题触发条件</td>
<td>1、从nautilus之前的集群进行了升级<br/>2、然后，您提供一个或多个新的BlueStore OSD，或在升级的OSD上运行“ceph-bluestore-tool repair”。</td>
</tr>
<tr>
<td>问题原因</td>
<td>根本原因是，从Nautilus开始，BlueStore维护了每个池的使用情况统计信息，但是它需要对磁盘上的格式进行少量更改。<br/>除非您运行ceph-bluestore-tool修复，否则升级后的OSD不会拥有新的统计信息。<br/>问题在于，一旦* any * OSD报告了er-pool统计信息，mon就开始使用新的统计信息（而不是等到* all * OSD都在这样做）。</td>
</tr>
<tr>
<td>问题解决办法</td>
<td>为避免此问题，可以<br/>1、升级后不要置备新的BlueStore OSD<br/>2、更新所有OSD，以保留新的每个池统计信息。现有的BlueStore OSD可以通过以下方式转换：<br/>     systemctl stop ceph-osd@$N<br/>     ceph-bluestore-tool repair –path /var/lib/ceph/osd/ceph-$N<br/>     systemctl start ceph-osd@$N<br/>请注意，FileStore根本不支持新版每个池统计信息，因此，如果集群中有文FileStore OSD，则没有解决方法。无需将文件存储OSD替换为bluestore。<br/>修复程序[1]正在通过QA检查，将在14.2.3中出现； 它不会在14.2.2完整发布。</td>
</tr>
<tr>
<td>ceph-users地址</td>
<td><a target="_blank" rel="noopener" href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/035889.html">http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/035889.html</a><br/><a target="_blank" rel="noopener" href="https://github.com/ceph/ceph/pull/28978">https://github.com/ceph/ceph/pull/28978</a><br/><a target="_blank" rel="noopener" href="https://tracker.ceph.com/versions/574">https://tracker.ceph.com/versions/574</a></td>
</tr>
<tr>
<td>备注</td>
<td></td>
</tr>
<tr>
<td>实践修复</td>
<td></td>
</tr>
<tr>
<td>问题修复版本（社区计划）</td>
<td>14.2.3</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>问题ID</th>
<th>2</th>
</tr>
</thead>
<tbody><tr>
<td>问题出现版本</td>
<td>14.2.2</td>
</tr>
<tr>
<td>问题现象</td>
<td>Legacy BlueStore stats reporting detected on 6 OSD(s)</td>
</tr>
<tr>
<td>问题触发条件</td>
<td>1、从nautilus之前的集群进行了升级</td>
</tr>
<tr>
<td>问题原因</td>
<td></td>
</tr>
<tr>
<td>问题解决办法</td>
<td>systemctl stop ceph-osd@$OSDID<br/>ceph-bluestore-tool repair –path /var/lib/ceph/osd/ceph-$OSDID<br/>systemctl start ceph-osd@$OSDID</td>
</tr>
<tr>
<td>ceph-users地址</td>
<td><a target="_blank" rel="noopener" href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/036010.html">http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/036010.html</a></td>
</tr>
<tr>
<td>备注</td>
<td>可以静默告警 bluestore warn on legacy statfs = false</td>
</tr>
<tr>
<td>实践修复</td>
<td>[root@ceph1 ~]# systemctl stop <a href="mailto:&#x63;&#101;&#x70;&#104;&#45;&#111;&#115;&#x64;&#x40;&#49;&#x2e;&#115;&#x65;&#x72;&#x76;&#x69;&#99;&#x65;">&#x63;&#101;&#x70;&#104;&#45;&#111;&#115;&#x64;&#x40;&#49;&#x2e;&#115;&#x65;&#x72;&#x76;&#x69;&#99;&#x65;</a><br/>[root@ceph1 ~]# ceph-bluestore-tool repair –path /var/lib/ceph/osd/ceph-1/<br/>2019-10-14 15:39:53.940 7f87c8114f80 -1 bluestore(/var/lib/ceph/osd/ceph-1) fsck error: legacy statfs record found, removing<br/>2019-10-14 15:39:53.940 7f87c8114f80 -1 bluestore(/var/lib/ceph/osd/ceph-1) fsck error: missing Pool StatFS record for pool 2<br/>2019-10-14 15:39:53.940 7f87c8114f80 -1 bluestore(/var/lib/ceph/osd/ceph-1) fsck error: missing Pool StatFS record for pool ffffffffffffffff<br/>repair success<br/>[root@ceph1 ~]# systemctl start <a href="mailto:&#x63;&#x65;&#112;&#104;&#x2d;&#111;&#x73;&#x64;&#x40;&#49;&#46;&#115;&#x65;&#114;&#118;&#x69;&#x63;&#x65;">&#x63;&#x65;&#112;&#104;&#x2d;&#111;&#x73;&#x64;&#x40;&#49;&#46;&#115;&#x65;&#114;&#118;&#x69;&#x63;&#x65;</a></td>
</tr>
<tr>
<td>问题修复版本（社区计划）</td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>问题ID</th>
<th>3</th>
</tr>
</thead>
<tbody><tr>
<td>问题出现版本</td>
<td>14.2.2</td>
</tr>
<tr>
<td>问题现象</td>
<td>Legacy BlueStore stats reporting detected on 6 OSD(s)</td>
</tr>
<tr>
<td>问题触发条件</td>
<td>1、从nautilus之前的集群进行了升级</td>
</tr>
<tr>
<td>问题原因</td>
<td></td>
</tr>
<tr>
<td>问题解决办法</td>
<td>systemctl stop ceph-osd@$OSDID<br/>ceph-bluestore-tool repair –path /var/lib/ceph/osd/ceph-$OSDID<br/>systemctl start ceph-osd@$OSDID</td>
</tr>
<tr>
<td>ceph-users地址</td>
<td><a target="_blank" rel="noopener" href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/036002.html">http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-July/036002.html</a></td>
</tr>
<tr>
<td>备注</td>
<td>可以静默告警 bluestore warn on legacy statfs = false</td>
</tr>
<tr>
<td>实践修复</td>
<td></td>
</tr>
<tr>
<td>问题修复版本（社区计划）</td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>问题ID</th>
<th>4</th>
</tr>
</thead>
<tbody><tr>
<td>问题出现版本</td>
<td>14.2.4</td>
</tr>
<tr>
<td>问题现象</td>
<td>3 monitors have not enabled msgr2告警</td>
</tr>
<tr>
<td>问题触发条件</td>
<td>1、从nautilus之前的集群进行了升级</td>
</tr>
<tr>
<td>问题原因</td>
<td>messenger v2 protocol（msgr2）是Ceph’s on-wire protocol第二次主要修订。</td>
</tr>
<tr>
<td>问题解决办法</td>
<td>ceph mon enable-msgr2<br>systemctl restart <a href="mailto:&#99;&#101;&#112;&#x68;&#x2d;&#x6d;&#111;&#110;&#64;&#x63;&#101;&#112;&#x68;&#x31;&#46;&#x73;&#x65;&#114;&#118;&#105;&#x63;&#101;">&#99;&#101;&#112;&#x68;&#x2d;&#x6d;&#111;&#110;&#64;&#x63;&#101;&#112;&#x68;&#x31;&#46;&#x73;&#x65;&#114;&#118;&#105;&#x63;&#101;</a></td>
</tr>
<tr>
<td>ceph-users地址</td>
<td></td>
</tr>
<tr>
<td>备注</td>
<td></td>
</tr>
<tr>
<td>实践修复</td>
<td>ceph mon enable-msgr2<br/>systemctl restart <a href="mailto:&#99;&#x65;&#112;&#x68;&#x2d;&#109;&#x6f;&#x6e;&#64;&#99;&#x65;&#112;&#104;&#49;&#x2e;&#115;&#101;&#x72;&#118;&#105;&#x63;&#x65;">&#99;&#x65;&#112;&#x68;&#x2d;&#109;&#x6f;&#x6e;&#64;&#99;&#x65;&#112;&#104;&#49;&#x2e;&#115;&#101;&#x72;&#118;&#105;&#x63;&#x65;</a></td>
</tr>
<tr>
<td>问题修复版本（社区计划）</td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>问题ID</th>
<th>5</th>
</tr>
</thead>
<tbody><tr>
<td>问题出现版本</td>
<td></td>
</tr>
<tr>
<td>问题现象</td>
<td></td>
</tr>
<tr>
<td>问题触发条件</td>
<td></td>
</tr>
<tr>
<td>问题原因</td>
<td></td>
</tr>
<tr>
<td>问题解决办法</td>
<td></td>
</tr>
<tr>
<td>ceph-users地址</td>
<td></td>
</tr>
<tr>
<td>备注</td>
<td></td>
</tr>
<tr>
<td>实践修复</td>
<td></td>
</tr>
<tr>
<td>问题修复版本（社区计划）</td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>问题ID</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>问题出现版本</td>
<td></td>
</tr>
<tr>
<td>问题现象</td>
<td></td>
</tr>
<tr>
<td>问题触发条件</td>
<td></td>
</tr>
<tr>
<td>问题原因</td>
<td></td>
</tr>
<tr>
<td>问题解决办法</td>
<td></td>
</tr>
<tr>
<td>ceph-users地址</td>
<td></td>
</tr>
<tr>
<td>备注</td>
<td></td>
</tr>
<tr>
<td>实践修复</td>
<td></td>
</tr>
<tr>
<td>问题修复版本（社区计划）</td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>问题ID</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>问题出现版本</td>
<td></td>
</tr>
<tr>
<td>问题现象</td>
<td></td>
</tr>
<tr>
<td>问题触发条件</td>
<td></td>
</tr>
<tr>
<td>问题原因</td>
<td></td>
</tr>
<tr>
<td>问题解决办法</td>
<td></td>
</tr>
<tr>
<td>ceph-users地址</td>
<td></td>
</tr>
<tr>
<td>备注</td>
<td></td>
</tr>
<tr>
<td>实践修复</td>
<td></td>
</tr>
<tr>
<td>问题修复版本（社区计划）</td>
<td></td>
</tr>
</tbody></table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2019/10/14/Ceph-MESSENGER-V2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/14/Ceph-MESSENGER-V2/" class="post-title-link" itemprop="url">Ceph MESSENGER V2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-14 11:00:56" itemprop="dateCreated datePublished" datetime="2019-10-14T11:00:56+08:00">2019-10-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-22 16:16:35" itemprop="dateModified" datetime="2020-03-22T16:16:35+08:00">2020-03-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="WHAT-IS-IT"><a href="#WHAT-IS-IT" class="headerlink" title="WHAT IS IT"></a>WHAT IS IT</h1><p>messenger v2 protocol（msgr2）是Ceph’s on-wire protocol第二次主要修订。它具有几个关键功能：</p>
<ul>
<li>安全模式，对通过网络传递的所有数据进行加密</li>
<li>改进了authentication payloads的封装，未来可以集成新的authentication模式（例如Kerberos）</li>
<li>改进了早期的advertisement和negotiation（协商）功能，支持未来的protocol（协议）修订</li>
</ul>
<p>Ceph daemons现在可以绑定到多个端口，从而允许旧版Ceph客户端和支持v2的新客户端连接到同一集群。</p>
<p>默认情况下，monitors现在绑定到新的v2协议的新IANA分配端口3300（ce4h或0xce4），同时还绑定到旧的默认端口6789（旧的v1协议）。</p>
<h1 id="ADDRESS-FORMATS"><a href="#ADDRESS-FORMATS" class="headerlink" title="ADDRESS FORMATS"></a>ADDRESS FORMATS</h1><p>在nautilus之前，所有网络地址都呈现为1.2.3.4:567/89012，其中有一个IP地址，一个端口和一个随机数，以唯一地标识网络上的客户端或daemon 。 从nautilus开始，我们现在具有三种不同的地址类型：</p>
<ul>
<li><p>v2：<code>v2:1.2.3.4:578/89012</code>标识daemon绑定到新v2协议的端口</p>
</li>
<li><p>v1：<code>v1:1.2.3.4:578/89012</code>标识绑定到旧版v1协议的端口的daemon。 以前使用任何前缀显示的任何地址现在都显示为v1: address。</p>
</li>
<li><p>TYPE_ANY地址标识表示客户端可以支持两种协议版本。 在nautilus之前，客户端将显示为<code>1.2.3.4:0/123456</code>，其中端口0表示它们是客户端，并且不接受连接。 从Nautilus开始，这些客户端现在在内部由TYPE_ANY address表示，并且仍显示为不带前缀，因为它们可能会使用v2或v1协议连接到daemons，具体取决于daemons使用的协议。</p>
</li>
</ul>
<p>因为daemons现在绑定到多个端口，所以现在用地址向量而不是单个地址来描述它们。 例如，在Nautilus cluster上dump monitor map会有如下输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# ceph mon dump</span><br><span class="line">epoch 1</span><br><span class="line">fsid 50fcf227-be32-4bcb-8b41-34ca8370bd16</span><br><span class="line">last_changed 2019-02-25 11:10:46.700821</span><br><span class="line">created 2019-02-25 11:10:46.700821</span><br><span class="line">min_mon_release 14 (nautilus)</span><br><span class="line">0: [v2:10.0.0.10:3300&#x2F;0,v1:10.0.0.10:6789&#x2F;0] mon.foo</span><br><span class="line">1: [v2:10.0.0.11:3300&#x2F;0,v1:10.0.0.11:6789&#x2F;0] mon.bar</span><br><span class="line">2: [v2:10.0.0.12:3300&#x2F;0,v1:10.0.0.12:6789&#x2F;0] mon.baz</span><br></pre></td></tr></table></figure>
<p>方括号或地址向量表示可以在多个端口（和协议）上访问同一daemon。如果可能，任何连接到该daemon的客户端或其他daemon都将使用v2协议（listed first）； 否则，它将返回到旧版v1协议。 旧版客户端将仅看到v1地址，并且将继续使用v1协议进行连接。</p>
<p>从Nautilus开始，mon_host配置选项和<code>-m &lt;mon-host&gt;</code>命令行选项支持相同的带括号的地址向量语法。</p>
<h2 id="BIND-CONFIGURATION-OPTIONS"><a href="#BIND-CONFIGURATION-OPTIONS" class="headerlink" title="BIND CONFIGURATION OPTIONS"></a>BIND CONFIGURATION OPTIONS</h2><p>两个新的配置选项控制是否使用v1 and/or v2协议：</p>
<ul>
<li>ms_bind_msgr1 [default: true]控制daemon是否绑定到使用v1协议的端口</li>
<li>ms_bind_msgr2 [default: true]控制daemon是否绑定到使用v2协议的端口</li>
</ul>
<p>同样，两个选项控制是否使用IPv4和IPv6地址：</p>
<ul>
<li>ms_bind_ipv4 [默认值：true]控制daemon是否绑定到IPv4地址</li>
<li>ms_bind_ipv6 [默认值：false]控制daemon是否绑定到IPv6地址</li>
</ul>
<h1 id="CONNECTION-MODES"><a href="#CONNECTION-MODES" class="headerlink" title="CONNECTION MODES"></a>CONNECTION MODES</h1><p>v2协议支持两种连接模式：</p>
<ul>
<li><p>crc模式提供：</p>
<ul>
<li>建立连接时进行强大的初始身份验证（通过cephx，双方相互认证，防止中间人或窃听者进入）</li>
<li>CRC32C完整性检查，以防止由于flaky hardware或cosmic rays引起的位翻转</li>
</ul>
<p>crc模式不提供：</p>
<ul>
<li>加密（网络上的窃听者可以看到所有经过身份验证后的流量）</li>
<li>免受恶意中间人的攻击（只要他们调整crc32c的值以使其匹配，就可以故意修改流量）</li>
</ul>
</li>
<li><p>secure模式提供：</p>
<ul>
<li>建立连接时进行强大的初始身份验证（通过cephx，双方相互认证，防止中间人或窃听者进入）</li>
<li>对所有认证后流量完全加密，包括加密完整性检查。</li>
</ul>
<p>在Nautilus中，secure模式使用<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Galois/Counter_Mode">AES-GCM</a> stream cipher，这在现代处理器上通常非常快（例如，比SHA-256 cryptographic hash更快）。</p>
</li>
</ul>
<h2 id="CONNECTION-MODE-CONFIGURATION-OPTIONS"><a href="#CONNECTION-MODE-CONFIGURATION-OPTIONS" class="headerlink" title="CONNECTION MODE CONFIGURATION OPTIONS"></a>CONNECTION MODE CONFIGURATION OPTIONS</h2><p>对于大多数连接，有一些选项可以控制使用哪种模式：</p>
<ul>
<li>ms_cluster_mode用于Ceph daemons之间的集群内通信的连接模式（或允许的模式）。 如果列出了多个模式，则首选第一个列出的模式。</li>
<li>ms_service_mode是客户端连接到群集时允许使用的模式的列表。</li>
<li>ms_client_mode是按优先顺序排列的连接模式列表，供客户端在与Ceph群集通信时使用（或允许）。</li>
</ul>
<p>有一组并行的选项专门适用于monitors，允许管理员设置与monitors通信的不同（通常更安全）要求。</p>
<ul>
<li>ms_mon_cluster_mode是monitors之间使用的连接模式（或允许的模式）。</li>
<li>ms_mon_service_mode是客户端或其他Ceph daemons连接到monitors时允许使用的模式的列表。</li>
<li>ms_mon_client_mode是按优先顺序排列的连接模式列表，供客户端或non-monitor daemons在连接monitors时使用。</li>
</ul>
<h1 id="TRANSITIONING-FROM-V1-ONLY-TO-V2-PLUS-V1"><a href="#TRANSITIONING-FROM-V1-ONLY-TO-V2-PLUS-V1" class="headerlink" title="TRANSITIONING FROM V1-ONLY TO V2-PLUS-V1"></a>TRANSITIONING FROM V1-ONLY TO V2-PLUS-V1</h1><p>默认情况下，从Nautilus 14.2.z开始，ms_bind_msgr2为true。 但是，在monitors开始使用v2之前，只有有限的服务可以使用v2地址。</p>
<p>对于大多数用户，monitors已绑定到v1协议的默认旧版端口6789。 在这种情况下，启用v2非常简单：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph mon enable-msgr2</span><br></pre></td></tr></table></figure>
<p>如果monitors绑定到non-standard端口，则需要为v2明确指定其端口。例如，如果monitors mon.a绑定到1.2.3.4:1111，并且您想要在端口1112上添加v2，则：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph mon set-addrs a [v2:1.2.3.4:1112,v1:1.2.3.4:1111]</span><br></pre></td></tr></table></figure>
<p>monitors绑定到v2后，每个daemon将在下一次重新启动时开始使用v2地址。</p>
<h1 id="UPDATING-CEPH-CONF-AND-MON-HOST"><a href="#UPDATING-CEPH-CONF-AND-MON-HOST" class="headerlink" title="UPDATING CEPH.CONF AND MON_HOST"></a>UPDATING CEPH.CONF AND MON_HOST</h1><p>在Nautilus之前，CLI用户或daemon通常将通过/etc/ceph/ceph.conf中的mon_host选项发现monitors。 此选项的语法从Nautilus开始扩展，以允许支持新的方括号列表格式。 例如，像这样的旧行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mon_host &#x3D; 10.0.0.1:6789,10.0.0.2:6789,10.0.0.3:6789</span><br></pre></td></tr></table></figure>
<p>可以更改为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mon_host &#x3D; [v2:10.0.0.1:3300&#x2F;0,v1:10.0.0.1:6789&#x2F;0],[v2:10.0.0.2:3300&#x2F;0,v1:10.0.0.2:6789&#x2F;0],[v2:10.0.0.3:3300&#x2F;0,v1:10.0.0.3:6789&#x2F;0]</span><br></pre></td></tr></table></figure>
<p>但是，使用默认端口（3300和6789）时，可以将其省略：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mon_host &#x3D; 10.0.0.1,10.0.0.2,10.0.0.3</span><br></pre></td></tr></table></figure>
<p>一旦在monitors上启用了v2，可能需要更新ceph.conf以不指定任何端口（这通常是最简单的），或者显式指定v2和v1地址。 但是请注意，Nautilus和更高版本才能理解新的带括号语法，因此请不要在尚未升级其ceph packages的主机上进行此更改。</p>
<p>当更新ceph.conf时，请注意新的<code>ceph config generate-minimal-conf</code>命令（它生成一个简单的配置文件，其中包含足够的信息来访问monitors）而<code>ceph config assimilate-conf</code>（将配置文件选项移动到monitors’配置数据库中）可能会有所帮助。 例如，：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># ceph config assimilate-conf &lt; &#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line"># ceph config generate-minimal-config &gt; &#x2F;etc&#x2F;ceph&#x2F;ceph.conf.new</span><br><span class="line"># cat &#x2F;etc&#x2F;ceph&#x2F;ceph.conf.new</span><br><span class="line"># minimal ceph.conf for 0e5a806b-0ce5-4bc6-b949-aa6f68f5c2a3</span><br><span class="line">[global]</span><br><span class="line">        fsid &#x3D; 0e5a806b-0ce5-4bc6-b949-aa6f68f5c2a3</span><br><span class="line">        mon_host &#x3D; [v2:10.0.0.1:3300&#x2F;0,v1:10.0.0.1:6789&#x2F;0]</span><br><span class="line"># mv &#x2F;etc&#x2F;ceph&#x2F;ceph.conf.new &#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br></pre></td></tr></table></figure>
<h1 id="PROTOCOL"><a href="#PROTOCOL" class="headerlink" title="PROTOCOL"></a>PROTOCOL</h1><p>有关v2 wire protocol的详细说明，请参见<a target="_blank" rel="noopener" href="https://docs.ceph.com/docs/nautilus/dev/msgr2/#msgr2-protocol">msgr2 protocol</a>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">110</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">45</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
