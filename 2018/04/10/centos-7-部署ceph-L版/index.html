<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="ceph L版已经发布很久了，官方说默认使用BlueStore作为OSD的存储后端，在Cephalocon APAC 2018上也是讨论的焦点之一。 提到BlueStore，不得不说一说Ceph的STORAGE DEVICES。 STORAGE DEVICESCeph守护进程将数据存储在磁盘上： 123456Ceph OSDs ( Object Storage Daemons )    Clien">
<meta property="og:type" content="article">
<meta property="og:title" content="Centos 7 部署Ceph L版">
<meta property="og:url" content="http://example.com/2018/04/10/centos-7-%E9%83%A8%E7%BD%B2ceph-L%E7%89%88/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="ceph L版已经发布很久了，官方说默认使用BlueStore作为OSD的存储后端，在Cephalocon APAC 2018上也是讨论的焦点之一。 提到BlueStore，不得不说一说Ceph的STORAGE DEVICES。 STORAGE DEVICESCeph守护进程将数据存储在磁盘上： 123456Ceph OSDs ( Object Storage Daemons )    Clien">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://docs.ceph.com/docs/master/_images/ditaa-b5a320fc160057a1a7da010b4215489fa66de242.png">
<meta property="og:image" content="http://docs.ceph.com/docs/master/_images/ditaa-50d12451eb76c5c72c4574b08f0320b39a42e5f1.png">
<meta property="article:published_time" content="2018-04-10T14:21:44.000Z">
<meta property="article:modified_time" content="2020-03-22T08:16:36.043Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="CEPH">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://docs.ceph.com/docs/master/_images/ditaa-b5a320fc160057a1a7da010b4215489fa66de242.png">

<link rel="canonical" href="http://example.com/2018/04/10/centos-7-%E9%83%A8%E7%BD%B2ceph-L%E7%89%88/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Centos 7 部署Ceph L版 | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/04/10/centos-7-%E9%83%A8%E7%BD%B2ceph-L%E7%89%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Centos 7 部署Ceph L版
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-10 22:21:44" itemprop="dateCreated datePublished" datetime="2018-04-10T22:21:44+08:00">2018-04-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-22 16:16:36" itemprop="dateModified" datetime="2020-03-22T16:16:36+08:00">2020-03-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>ceph L版已经发布很久了，官方说默认使用BlueStore作为OSD的存储后端，在Cephalocon APAC 2018上也是讨论的焦点之一。</p>
<p>提到BlueStore，不得不说一说Ceph的STORAGE DEVICES。</p>
<h2 id="STORAGE-DEVICES"><a href="#STORAGE-DEVICES" class="headerlink" title="STORAGE DEVICES"></a>STORAGE DEVICES</h2><p>Ceph守护进程将数据存储在磁盘上：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Ceph OSDs ( Object Storage Daemons )</span><br><span class="line">    Client端的大多数数据写入Ceph后被存储的地方，一般而言，每个OSD都由单一存储设备支持，如传统硬盘（HDD）或固态硬盘（SSD）。</span><br><span class="line">    OSD还可以由多种设备组合，如存储数据的HDD和存储某些元数据的SSD（或SSD的分区）。</span><br><span class="line">    群集中OSD的数量通常取决于你要存储的数据量，还需要考虑每个存储设备的容量以及冗余级别和存储类型（replication或erasure coding）。</span><br><span class="line">Ceph Monitor</span><br><span class="line">    管理关键群集状态，如cluster membership和authentication信息。对于较小的集群，需要几千兆字节（几个GB），然而对于较大的集群，monitor的数据库可以达到几十甚至几百千兆（几十个GB甚至几百个GB）。</span><br></pre></td></tr></table></figure>
<h2 id="OSD-BACKENDS"><a href="#OSD-BACKENDS" class="headerlink" title="OSD BACKENDS"></a>OSD BACKENDS</h2><p>OSD可以通过两种方式管理存储的数据。从Luminous 12.2.z发行版开始，新的默认（推荐）后端是 BlueStore。在Luminous之前，默认（也是唯一的选择）是 FileStore。</p>
<h3 id="BLUESTORE"><a href="#BLUESTORE" class="headerlink" title="BLUESTORE"></a>BLUESTORE</h3><p>BlueStore是专门用于Ceph OSD管理磁盘上的数据的专用存储后端。在过去十年间，受到了FileStore管理OSD经验的启发.<br>BlueStore的主要功能包括：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">直接管理存储设备 ( Direct management of storage devices )</span><br><span class="line">    BlueStore使用原始块设备或分区。这避免了任何可能限制性能或增加复杂性的抽象层（如像XFS这样的本地文件系统）。</span><br><span class="line"></span><br><span class="line">使用RocksDB进行元数据管理 ( Metadata management with RocksDB )</span><br><span class="line">    为了管理内部元数据，我们嵌入了RocksDB的key&#x2F;value数据库。例如在磁盘上，从object names到block locations的映射。</span><br><span class="line"></span><br><span class="line">完整的数据和元数据校验 ( Full data and metadata checksumming )</span><br><span class="line">    默认情况下，写入BlueStore的所有数据和元数据都受到一个或多个校验和的保护。没有数据或元数据在未经过验证的情况下，就从磁盘读取或返回给用户。</span><br><span class="line"></span><br><span class="line">内置压缩 ( Inline compression )</span><br><span class="line">    写入的数据在写入磁盘之前可以选择压缩。</span><br><span class="line"></span><br><span class="line">多设备元数据分层 ( Multi-device metadata tiering )</span><br><span class="line">    BlueStore允许将其内部journal（预写日志，write-ahead log）写入单独的高速设备（如SSD，NVMe或NVDIMM）以提高性能。</span><br><span class="line">    如果有大量更快速的存储可用，则内部元数据也可以存储在更快的设备上。</span><br><span class="line"></span><br><span class="line">高效的写时复制 ( Efficient copy-on-write )</span><br><span class="line">    RBD和CephFS快照依赖于copy-on-write clone机制，也在BlueStore中得到了有效的实现。这将为常规快照和erasure coded池提供高效的IO（依靠clone实现高效的two-phase commits）</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/">http://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/</a><br><a target="_blank" rel="noopener" href="http://docs.ceph.com/docs/master/rados/operations/bluestore-migration/">http://docs.ceph.com/docs/master/rados/operations/bluestore-migration/</a></p>
<h3 id="FILESTORE"><a href="#FILESTORE" class="headerlink" title="FILESTORE"></a>FILESTORE</h3><p>FileStore是在Ceph中存储objects的传统方法。它依赖于标准文件系统（通常是XFS）和某个元数据的key/value数据库（传统上是LevelDB，现在是RocksDB）结合使用。<br>FileStore经过良好测试并广泛用于生产，但由于其整体设计和对传统文件系统存储object数据的依赖性，因此存在许多性能缺陷。<br>尽管FileStore通常能够在大多数与POSIX兼容的文件系统（包括btrfs和ext4）上运行，但我们只建议使用XFS。<br>btrfs和ext4都有已知的bug和缺陷，使用它们可能会导致数据丢失。默认情况下，所有的Ceph提供的工具都将使用XFS。</p>
<p><a target="_blank" rel="noopener" href="http://docs.ceph.com/docs/master/rados/configuration/filestore-config-ref/">http://docs.ceph.com/docs/master/rados/configuration/filestore-config-ref/</a></p>
<p>在ceph L版代码结构改动比较大，增加了CEPH-MGR向外部监测和管理系统提供额外的监测接口，今天就用虚拟机搭建实验环境玩一玩。</p>
<h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ~]# yum install -y redhat-lsb</span><br><span class="line">[root@cephL ~]# lsb_release -a</span><br><span class="line">LSB Version:	:core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch</span><br><span class="line">Distributor ID:	CentOS</span><br><span class="line">Description:	CentOS Linux release 7.4.1708 (Core)</span><br><span class="line">Release:	7.4.1708</span><br><span class="line">Codename:	Core</span><br><span class="line"></span><br><span class="line">[root@cephL ~]# lsblk</span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0   40G  0 disk</span><br><span class="line">├─sda1            8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2            8:2    0   39G  0 part</span><br><span class="line">  ├─centos-root 253:0    0   36G  0 lvm  /</span><br><span class="line">  └─centos-swap 253:1    0    3G  0 lvm  [SWAP]</span><br><span class="line">sdb               8:16   0   30G  0 disk</span><br><span class="line">sdc               8:32   0   30G  0 disk</span><br><span class="line">sr0              11:0    1 1024M  0 rom</span><br></pre></td></tr></table></figure>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="安装pip和ceph-deploy"><a href="#安装pip和ceph-deploy" class="headerlink" title="安装pip和ceph-deploy"></a>安装pip和ceph-deploy</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ~]# curl &quot;https://bootstrap.pypa.io/get-pip.py&quot; -o &quot;get-pip.py&quot;</span><br><span class="line">[root@cephL ~]# python get-pip.py</span><br><span class="line">[root@cephL ~]# python -m pip install -U pip</span><br><span class="line">[root@cephL ~]# pip install --upgrade setuptools</span><br><span class="line">[root@cephL ~]# pip install ceph-deploy</span><br><span class="line">[root@cephL ~]# ceph-deploy --version</span><br><span class="line">2.0.0</span><br></pre></td></tr></table></figure>
<h3 id="安装ceph软件包"><a href="#安装ceph软件包" class="headerlink" title="安装ceph软件包"></a>安装ceph软件包</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ~]# mkdir ceph-deploy &amp;&amp; cd ceph-deploy</span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy install cephL --release luminous</span><br></pre></td></tr></table></figure>
<h3 id="开始部署一个新的集群，然后为它写一个CLUSTER-conf和keyring"><a href="#开始部署一个新的集群，然后为它写一个CLUSTER-conf和keyring" class="headerlink" title="开始部署一个新的集群，然后为它写一个CLUSTER.conf和keyring"></a>开始部署一个新的集群，然后为它写一个CLUSTER.conf和keyring</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ceph-deploy]# ceph-deploy new --public-network 192.168.56.101/24  --cluster-network 192.168.56.101/24 cephL</span><br></pre></td></tr></table></figure>
<h3 id="部署MON"><a href="#部署MON" class="headerlink" title="部署MON"></a>部署MON</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ceph-deploy]# ceph-deploy mon create-initial</span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy mon create cephL</span><br><span class="line">ceph        1110       1  0 12:57 ?        00:00:01 /usr/bin/ceph-mon -f --cluster ceph --id cephL --setuser ceph --setgroup ceph</span><br></pre></td></tr></table></figure>
<h3 id="部署OSD"><a href="#部署OSD" class="headerlink" title="部署OSD"></a>部署OSD</h3><p>bluestore方法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在创建osd时，L版默认是bluestore</span></span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy osd create --data /dev/sdb cephL</span><br><span class="line">ceph        1514       1  0 12:57 ?        00:00:01 /usr/bin/ceph-osd -f --cluster ceph --id 0 --setuser ceph --setgroup ceph</span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy osd create --data /dev/sdc cephL</span><br><span class="line">ceph        1518       1  0 12:57 ?        00:00:01 /usr/bin/ceph-osd -f --cluster ceph --id 1 --setuser ceph --setgroup ceph</span><br></pre></td></tr></table></figure>
<p>遇到问题</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ceph-deploy]# ceph -s</span><br><span class="line">2018-04-10 12:00:19.660298 7fd1fe0ae700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory</span><br><span class="line">2018-04-10 12:00:19.660310 7fd1fe0ae700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication</span><br><span class="line">2018-04-10 12:00:19.660312 7fd1fe0ae700  0 librados: client.admin initialization error (2) No such file or directory</span><br><span class="line">[errno 2] error connecting to the cluster</span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# chmod +r *</span><br><span class="line">[root@cephL ceph-deploy]# cp ceph.client.admin.keyring /etc/ceph/</span><br><span class="line">[root@cephL ceph-deploy]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     765752b7-1f77-4d0d-bc18-936b8ad409fd</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            no active mgr</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum cephL</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 2 osds: 2 up, 2 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 bytes</span><br><span class="line">    usage:   0 kB used, 0 kB / 0 kB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>
<p>filestore方法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 如果是filestore则需要对data device和journal device先做GPT partition</span></span><br><span class="line">--data DATA           The OSD data logical volume (vg/lv) or absolute path to device</span><br><span class="line">--journal JOURNAL     Logical Volume (vg/lv) or path to GPT partition</span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# fdisk /dev/sdb</span><br><span class="line">WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.</span><br><span class="line">欢迎使用 fdisk (util-linux 2.23.2)。</span><br><span class="line">更改将停留在内存中，直到您决定将更改写入磁盘。</span><br><span class="line">使用写入命令前请三思。</span><br><span class="line">命令(输入 m 获取帮助)：g</span><br><span class="line">Building a new GPT disklabel (GUID: 80097CEF-475B-4161-ACC7-7164F6A39DD2)</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">分区号 (1-128，默认 1)：</span><br><span class="line">第一个扇区 (2048-62914526，默认 2048)：</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (2048-62914526，默认 62914526)：</span><br><span class="line">已创建分区 1</span><br><span class="line">命令(输入 m 获取帮助)：w</span><br><span class="line">The partition table has been altered!</span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">正在同步磁盘。</span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# fdisk /dev/sdc</span><br><span class="line">WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.</span><br><span class="line">欢迎使用 fdisk (util-linux 2.23.2)。</span><br><span class="line">更改将停留在内存中，直到您决定将更改写入磁盘。</span><br><span class="line">使用写入命令前请三思。</span><br><span class="line">命令(输入 m 获取帮助)：g</span><br><span class="line">Building a new GPT disklabel (GUID: 21DFA98C-5BCF-40E7-A120-3DEDEA6600ED)</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">分区号 (1-128，默认 1)：</span><br><span class="line">第一个扇区 (2048-62914526，默认 2048)：</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (2048-62914526，默认 62914526)：</span><br><span class="line">已创建分区 1</span><br><span class="line">命令(输入 m 获取帮助)：w</span><br><span class="line">The partition table has been altered!</span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">正在同步磁盘。</span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# lsblk</span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0   40G  0 disk </span><br><span class="line">├─sda1            8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2            8:2    0   39G  0 part </span><br><span class="line">  ├─centos-root 253:0    0   36G  0 lvm  /</span><br><span class="line">  └─centos-swap 253:1    0    3G  0 lvm  [SWAP]</span><br><span class="line">sdb               8:16   0   30G  0 disk </span><br><span class="line">└─sdb1            8:17   0   30G  0 part </span><br><span class="line">sdc               8:32   0   30G  0 disk </span><br><span class="line">└─sdc1            8:33   0   30G  0 part </span><br><span class="line">sr0              11:0    1 1024M  0 rom  </span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy osd create --filestore --fs-type xfs --data /dev/sdb1 --journal /dev/sdc1 cephL</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.0): /usr/bin/ceph-deploy osd create --filestore --fs-type xfs --data /dev/sdb1 --journal /dev/sdc1 cephL</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  bluestore                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x22c7320&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  fs_type                       : xfs</span><br><span class="line">[ceph_deploy.cli][INFO  ]  block_wal                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  journal                       : /dev/sdc1</span><br><span class="line">[ceph_deploy.cli][INFO  ]  subcommand                    : create</span><br><span class="line">[ceph_deploy.cli][INFO  ]  host                          : cephL</span><br><span class="line">[ceph_deploy.cli][INFO  ]  filestore                     : True</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function osd at 0x225ae60&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  zap_disk                      : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  data                          : /dev/sdb1</span><br><span class="line">[ceph_deploy.cli][INFO  ]  block_db                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  dmcrypt                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  debug                         : False</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Creating OSD on cluster ceph with data device /dev/sdb1</span><br><span class="line">[cephL][DEBUG ] connected to host: cephL </span><br><span class="line">[cephL][DEBUG ] detect platform information from remote host</span><br><span class="line">[cephL][DEBUG ] detect machine type</span><br><span class="line">[cephL][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.4.1708 Core</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Deploying osd to cephL</span><br><span class="line">[cephL][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[cephL][DEBUG ] find the location of an executable</span><br><span class="line">[cephL][INFO  ] Running command: /usr/sbin/ceph-volume --cluster ceph lvm create --filestore --data /dev/sdb1 --journal /dev/sdc1</span><br><span class="line">[cephL][DEBUG ] Running command: /bin/ceph-authtool --gen-print-key</span><br><span class="line">[cephL][DEBUG ] Running command: /bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new 8b7be4a6-b563-434e-b030-132880a10d31</span><br><span class="line">[cephL][DEBUG ] Running command: vgcreate --force --yes ceph-8e2515c1-6170-4299-b82c-a5a47681f946 /dev/sdb1</span><br><span class="line">[cephL][DEBUG ]  stdout: Physical volume &quot;/dev/sdb1&quot; successfully created.</span><br><span class="line">[cephL][DEBUG ]  stdout: Volume group &quot;ceph-8e2515c1-6170-4299-b82c-a5a47681f946&quot; successfully created</span><br><span class="line">[cephL][DEBUG ] Running command: lvcreate --yes -l 100%FREE -n osd-data-8b7be4a6-b563-434e-b030-132880a10d31 ceph-8e2515c1-6170-4299-b82c-a5a47681f946</span><br><span class="line">[cephL][DEBUG ]  stdout: Logical volume &quot;osd-data-8b7be4a6-b563-434e-b030-132880a10d31&quot; created.</span><br><span class="line">[cephL][DEBUG ] Running command: /bin/ceph-authtool --gen-print-key</span><br><span class="line">[cephL][DEBUG ] Running command: mkfs -t xfs -f -i size=2048 /dev/ceph-8e2515c1-6170-4299-b82c-a5a47681f946/osd-data-8b7be4a6-b563-434e-b030-132880a10d31</span><br><span class="line">[cephL][DEBUG ]  stdout: meta-data=/dev/ceph-8e2515c1-6170-4299-b82c-a5a47681f946/osd-data-8b7be4a6-b563-434e-b030-132880a10d31 isize=2048   agcount=4, agsize=1965824 blks</span><br><span class="line">[cephL][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">[cephL][DEBUG ]          =                       crc=1        finobt=0, sparse=0</span><br><span class="line">[cephL][DEBUG ] data     =                       bsize=4096   blocks=7863296, imaxpct=25</span><br><span class="line">[cephL][DEBUG ]          =                       sunit=0      swidth=0 blks</span><br><span class="line">[cephL][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1</span><br><span class="line">[cephL][DEBUG ] log      =internal log           bsize=4096   blocks=3839, version=2</span><br><span class="line">[cephL][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1</span><br><span class="line">[cephL][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">[cephL][DEBUG ] Running command: mount -t xfs -o rw,noatime,inode64 /dev/ceph-8e2515c1-6170-4299-b82c-a5a47681f946/osd-data-8b7be4a6-b563-434e-b030-132880a10d31 /var/lib/ceph/osd/ceph-0</span><br><span class="line">[cephL][DEBUG ] Running command: chown -R ceph:ceph /dev/sdc1</span><br><span class="line">[cephL][DEBUG ] Running command: ln -s /dev/sdc1 /var/lib/ceph/osd/ceph-0/journal</span><br><span class="line">[cephL][DEBUG ] Running command: ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/osd/ceph-0/activate.monmap</span><br><span class="line">[cephL][DEBUG ]  stderr: got monmap epoch 1</span><br><span class="line">[cephL][DEBUG ] Running command: chown -R ceph:ceph /dev/sdc1</span><br><span class="line">[cephL][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-0/</span><br><span class="line">[cephL][DEBUG ] Running command: ceph-osd --cluster ceph --osd-objectstore filestore --mkfs -i 0 --monmap /var/lib/ceph/osd/ceph-0/activate.monmap --osd-data /var/lib/ceph/osd/ceph-0/ --osd-journal /var/lib/ceph/osd/ceph-0/journal --osd-uuid 8b7be4a6-b563-434e-b030-132880a10d31 --setuser ceph --setgroup ceph</span><br><span class="line">[cephL][DEBUG ]  stderr: 2018-05-07 23:01:34.834993 7f315e466d00 -1 journal check: ondisk fsid 00000000-0000-0000-0000-000000000000 doesn&#x27;t match expected 8b7be4a6-b563-434e-b030-132880a10d31, invalid (someone else&#x27;s?) journal</span><br><span class="line">[cephL][DEBUG ]  stderr: 2018-05-07 23:01:34.865621 7f315e466d00 -1 journal do_read_entry(4096): bad header magic</span><br><span class="line">[cephL][DEBUG ] 2018-05-07 23:01:34.865667 7f315e466d00 -1 journal do_read_entry(4096): bad header magic</span><br><span class="line">[cephL][DEBUG ] 2018-05-07 23:01:34.865988 7f315e466d00 -1 read_settings error reading settings: (2) No such file or directory</span><br><span class="line">[cephL][DEBUG ]  stderr: 2018-05-07 23:01:34.916284 7f315e466d00 -1 created object store /var/lib/ceph/osd/ceph-0/ for osd.0 fsid 39f3b85e-ee3c-4d8d-93c2-7f7c8aa47121</span><br><span class="line">[cephL][DEBUG ] Running command: ceph-authtool /var/lib/ceph/osd/ceph-0/keyring --create-keyring --name osd.0 --add-key AQBDavBa0IPpIBAAlQxlaWxNrnTX/uaOMdZEQw==</span><br><span class="line">[cephL][DEBUG ]  stdout: creating /var/lib/ceph/osd/ceph-0/keyring</span><br><span class="line">[cephL][DEBUG ] added entity osd.0 auth auth(auid = 18446744073709551615 key=AQBDavBa0IPpIBAAlQxlaWxNrnTX/uaOMdZEQw== with 0 caps)</span><br><span class="line">[cephL][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-0/keyring</span><br><span class="line">[cephL][DEBUG ] --&gt; ceph-volume lvm prepare successful for: /dev/sdb1</span><br><span class="line">[cephL][DEBUG ] Running command: ln -snf /dev/sdc1 /var/lib/ceph/osd/ceph-0/journal</span><br><span class="line">[cephL][DEBUG ] Running command: chown -R ceph:ceph /dev/sdc1</span><br><span class="line">[cephL][DEBUG ] Running command: systemctl enable ceph-volume@lvm-0-8b7be4a6-b563-434e-b030-132880a10d31</span><br><span class="line">[cephL][DEBUG ]  stderr: Created symlink from /etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-0-8b7be4a6-b563-434e-b030-132880a10d31.service to /usr/lib/systemd/system/ceph-volume@.service.</span><br><span class="line">[cephL][DEBUG ] Running command: systemctl start ceph-osd@0</span><br><span class="line">[cephL][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: 0</span><br><span class="line">[cephL][DEBUG ] --&gt; ceph-volume lvm create successful for: /dev/sdb1</span><br><span class="line">[cephL][INFO  ] checking OSD status...</span><br><span class="line">[cephL][DEBUG ] find the location of an executable</span><br><span class="line">[cephL][INFO  ] Running command: /bin/ceph --cluster=ceph osd stat --format=json</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Host cephL is now ready for osd use.</span><br></pre></td></tr></table></figure>
<h3 id="移除OSD"><a href="#移除OSD" class="headerlink" title="移除OSD"></a>移除OSD</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 使OSD进入out状态</span></span><br><span class="line">[root@cephL ceph-deploy]# ceph osd out 0</span><br><span class="line">marked out osd.0.</span><br><span class="line"><span class="meta">#</span><span class="bash"> 观察数据迁移</span></span><br><span class="line">[root@cephL ceph-deploy]# ceph -w</span><br><span class="line"><span class="meta">#</span><span class="bash"> 停止对应的OSD进程</span></span><br><span class="line">[root@cephL ceph-deploy]# sudo systemctl stop ceph-osd@0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 清除数据</span></span><br><span class="line">[root@cephL ceph-deploy]# ceph osd purge 0 --yes-i-really-mean-it</span><br><span class="line">purged osd.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在ceph.conf中移除osd配置</span></span><br><span class="line">[root@cephL ceph-deploy]# vi /etc/ceph/ceph.conf </span><br></pre></td></tr></table></figure>
<h3 id="部署CEPH-MGR"><a href="#部署CEPH-MGR" class="headerlink" title="部署CEPH-MGR"></a>部署CEPH-MGR</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">install netstat tool</span><br><span class="line">[root@cephL ~]# yum -y install net-tools</span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy mgr create cephL:cephLMGR</span><br><span class="line">ceph        1111       1  0 12:57 ?        00:00:08 /usr/bin/ceph-mgr -f --cluster ceph --id cephLMGR --setuser ceph --setgroup ceph</span><br><span class="line">[root@cephL ceph-deploy]# ceph mgr module enable dashboard</span><br><span class="line"></span><br><span class="line">open 7000 port</span><br><span class="line">[root@cephL ceph-deploy]# sudo firewall-cmd --zone=public --add-port=7000/tcp --permanent</span><br><span class="line">[root@cephL ceph-deploy]# sudo firewall-cmd --reload</span><br></pre></td></tr></table></figure>
<p>相关命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ceph-deploy]# ceph mgr module ls</span><br><span class="line">[root@cephL ceph-deploy]# ceph mgr services</span><br><span class="line">[root@cephL ceph-deploy]# ceph tell mgr help</span><br></pre></td></tr></table></figure>
<h3 id="部署MDS并创建CEPH-FS"><a href="#部署MDS并创建CEPH-FS" class="headerlink" title="部署MDS并创建CEPH FS"></a>部署MDS并创建CEPH FS</h3><p><img src="http://docs.ceph.com/docs/master/_images/ditaa-b5a320fc160057a1a7da010b4215489fa66de242.png"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ceph-deploy]# ceph-deploy mds create cephL</span><br><span class="line">ceph        2150       1  0 13:00 ?        00:00:00 /usr/bin/ceph-mds -f --cluster ceph --id cephL --setuser ceph --setgroup ceph</span><br></pre></td></tr></table></figure>
<p>Ceph文件系统至少需要两个RADOS pool，一个用于存储数据，一个用于存储元数据。</p>
<p>配置这些pool时，可以考虑：</p>
<p>​     对元数据pool使用更多的replication数量，因为该pool中的任何数据丢失都可能导致整个文件系统无法访问。</p>
<p>​     为元数据pool使用SSD等较低延迟的存储设备，因为这将直接影响客户端上文件系统操作的延迟。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create cephfs_data &lt;pg_num&gt;</span><br><span class="line">ceph osd pool create cephfs_metadata &lt;pg_num&gt;</span><br><span class="line">例如：</span><br><span class="line">[root@cephL ceph-deploy]# ceph osd pool create cephfs_data 32</span><br><span class="line">[root@cephL ceph-deploy]# ceph osd pool create cephfs_metadata 32</span><br></pre></td></tr></table></figure>
<p>更改pool的副本数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &#123;poolname&#125; size &#123;num-replicas&#125;</span><br><span class="line">例如：</span><br><span class="line">[root@cephL ceph-deploy]# ceph osd pool set cephfs_data size 1</span><br><span class="line">[root@cephL ceph-deploy]# ceph osd pool set cephfs_data size 1</span><br></pre></td></tr></table></figure>
<p>一旦创建了pool，就可以使用fs new命令启用文件系统：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph fs new &lt;fs_name&gt; &lt;metadata&gt; &lt;data&gt;</span><br><span class="line">例如：</span><br><span class="line">ceph fs new cephFS cephfs_metadata cephfs_data</span><br></pre></td></tr></table></figure>
<p>一旦创建了文件系统，您的MDS将能够进入active状态。例如，在single MDS system中：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@cephL ceph-deploy]# ceph mds stat</span><br><span class="line">cephFS-1/1/1 up  &#123;0=cephL=up:active&#125;</span><br></pre></td></tr></table></figure>
<p>一旦创建了文件系统并且MDS处于active状态，你就可以挂载文件系统了。如果您创建了多个文件系统，在挂载文件系统时，选择使用哪一个。</p>
<p>如果创建了多个文件系统，并且client在挂载时没有指定挂载哪个文件系统，你可以使用ceph fs set-default命令来设置client默认看到的文件系统。</p>
<p>挂载CEPH FS ( File System ) 有两种方式：</p>
<p><strong>KERNEL DRIVER</strong></p>
<p>要挂载Ceph文件系统，您可以在知道monitor主机IP地址的情况下使用mount命令，或使用mount.ceph utility将monitor主机名解析为IP地址。例如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /mnt/mycephfs</span><br><span class="line">sudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs</span><br><span class="line">例如：</span><br><span class="line">[root@cephL ceph-deploy]# sudo mount -t ceph 192.168.56.101:6789:/ /mnt/mycephfs</span><br><span class="line">mount error 22 = Invalid argument</span><br><span class="line">Ceph 10.x (Jewel)版本开始，如果使用kernel方式（无论是krbd还是cephFS）官方推荐至少使用4.x的kernel。</span><br><span class="line">如果无法升级linux kernel，那么映射rbd请使用librbd方式，cephFS请使用fuse方式。</span><br></pre></td></tr></table></figure>
<p>如果挂载Ceph文件系统时开启了cephx authentication，您必须指定user和secret。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t ceph 192.168.0.1:6789:&#x2F; &#x2F;mnt&#x2F;mycephfs -o name&#x3D;admin,secret&#x3D;AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ&#x3D;&#x3D;</span><br></pre></td></tr></table></figure>
<p>上述用法在Bash history中留下了secret。更安全的方法是从文件中读取secret。 例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t ceph 192.168.0.1:6789:&#x2F; &#x2F;mnt&#x2F;mycephfs -o name&#x3D;admin,secretfile&#x3D;&#x2F;etc&#x2F;ceph&#x2F;admin.secret</span><br></pre></td></tr></table></figure>
<p>如果您有多个文件系统，请使用mds_namespace选项指定要挂载的文件系统，例如-o mds_namespace=myfs</p>
<p>要卸载Ceph文件系统，可以使用umount命令。 例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo umount &#x2F;mnt&#x2F;mycephfs</span><br><span class="line">提示：在执行此命令之前，请确保您不在挂载的目录中。</span><br></pre></td></tr></table></figure>
<p><strong>FUSE</strong></p>
<p>在用户空间（FUSE）中挂载Ceph文件系统之前，请确保客户端主机具有Ceph配置文件的副本以及Ceph元数据服务器的CAPS keyring。</p>
<p>在您的客户端主机上，将Ceph配置文件从monitor主机复制到/etc/ceph目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p &#x2F;etc&#x2F;ceph</span><br><span class="line">sudo scp &#123;user&#125;@&#123;server-machine&#125;:&#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br></pre></td></tr></table></figure>
<p>在您的客户端主机上，将monitor主机的Ceph keyring复制到/etc/ceph目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo scp &#123;user&#125;@&#123;server-machine&#125;:&#x2F;etc&#x2F;ceph&#x2F;ceph.keyring &#x2F;etc&#x2F;ceph&#x2F;ceph.keyring</span><br></pre></td></tr></table></figure>
<p>确保Ceph配置文件和keyring在您的客户端机器上设置了适当的权限（例如，chmod 644）。</p>
<p>要将Ceph文件系统挂在为FUSE，可以使用ceph-fuse命令。 例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir &#x2F;home&#x2F;usernname&#x2F;cephfs</span><br><span class="line">sudo ceph-fuse -m 192.168.0.1:6789 &#x2F;home&#x2F;username&#x2F;cephfs</span><br></pre></td></tr></table></figure>
<p>如果您拥有多个文件系统，请使用 –client_mds_namespace 命令行参数指定要挂载哪一个文件系统，或者向ceph.conf中添加client_mds_namespace设置。</p>
<p>要自动挂载ceph-fuse，您可以在system fstab中添加一个条目。此外还可以使用ceph-fuse@.service和ceph-fuse.target systemd units。通常这些unit文件为ceph-fuse描述默认的dependencies和推荐的execution context。例如使用ceph-fuse挂载到/mnt：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start ceph-fuse@&#x2F;mnt.service</span><br></pre></td></tr></table></figure>
<p>持久化挂载点可通过以下方式进行设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl enable ceph-fuse@&#x2F;mnt.service</span><br></pre></td></tr></table></figure>
<h3 id="部署RGW"><a href="#部署RGW" class="headerlink" title="部署RGW"></a>部署RGW</h3><p><img src="http://docs.ceph.com/docs/master/_images/ditaa-50d12451eb76c5c72c4574b08f0320b39a42e5f1.png"></p>
<p>Ceph Object Gateway原来叫RADOS Gateway，它是构建在librados之上的对象存储接口，为应用程序提供了一个RESTful gateway，用户可以通过HTTP协议访问Ceph存储集群。</p>
<p>Ceph Object Storage支持两个接口：</p>
<ul>
<li><p>S3-compatible：与Amazon S3 RESTful API中一些子集兼容的接口，提供对象存储功能。</p>
</li>
<li><p>Swift-compatible：与OpenStack Swift API中一些子集兼容的接口，提供对象存储功能。</p>
</li>
</ul>
<p>Ceph Object Storage使用Ceph Object Gateway daemon (radosgw)，它是一个HTTP server，用于与Ceph存储集群进行交互。由于它提供了与OpenStack Swift和Amazon S3兼容的接口，因此Ceph Object Gateway具有自己的用户管理。Ceph Object Gateway可以将数据存储在与Ceph Filesystem和Ceph Block Device相同的Ceph存储集群中。但是我相信在生产环境中不会这么做，如果数据量大的话会影响Ceph Filesystem和Ceph Block Device的性能，个人一般会独立出一个Ceph Object Gateway集群。S3和Swift API共享一个通用的namespace，因此您可以使用一个API编写数据并使用另一个API检索它。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Note：Ceph Object Storage 不使用 Ceph Metadata Server</span><br></pre></td></tr></table></figure>




<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 必须部署MGR，才能部署RGW</span></span><br><span class="line"></span><br><span class="line">[root@cephL ceph-deploy]# ceph-deploy rgw create cephL:RGW</span><br><span class="line">root        2799       1  0 13:13 ?        00:00:00 /usr/bin/radosgw -f --cluster ceph --name client.rgw.RGW --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启RGW</span></span><br><span class="line">[root@cephL ~]# systemctl restart ceph-radosgw@rgw.cephL.service</span><br><span class="line">[root@cephL ~]# systemctl restart ceph-radosgw@rgw</span><br><span class="line"></span><br><span class="line">问题一，这难道是ceph-deploy 2.0.0的坑？</span><br><span class="line">[root@cephL ~]# tailf /var/log/ceph/ceph-client.rgw.log</span><br><span class="line">2018-05-11 22:30:31.999421 7f537c31fe00  0 ceph version 12.2.4 (52085d5249a80c5f5121a76d6288429f35e4e77b) luminous (stable), process (unknown), pid 3450</span><br><span class="line">2018-05-11 22:30:32.021546 7f537c31fe00 -1 auth: unable to find a keyring on /var/lib/ceph/radosgw/ceph-rgw/keyring: (2) No such file or directory</span><br><span class="line">2018-05-11 22:30:32.021561 7f537c31fe00 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication</span><br><span class="line">2018-05-11 22:30:32.021563 7f537c31fe00  0 librados: client.rgw initialization error (2) No such file or directory</span><br><span class="line">2018-05-11 22:30:32.022900 7f537c31fe00 -1 Couldn&#x27;t init storage provider (RADOS)</span><br><span class="line"></span><br><span class="line">[root@cephL radosgw]# pwd</span><br><span class="line">/var/lib/ceph/radosgw</span><br><span class="line">[root@cephL radosgw]# ls</span><br><span class="line">ceph-rgw.RGW</span><br><span class="line">[root@cephL radosgw]# mv ceph-rgw.RGW  ceph-rgw </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="配置变动"><a href="#配置变动" class="headerlink" title="配置变动"></a>配置变动</h3><p>在L版中，删除pool的操作做了强制限制。需要在/etc/ceph/ceph.conf中加入相关参数才允许删除pool。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 允许删除pool，需要添加</span><br><span class="line">mon allow pool delete &#x3D; true</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CEPH/" rel="tag"># CEPH</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/04/07/Go%E5%91%BD%E4%BB%A4/" rel="prev" title="Go命令">
      <i class="fa fa-chevron-left"></i> Go命令
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/04/12/RocksDB%E4%BB%8B%E7%BB%8D/" rel="next" title="RocksDB介绍">
      RocksDB介绍 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#STORAGE-DEVICES"><span class="nav-number">1.</span> <span class="nav-text">STORAGE DEVICES</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#OSD-BACKENDS"><span class="nav-number">2.</span> <span class="nav-text">OSD BACKENDS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BLUESTORE"><span class="nav-number">2.1.</span> <span class="nav-text">BLUESTORE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FILESTORE"><span class="nav-number">2.2.</span> <span class="nav-text">FILESTORE</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E4%BF%A1%E6%81%AF"><span class="nav-number">3.</span> <span class="nav-text">环境信息</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85"><span class="nav-number">4.</span> <span class="nav-text">安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85pip%E5%92%8Cceph-deploy"><span class="nav-number">4.1.</span> <span class="nav-text">安装pip和ceph-deploy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85ceph%E8%BD%AF%E4%BB%B6%E5%8C%85"><span class="nav-number">4.2.</span> <span class="nav-text">安装ceph软件包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%80%E5%A7%8B%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84%E9%9B%86%E7%BE%A4%EF%BC%8C%E7%84%B6%E5%90%8E%E4%B8%BA%E5%AE%83%E5%86%99%E4%B8%80%E4%B8%AACLUSTER-conf%E5%92%8Ckeyring"><span class="nav-number">4.3.</span> <span class="nav-text">开始部署一个新的集群，然后为它写一个CLUSTER.conf和keyring</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2MON"><span class="nav-number">4.4.</span> <span class="nav-text">部署MON</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2OSD"><span class="nav-number">4.5.</span> <span class="nav-text">部署OSD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A7%BB%E9%99%A4OSD"><span class="nav-number">4.6.</span> <span class="nav-text">移除OSD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2CEPH-MGR"><span class="nav-number">4.7.</span> <span class="nav-text">部署CEPH-MGR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2MDS%E5%B9%B6%E5%88%9B%E5%BB%BACEPH-FS"><span class="nav-number">4.8.</span> <span class="nav-text">部署MDS并创建CEPH FS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2RGW"><span class="nav-number">4.9.</span> <span class="nav-text">部署RGW</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E5%8F%98%E5%8A%A8"><span class="nav-number">4.10.</span> <span class="nav-text">配置变动</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">110</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">45</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
